{"prompt":"Based on the book of the same name, what are the requirements for “Parallel Distributed Processing”?\n\n###\n\n","completion":" - A set of processing units\n- A state of activation\n- An output function for each unit\n- A pattern of connectivity among units\n- A propagation rule for propagating patterns of activities through the network of connectivities\n- An activation rule for combining the inputs impinging on a unit with the current state of that unit to produce a new level of activation for the unit\n- A learning rule whereby patterns of connectivity are modified by experience\n- An environment within which the system must operate. END"}
{"prompt":"What were the two theoretical misunderstandings that held back the field of neural networks?\n\n###\n\n","completion":" In 1969, Marvin Minsky and Seymour Papert demonstrated in their book, “Perceptrons”, that a single layer of artificial neurons cannot learn simple, critical mathematical functions like XOR logic gate. While they subsequently demonstrated in the same book that additional layers can solve this problem, only the first insight was recognized, leading to the start of the first AI winter.\nIn the 1980’s, models with two layers were being explored. Theoretically, it is possible to approximate any mathematical function using two layers of artificial neurons. However, in practices, these networks were too big and too slow. While it was demonstrated that adding additional layers improved performance, this insight was not acknowledged, and the second AI winter began. In this past decade, with increased data availability, and improvements in computer hardware (both in CPU performance but more importantly in GPU performance), neural networks are finally living up to its potential. END"}
{"prompt":"What is a GPU?\n\n###\n\n","completion":" GPU stands for Graphics Processing Unit (also known as a graphics card). Standard computers have various components like CPUs, RAM, etc. CPUs, or central processing units, are the core units of all standard computers, and they execute the instructions that make up computer programs. GPUs, on the other hand, are specialized units meant for displaying graphics, especially the 3D graphics in modern computer games. The hardware optimizations used in GPUs allow it to handle thousands of tasks at the same time. Incidentally, these optimizations allow us to run and train neural networks hundreds of times faster than a regular CPU. END"}
{"prompt":"Why is it hard to use a traditional computer program to recognize images in a photo?\n\n###\n\n","completion":" For us humans, it is easy to identify images in a photos, such as identifying cats vs dogs in a photo. This is because, subconsciously our brains have learned which features define a cat or a dog for example. But it is hard to define set rules for a traditional computer program to recognize a cat or a dog. Can you think of a universal rule to determine if a photo contains a cat or dog? How would you encode that as a computer program? This is very difficult because cats, dogs, or other objects, have a wide variety of shapes, textures, colors, and other features, and it is close to impossible to manually encode this in a traditional computer program. END"}
{"prompt":"What term do we normally use in deep learning for what Samuel called “Weights”?\n\n###\n\n","completion":" We instead use the term parameters. In deep learning, the term “weights” has a separate meaning. (The neural network has various parameters that we fit our data to. As shown in upcoming chapters, the two types of neural network parameters are weights and biases). END"}
{"prompt":"Why is it hard to understand why a deep learning model makes a particular prediction?\n\n###\n\n","completion":" This is a highly-researched topic known as interpretability of deep learning models. Deep learning models are hard to understand in part due to their “deep” nature. Think of a linear regression model. Simply, we have some input variables\/data that are multiplied by some weights, giving us an output. We can understand which variables are more important and which are less important based on their weights. A similar logic might apply for a small neural network with 1-3 layers. However, deep neural networks have hundreds, if not thousands, of layers. It is hard to determine which factors are important in determining the final output. The neurons in the network interact with each other, with the outputs of some neurons feeding into other neurons. Altogether, due to the complex nature of deep learning models, it is very difficult to understand why a neural network makes a given prediction.\nHowever, in some cases, recent research has made it easier to better understand a neural network’s prediction. For example, as shown in this chapter, we can analyze the sets of weights and determine what kind of features activate the neurons. When applying CNNs to images, we can also see which parts of the images highly activate the model. We will see how we can make our models interpretable later in the book. END"}
{"prompt":"What is the name of the theorem that a neural network can solve any mathematical problem to any level of accuracy?\n\n###\n\n","completion":" The universal approximation theorem states that neural networks can theoretically represent any mathematical function. However, it is important to realize that practically, due to the limits of available data and computer hardware, it is impossible to practically train a model to do so. But we can get very close!. END"}
{"prompt":"What do you need in order to train a model?\n\n###\n\n","completion":" You will need an architecture for the given problem. You will need data to input to your model. For most use-cases of deep learning, you will need labels for your data to compare your model predictions to. You will need a loss function that will quantitatively measure the performance of your model. And you need a way to update the parameters of the model in order to improve its performance (this is known as an optimizer). END"}
{"prompt":"How could a feedback loop impact the rollout of a predictive policing model?\n\n###\n\n","completion":" In a predictive policing model, we might end up with a positive feedback loop, leading to a highly biased model with little predictive power. For example, we may want a model that would predict crimes, but we use information on arrests as a proxy . However, this data itself is slightly biased due to the biases in existing policing processes. Training with this data leads to a biased model. Law enforcement might use the model to determine where to focus police activity, increasing arrests in those areas. These additional arrests would be used in training future iterations of models, leading to an even more biased model. This cycle continues as a positive feedback loop. END"}
{"prompt":"Do we always have to use 224x224 pixel images with the cat recognition model?\n\n###\n\n","completion":" No we do not. 224x224 is commonly used for historical reasons. You can increase the size and get better performance, but at the price of speed and memory consumption. END"}
{"prompt":"What is the difference between classification and regression?\n\n###\n\n","completion":" Classification is focused on predicting a class or category (ex: type of pet). Regression is focused on predicting a numeric quantity (ex: age of pet). END"}
{"prompt":"What is a validation set? What is a test set? Why do we need them?\n\n###\n\n","completion":" The validation set is the portion of the dataset that is not used for training the model, but for evaluating the model during training, in order to prevent overfitting. This ensures that the model performance is not due to “cheating” or memorization of the dataset, but rather because it learns the appropriate features to use for prediction. However, it is possible that we overfit the validation data as well. This is because the human modeler is also part of the training process, adjusting hyperparameters (see question 32 for definition) and training procedures according to the validation performance. Therefore, another unseen portion of the dataset, the test set, is used for final evaluation of the model. This splitting of the dataset is necessary to ensure that the model generalizes to unseen data. END"}
{"prompt":"What will fastai do if you don’t provide a validation set?\n\n###\n\n","completion":" fastai will automatically create a validation dataset. It will randomly take 20% of the data and assign it as the validation set ( 'valid_pct' = '0.2' ). END"}
{"prompt":"Can we always use a random sample for a validation set? Why or why not?\n\n###\n\n","completion":" A good validation or test set should be representative of new data you will see in the future. Sometimes this isn’t true if a random sample is used. For example, for time series data, selecting sets randomly does not make sense. Instead, defining different time periods for the train, validation, and test set is a better approach. END"}
{"prompt":"What is overfitting? Provide an example.\n\n###\n\n","completion":" Overfitting is the most challenging issue when it comes to training machine learning models. Overfitting refers to when the model fits too closely to a limited set of data but does not generalize well to unseen data. This is especially important when it comes to neural networks, because neural networks can potentially “memorize” the dataset that the model was trained on, and will perform abysmally on unseen data because it didn’t “memorize” the ground truth values for that data. This is why a proper validation framework is needed by splitting the data into training, validation, and test sets. END"}
{"prompt":"What is a metric? How does it differ to “loss”?\n\n###\n\n","completion":" A metric is a function that measures quality of the model’s predictions using the validation set. This is similar to the ­ loss , which is also a measure of performance of the model. However, loss is meant for the optimization algorithm (like SGD) to efficiently update the model parameters, while metrics are human-interpretable measures of performance. Sometimes, a metric may also be a good choice for the loss. END"}
{"prompt":"How can pretrained models help?\n\n###\n\n","completion":" Pretrained models have been trained on other problems that may be quite similar to the current task. For example, pretrained image recognition models were often trained on the ImageNet dataset, which has 1000 classes focused on a lot of different types of visual objects. Pretrained models are useful because they have already learned how to handle a lot of simple features like edge and color detection. However, since the model was trained for a different task than already used, this model cannot be used as is. END"}
{"prompt":"What is the “head” of a model?\n\n###\n\n","completion":" 'crop' is the default 'Resize()' method, and it  crops  the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. For instance, if we were trying to recognize the breed of dog or cat, we may end up cropping out a key part of the body or the face necessary to distinguish between similar breeds.\n'pad' is an alternative 'Resize()' method, which pads the matrix of the image’s pixels with zeros (which shows as black when viewing the images). If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model, and results in a lower effective resolution for the part of the image we actually use.\n\n'squish' is another alternative 'Resize()' method, which can either squish or stretch the image. This can cause the image to take on an unrealistic shape, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy.\n\nWhich resizing method to use therefore depends on the underlying problem and dataset. For example, if the features in the dataset images take up the whole image and cropping may result in loss of information, squishing or padding may be more useful.\nAnother better method is 'RandomResizedCrop', in which we crop on a randomly selected region of the image. So every epoch, the model will see a different part of the image and will learn accordingly. END"}
{"prompt":"What kinds of features do the early layers of a CNN find? How about the later layers?\n\n###\n\n","completion":" Earlier layers learn simple features like diagonal, horizontal, and vertical edges. Later layers learn more advanced features like car wheels, flower petals, and even outlines of animals. END"}
{"prompt":"Are image models only useful for photos?\n\n###\n\n","completion":" No. Image models can be useful for other types of images like sketches, medical data, etc.\nHowever, a lot of information can be represented as images . For example, a sound can be converted into a spectrogram, which is a visual interpretation of the audio. Time series (ex: financial data) can be converted to image by plotting on a graph. Even better, there are various transformations that generate images from time series, and have achieved good results for time series classification. There are many other examples, and by being creative, it may be possible to formulate your problem as an image classification problem, and use pretrained image models to obtain state-of-the-art results!. END"}
{"prompt":"What is an “architecture”?\n\n###\n\n","completion":" The architecture is the template or structure of the model we are trying to fit. It defines the mathematical model we are trying to fit. END"}
{"prompt":"What is segmentation?\n\n###\n\n","completion":" At its core, segmentation is a pixelwise classification problem. We attempt to predict a label for every single pixel in the image. This provides a mask for which parts of the image correspond to the given label. END"}
{"prompt":"What is y_range used for? When do we need it?\n\n###\n\n","completion":" y_range is being used to limit the values predicted when our problem is focused on predicting a numeric value in a given range (ex: predicting movie ratings, range of 0.5-5). END"}
{"prompt":"What are “hyperparameters”?\n\n###\n\n","completion":" Training models requires various other parameters that define how the model is trained. For example, we need to define how long we train for, or what learning rate (how fast the model parameters are allowed to change) is used. These sorts of parameters are hyperparameters. END"}
{"prompt":"What’s the best way to avoid failures when using AI in an organization?\n\n###\n\n","completion":" Key things to consider when using AI in an organization:\n\nMake sure a training, validation, and testing set is defined properly in order to evaluate the model in an appropriate manner.\nTry out a simple baseline, which future models should hopefully beat. Or even this simple baseline may be enough in some cases. END"}
{"prompt":"Where do text models currently have a major deficiency?\n\n###\n\n","completion":" Text models can generate context-appropriate text (like replies or imitating author style). However, text models still struggle with correct responses. Given factual information (such as a knowledge base), it is still hard to generate responses that utilizes this information to generate factually correct responses, though the text can seem very compelling. This can be very dangerous, as the layman may not be able to evaluate the factual accuracy of the generated text. END"}
{"prompt":"What are the possible negative societal implications of text generation models?\n\n###\n\n","completion":" The ability for text generation models to generate context-aware, highly compelling responses can be used at a massive scale to spread disinformation (“fake news”) and encourage conflict.\nModels reinforce bias (like gender bias, racial bias) in training data and create a vicious cycle of biased outputs. END"}
{"prompt":"In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\n\n###\n\n","completion":" The predictions of the model could be reviewed by human experts for them to evaluate the results and determine what is the best next step. This is especially true for applying machine learning for medical diagnoses. For example, a machine learning model for identifying strokes in CT scans can alert high priority cases for expedited review, while other cases are still sent to radiologists for review. Or other models can also augment the medical professional’s abilities, reducing risk but still improving efficiency of the workflow. For example, deep learning models can provide useful measurements for radiologists or pathologists. END"}
{"prompt":"What kind of tabular data is deep learning particularly good at?\n\n###\n\n","completion":" Deep learning is good at analyzing tabular data that includes natural language, or high cardinality categorical columns (containing larger number of discrete choices like zip code). END"}
{"prompt":"What’s a key downside of directly using a deep learning model for recommendation systems?\n\n###\n\n","completion":" Machine learning approaches for recommendation systems will often only tell what products a user might like, and may not be recommendations that would be helpful to the user. For example, if a user is familiar with other books from the same author, it isn’t helpful to recommend those products even though the user bought the author’s book. Or, recommending products a user may have already purchased. END"}
{"prompt":"How do the steps of the Drivetrain approach map to a recommendation system?\n\n###\n\n","completion":" The  objective  of a recommendation engine is to drive additional sales by surprising and delighting the customer with recommendations of items they would not have purchased without the recommendation. The  lever  is the ranking of the recommendations. New  data  must be collected to generate recommendations that will  cause new sales . This will require conducting many randomized experiments in order to collect data about a wide range of recommendations for a wide range of customers. This is a step that few organizations take; but without it, you don’t have the information you need to actually optimize recommendations based on your true objective (more sales!). END"}
{"prompt":"What is 'DataLoaders'?\n\n###\n\n","completion":" The 'DataLoaders' class is the class that passes the data to the fastai model. It is essentially a class that stores the required 'Dataloader' objects (usually for train and validation sets). END"}
{"prompt":"How do we ensure a random split always gives the same validation set?\n\n###\n\n","completion":" It turns out it is impossible for our computers to generate truly random numbers. Instead, they use a process known as a pseudo-random generator. However, this process can be controlled using a random seed. By setting a random seed value, the pseudo-random generator will generate the “random” numbers in a fixed manner and it will be the same for every run. Using a random seed, we can generate a random split that gives the same validation set always. END"}
{"prompt":"What letters are often used to signify the independent and dependent variables?\n\n###\n\n","completion":" `x` is independent. `y` is dependent. END"}
{"prompt":"What’s the difference between 'crop', 'pad', and 'squish' 'Resize()' approaches? When might you choose one over the other?\n\n###\n\n","completion":" crop' is the default 'Resize()' method, and it  crops  the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. For instance, if we were trying to recognize the breed of dog or cat, we may end up cropping out a key part of the body or the face necessary to distinguish between similar breeds.\n'pad' is an alternative 'Resize()' method, which pads the matrix of the image’s pixels with zeros (which shows as black when viewing the images). If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model, and results in a lower effective resolution for the part of the image we actually use.\n\n'squish' is another alternative 'Resize()' method, which can either squish or stretch the image. This can cause the image to take on an unrealistic shape, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy.\n\nWhich resizing method to use therefore depends on the underlying problem and dataset. For example, if the features in the dataset images take up the whole image and cropping may result in loss of information, squishing or padding may be more useful.\nAnother better method is 'RandomResizedCrop', in which we crop on a randomly selected region of the image. So every epoch, the model will see a different part of the image and will learn accordingly. END"}
{"prompt":"What is data augmentation? Why is it needed?\n\n###\n\n","completion":" Data augmentation refers to creating random variations of our input data, such that they appear different, but not so different that it changes the meaning of the data. Examples include flipping, rotation, perspective warping, brightness changes, etc. Data augmentation is useful for the model to better understand the basic concept of what an object is and how the objects of interest are represented in images. Therefore, data augmentation allows machine learning models to generalize . This is especially important when it can be slow and expensive to label data. END"}
{"prompt":"What is the difference between 'item_tfms' and 'batch_tfms'?\n\n###\n\n","completion":" 'item_tfms' are transformations applied to a single data sample 'x' on the CPU. 'Resize()' is a common transform because the mini-batch of input images to a cnn must have the same dimensions. Assuming the images are RGB with 3 channels, then 'Resize'() as item_tfms will make sure the images have the same width and height.\n'batch_tfms' are applied to batched data samples (aka individual samples that have been collated into a mini-batch) on the GPU. They are faster and more efficient than 'item_tfms'. A good example of these are the ones provided by 'aug_transforms()'. Inside are several batch-level augmentations that help many models. END"}
{"prompt":"What is a confusion matrix?\n\n###\n\n","completion":" A class confusion matrix is a representation of the predictions made vs the correct labels. The rows of the matrix represent the actual labels while the columns represent the predictions. Therefore, the number of images in the diagonal elements represent the number of correctly classified images, while the off-diagonal elements are incorrectly classified images. Class confusion matrices provide useful information about how well the model is doing and which classes the model might be confusing . END"}
{"prompt":"What does 'export' save do?\n\n###\n\n","completion":" 'export' saves both the architecture, as well as the trained parameters of the neural network architecture. It also saves how the 'DataLoaders' are defined. END"}
{"prompt":"What are IPython widgets?\n\n###\n\n","completion":" IPython widgets are JavaScript and Python combined functionalities that let us build and interact with GUI components directly in a Jupyter notebook. An example of this would be an upload button, which can be created with the Python function 'widgets.FileUpload()'. END"}
{"prompt":"When might you want to use CPU for deployment? When might GPU be better?\n\n###\n\n","completion":" GPUs are best for doing identical work in parallel. If you will be analyzing single pieces of data at a time (like a single image or single sentence), then CPUs may be more cost effective instead, especially with more market competition for CPU servers versus GPU servers. GPUs could be used if you collect user responses into a batch at a time, and perform inference on the batch. This may require the user to wait for model predictions. Additionally, there are many other complexities when it comes to GPU inference, like memory management and queuing of the batches. END"}
{"prompt":"What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?\n\n###\n\n","completion":" The application will require network connection, and there will be extra network latency time when submitting input and returning results. Additionally, sending private data to a network server can lead to security concerns.\nOn the flip side deploying a model to a server makes it easier to iterate and roll out new versions of a model. This is because you as a developer have full control over the server environment and only need to do it once rather than having to make sure that all the endpoints (phones, PCs) upgrade their version individually. END"}
{"prompt":"What is “out of domain data”?\n\n###\n\n","completion":" “Out of domain data” is data that is fundamentally different in some aspect compared to the model’s training data. For example, an object detector that was trained exclusively with outside daytime photos is given a photo taken at night. END"}
{"prompt":"What is “domain shift”?\n\n###\n\n","completion":" “Domain shift” is when the type of data changes gradually over time. For example, an insurance company is using a deep learning model as part of their pricing algorithm, but over time their customers will be different, with the original training data not being representative of current data, and the deep learning model being applied on effectively out-of-domain data. END"}
{"prompt":"What are the 3 steps in the deployment process?\n\n###\n\n","completion":" Manual process – the model is run in parallel and not directly driving any actions, with humans still checking the model outputs.\nLimited scope deployment – The model’s scope is limited and carefully supervised. For example, doing a geographically and time-constrained trial of model deployment, that is carefully supervised.\nGradual expansion – The model scope is gradually increased, while good reporting systems are implemented in order to check for any significant changes to the actions taken compared to the manual process (i.e. the models should perform similarly to the humans, unless it is already anticipated to be better). END"}
{"prompt":"What was the problem with a database of suspected gang members maintained by California law enforcement officials?\n\n###\n\n","completion":" A database of suspected gang members maintained by California law enforcement officials was found to be full of errors, including 42 babies who had been added to the database when they were less than 1 year old (28 of whom were marked as “admitting to being gang members”). In this case, there was no process in place for correcting mistakes or removing people once they’d been added. END"}
{"prompt":"Why did YouTube’s recommendation algorithm recommend videos of partially clothed children to pedophiles, even although no employee at Google programmed this feature?\n\n###\n\n","completion":" The problem here is the centrality of metrics in driving a financially important system. When an algorithm has a metric to optimise, as you have seen, it will do everything it can to optimise that number. This tends to lead to all kinds of edge cases, and humans interacting with a system will search for, find, and exploit these edge cases and feedback loops for their advantage. END"}
{"prompt":"What are the problems with the centrality of metrics?\n\n###\n\n","completion":" When an algorithm has a metric to optimise, as you have seen, it will do everything it can to optimise that number. This tends to lead to all kinds of edge cases, and humans interacting with a system will search for, find, and exploit these edge cases and feedback loops for their advantage. END"}
{"prompt":"How is a greyscale image represented on a computer? How about a color image?\n\n###\n\n","completion":" Images are represented by arrays with pixel values representing the content of the image. For greyscale images, a 2-dimensional array is used with the pixels representing the greyscale values, with a range of 256 integers. A value of 0 would represent white, and a value of 255 represents black, and different shades of greyscale in between. For color images, three color channels (red, green, blue) are typicall used, with a separate 256-range 2D array used for each channel. A pixel value of 0 again represents white, with 255 representing solid red, green, or blue. The three 2-D arrays form a final 3-D array (rank 3 tensor) representing the color image. END"}
{"prompt":"How are the files and folders in the  'MNIST_SAMPLE'  dataset structured? Why?\n\n###\n\n","completion":" There are two subfolders, 'train' and 'valid', the former contains the data for model training, the latter contains the data for validating model performance after each training step. Evaluating the model on the validation set serves two purposes: a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training), b) to facilitate the detection of overfitting by evaluating the model on a dataset it hasn’t been trained on (in short, an overfitting model performs increasingly well on the training set but decreasingly so on the validation set). Of course, every practicioner could generate their own train\/validation-split of the data. Public datasets are usually pre-split to simplifiy comparing results between implementations\/publications.\nEach subfolder has two subsubfolders '3' and '7' which contain the '.jpg' files for the respective class of images. This is a common way of organizing datasets comprised of pictures. For the full 'MNIST' dataset there are 10 subsubfolders, one for the images for each digit. END"}
{"prompt":"Explain how the “pixel similarity” approach to classifying digits works.\n\n###\n\n","completion":" In the “pixel similarity” approach, we generate an archetype for each class we want to identify. In our case, we want to distinguish images of 3’s from images of 7’s. We define the archetypical 3 as the pixel-wise mean value of all 3’s in the training set. Analoguously for the 7’s. You can visualize the two archetypes and see that they are in fact blurred versions of the numbers they represent.\nIn order to tell if a previously unseen image is a 3 or a 7, we calculate its distance to the two archetypes (here: mean pixel-wise absolute difference). We say the new image is a 3 if its distance to the archetypical 3 is lower than two the archetypical 7. END"}
{"prompt":"What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n\n###\n\n","completion":" Lists (arrays in other programming languages) are often generated using a 'for'-loop. A list comprehension is a Pythonic way of condensing the creation of a list using a 'for'-loop into a single expression. List comprehensions will also often include if clauses for filtering.\n```lst_in = range(10)\nlst_out = [2*el for el in lst_in if el%2==1]\n# is equivalent to:\nlst_out = []\nfor el in lst_in:\n   if el%2==1:\n       lst_out.append(2*el)\n``` END"}
{"prompt":"What is a “rank 3 tensor”?\n\n###\n\n","completion":" The rank of a tensor is the number of dimensions it has. An easy way to identify the rank is the number of indices you would need to reference a number within a tensor. A scalar can be represented as a tensor of rank 0 (no index), a vector can be represented as a tensor of rank 1 (one index, e.g., 'v[i]'), a matrix can be represented as a tensor of rank 2 (two indices, e.g., 'a[i,j]'), and a tensor of rank 3 is a cuboid or a “stack of matrices” (three indices, e.g., 'b[i,j,k]'). In particular, the rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3.\nNote that the term “rank” has different meanings in the context of tensors and matrices (where it refers to the number of linearly independent column vectors). END"}
{"prompt":"What is the difference between tensor rank and shape?\n\n###\n\n","completion":" Rank is the number of axes or dimensions in a tensor; shape is the size of each axis of a tensor.\nHow do you get the rank from the shape?\nThe length of a tensor’s shape is its rank.\nSo if we have the images of  the 3 folder from the MINST_SAMPLE dataset in a tensor called 'stacked_threes' and we find its shape like this.\n```In [ ]: stacked_threes.shape\nOut[ ]: torch.Size([6131, 28, 28])\n```\nWe just need to find its length to know its rank. This is done as follows.\n```In [ ]: len(stacked_threes.shape)\nOut[ ]: 3\n```\nYou can also get a tensor’s rank directly with 'ndim' .\n```In [ ]: stacked_threes.ndim\nOut[ ]: 3\n``` END"}
{"prompt":"What are RMSE and L1 norm?\n\n###\n\n","completion":" Root mean square error (RMSE), also called the L2 norm, and mean absolute difference (MAE), also called the L1 norm, are two commonly used methods of measuring “distance”. Simple differences do not work because some difference are positive and others are negative, canceling each other out. Therefore, a function that focuses on the magnitudes of the differences is needed to properly measure distances. The simplest would be to add the absolute values of the differences, which is what MAE is. RMSE takes the mean of the square (makes everything positive) and then takes the square root (undoes squaring). END"}
{"prompt":"How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\n\n###\n\n","completion":" As loops are very slow in Python, it is best to represent the operations as array operations rather than looping through individual elements. If this can be done, then using NumPy or PyTorch will be thousands of times faster, as they use underlying C code which is much faster than pure Python. Even better, PyTorch allows you to run operations on GPU, which will have significant speedup if there are parallel operations that can be done. END"}
{"prompt":"Create a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom right 4 numbers.\n\n###\n\n","completion":" ```In [ ]: a = torch.Tensor(list(range(1,10))).view(3,3); print(a)\n    Out [ ]: tensor([[1., 2., 3.],\n                     [4., 5., 6.],\n                     [7., 8., 9.]])\n    In [ ]: b = 2*a; print(b)\n    Out [ ]: tensor([[ 2.,  4.,  6.],\n                     [ 8., 10., 12.],\n                     [14., 16., 18.]])\n    In [ ]:  b[1:,1:]\n    Out []: tensor([[10., 12.],\n                    [16., 18.]])\n``` END"}
{"prompt":"What is broadcasting?\n\n###\n\n","completion":" Scientific\/numerical Python packages like NumPy and PyTorch will often implement broadcasting that often makes code easier to write. In the case of PyTorch, tensors with smaller rank are expanded to have the same size as the larger rank tensor. In this way, operations can be performed between tensors with different rank. END"}
{"prompt":"Are metrics generally calculated using the training set, or the validation set? Why?\n\n###\n\n","completion":" Metrics are generally calculated on a validation set. As the validation set is unseen data for the model, evaluating the metrics on the validation set is better in order to determine if there is any overfitting and how well the model might generalize if given similar data. END"}
{"prompt":"What is SGD?\n\n###\n\n","completion":" SGD, or stochastic gradient descent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the predictions and target. The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function. This is what SGD does. END"}
{"prompt":"Why does SGD use mini batches?\n\n###\n\n","completion":" We need to calculate our loss function (and our gradient) on one or more data points. We cannot calculate on the whole datasets due to compute limitations and time constraints. If we iterated through each data point, however, the gradient will be unstable and imprecise, and is not suitable for training. As a compromise, we calculate the average loss for a small subset of the dataset at a time. This subset is called a mini-batch. Using mini-batches are also more computationally efficient than single items on a GPU. END"}
{"prompt":"What are the 7 steps in SGD for machine learning?\n\n###\n\n","completion":" Initialize the parameters – Random values often work best.\nCalculate the predictions – This is done on the training set, one mini-batch at a time.\nCalculate the loss – The average loss over the minibatch is calculated\nCalculate the gradients – this is an approximation of how the parameters need to change in order to minimize the loss function\nStep the weights – update the parameters based on the calculated weights\nRepeat the process\nStop – In practice, this is either based on time constraints or usually based on when the training\/validation losses and metrics stop improving. END"}
{"prompt":"How do we initialize the weights in a model?\n\n###\n\n","completion":" Random weights work pretty well. END"}
{"prompt":"What is “loss”?\n\n###\n\n","completion":" The loss function will return a value based on the given predictions and targets, where lower values correspond to better model predictions. END"}
{"prompt":"Why can’t we always use a high learning rate?\n\n###\n\n","completion":" The loss may “bounce” around (oscillate) or even diverge, as the optimizer is taking steps that are too large, and updating the parameters faster than it should be. END"}
{"prompt":"What is a “gradient”?\n\n###\n\n","completion":" The gradients tell us how much we have to change each weight to make our model better. It is essentially a measure of how the loss function changes with changes of the weights of the model (the derivative). END"}
{"prompt":"Do you need to know how to calculate gradients yourself?\n\n###\n\n","completion":" Manual calculation of the gradients are not required, as deep learning libraries will automatically calculate the gradients for you. This feature is known as automatic differentiation. In PyTorch, if 'requires_grad=True', the gradients can be returned by calling the backward method: 'a.backward()'. END"}
{"prompt":"Why can’t we use accuracy as a loss function?\n\n###\n\n","completion":" A loss function needs to change as the weights are being adjusted. Accuracy only changes if the predictions of the model change. So if there are slight changes to the model that, say, improves confidence in a prediction, but does not change the prediction, the accuracy will still not change. Therefore, the gradients will be zero everywhere except when the actual predictions change. The model therefore cannot learn from the gradients equal to zero, and the model’s weights will not update and will not train. A good loss function gives a slightly better loss when the model gives slightly better predictions. Slightly better predictions mean if the model is more confident about the correct prediction. For example, predicting 0.9 vs 0.7 for probability that a MNIST image is a 3 would be slightly better prediction. The loss function needs to reflect that. END"}
{"prompt":"Draw the sigmoid function. What is special about its shape?\n\n###\n\n","completion":" Sigmoid function is a smooth curve that squishes all values into values between 0 and 1. Most loss functions assume that the model is outputting some form of a probability or confidence level between 0 and 1 so we use a sigmoid function at the end of the model in order to do this. END"}
{"prompt":"What is the difference between loss and metric?\n\n###\n\n","completion":" The key difference is that metrics drive human understanding and losses drive automated learning. In order for loss to be useful for training, it needs to have a meaningful derivative. Many metrics, like accuracy are not like that. Metrics instead are the numbers that humans care about, that reflect the performance of the model. END"}
{"prompt":"What is the function to calculate new weights using a learning rate?\n\n###\n\n","completion":" The optimizer step function. END"}
{"prompt":"What does the DataLoader class do?\n\n###\n\n","completion":" The DataLoader class can take any Python collection and turn it into an iterator over many batches. END"}
{"prompt":"Write pseudo-code showing the basic steps taken each epoch for SGD.\n\n###\n\n","completion":" ```for x,y in dl:\n   pred = model(x)\n   loss = loss_func(pred, y)\n   loss.backward()\n   parameters -= parameters.grad * lr\n``` END"}
{"prompt":"Create a function which, if passed two arguments [1,2,3,4] and ‘abcd’ , returns [(1, ‘a’), (2, ‘b’), (3, ‘c’), (4, ‘d’)] . What is special about that output data structure?\n\n###\n\n","completion":" ```def func(a,b): return list(zip(a,b))\n```\nThis data structure is useful for machine learning models when you need lists of tuples where each tuple would contain input data and a label. END"}
{"prompt":"What does view do in PyTorch?\n\n###\n\n","completion":" It changes the shape of a Tensor without changing its contents. END"}
{"prompt":"What are the “bias” parameters in a neural network? Why do we need them?\n\n###\n\n","completion":" Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model. END"}
{"prompt":"What does the @ operator do in python?\n\n###\n\n","completion":" This is the matrix multiplication operator. END"}
{"prompt":"What does the backward method do?\n\n###\n\n","completion":" This method returns the current gradients. END"}
{"prompt":"Why do we have to zero the gradients?\n\n###\n\n","completion":" PyTorch will add the gradients of a variable to any previously stored gradients. If the training loop function is called multiple times, without zeroing the gradients, the gradient of current 'loss' would be added to the previously stored gradient value. END"}
{"prompt":"What information do we have to pass to Learner ?\n\n###\n\n","completion":" We need to pass in the DataLoaders, the model, the optimization function, the loss function, and optionally any metrics to print. END"}
{"prompt":"Show python or pseudo-code for the basic steps of a training loop.\n\n###\n\n","completion":" ```def train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\nfor i in range(20):\n    train_epoch(model, lr, params)\n``` END"}
{"prompt":"What is “ReLU”? Draw a plot of it for values from -2 to +2 .\n\n###\n\n","completion":" ReLU just means “replace any negative numbers with zero”. It is a commonly used activation function. END"}
{"prompt":"What is an “activation function”?\n\n###\n\n","completion":" The activation function is another function that is part of the neural network, which has the purpose of providing non-linearity to the model. The idea is that without an activation function, we just have multiple linear functions of the form 'y=mx+b'. However, a series of linear layers is equivalent to a single linear layer, so our model can only fit a line to the data. By introducing a non-linearity in between the linear layers, this is no longer true. Each layer is somewhat decoupled from the rest of the layers, and the model can now fit much more complex functions. In fact, it can be mathematically proven that such a model can solve any computable problem to an arbitrarily high accuracy, if the model is large enough with the correct weights. This is known as the universal approximation theorem. END"}
{"prompt":"What’s the difference between F.relu and nn.ReLU ?\n\n###\n\n","completion":" F.relu is a Python function for the relu activation function. On the other hand, nn.ReLU is a PyTorch module. This means that it is a Python class that can be called as a function in the same way as F.relu. END"}
{"prompt":"The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\n\n###\n\n","completion":" There are practical performance benefits to using more than one nonlinearity. We can use a deeper model with less number of parameters, better performance, faster training, and less compute\/memory requirements. END"}
{"prompt":"Why do we first resize to a large size on the CPU, and then to a smaller size on the GPU?\n\n###\n\n","completion":" This concept is known as presizing. Data augmentation is often applied to the images and in fastai it is done on the GPU. However, data augmentation can lead to degradation and artifacts, especially at the edges. Therefore, to minimize data destruction, the augmentations are done on a larger image, and then RandomResizeCrop is performed to resize to the final image size. END"}
{"prompt":"What are the two ways in which data is most commonly provided, for most deep learning datasets?\n\n###\n\n","completion":" Individual files representing items of data, such as text documents or images.\nA table of data, such as in CSV format, where each row is an item, each row which may include filenames providing a connection between the data in the table and data in other formats such as text documents and images. END"}
{"prompt":"Give two examples of ways that image transformations can degrade the quality of the data.\n\n###\n\n","completion":" Rotation can leave empty areas in the final image\nOther operations may require interpolation which is based on the original image pixels but are still of lower image quality. END"}
{"prompt":"Should you hold off on training a model until you have thoroughly cleaned your data?\n\n###\n\n","completion":" No. It is best to create a baseline model as soon as possible. END"}
{"prompt":"What are the two pieces that are combined into cross entropy loss in PyTorch?\n\n###\n\n","completion":" Cross Entropy Loss is a combination of a Softmax function and Negative Log Likelihood Loss. END"}
{"prompt":"What are the two properties of activations that softmax ensures? Why is this important?\n\n###\n\n","completion":" It makes the outputs for the classes add up to one. This means the model can only predict one class. Additionally, it amplifies small changes in the output activations, which is helpful as it means the model will select a label with higher confidence (good for problems with definite labels). END"}
{"prompt":"When might you want your activations to not have these two properties?\n\n###\n\n","completion":" When you have multi-label classification problems (more than one label possible). END"}
{"prompt":"Why can’t we use torch.where to create a loss function for datasets where our label can have more than two categories?\n\n###\n\n","completion":" Because 'torch.where' can only select between two possibilities while for multi-class classification, we have multiple possibilities. END"}
{"prompt":"What is the value of log(-2)? Why?\n\n###\n\n","completion":" This value is not defined. The logarithm is the inverse of the exponential function, and the exponential function is always positive no matter what value is passed. So the logarithm is not defined for negative values. END"}
{"prompt":"What are two good rules of thumb for picking a learning rate from the learning rate finder?\n\n###\n\n","completion":" Either one of these two points should be selected for the learning rate:\n\n\none order of magnitude less than where the minimum loss was achieved (i.e. the minimum divided by 10)\n\n\nthe last point where the loss was clearly decreasing. END"}
{"prompt":"What two steps does the fine_tune method do?\n\n###\n\n","completion":" Train the new head (with random weights) for one epoch\nUnfreeze all the layers and train them all for the requested number of epochs. END"}
{"prompt":"In Jupyter notebook, how do you get the source code for a method or function?\n\n###\n\n","completion":" Use '??' after the function ex: 'DataBlock.summary??'. END"}
{"prompt":"What are discriminative learning rates?\n\n###\n\n","completion":" Discriminative learning rates refers to the training trick of using different learning rates for different layers of the model. This is commonly used in transfer learning. The idea is that when you train a pretrained model, you don’t want to drastically change the earlier layers as it contains information regarding simple features like edges and shapes. But later layers may be changed a little more as it may contain information regarding facial feature or other object features that may not be relevant to your task. Therefore, the earlier layers have a lower learning rate and the later layers have higher learning rates. END"}
{"prompt":"How is a Python slice object interpreted when passed as a learning rate to fastai?\n\n###\n\n","completion":" The first value of the slice object is the learning rate for the earliest layer, while the second value is the learning rate for the last layer. The layers in between will have learning rates that are multiplicatively equidistant throughout that range. END"}
{"prompt":"Why is early stopping a poor choice when using one cycle training?\n\n###\n\n","completion":" If early stopping is used, the training may not have time to reach lower learning rate values in the learning rate schedule, which could easily continue to improve the model. Therefore, it is recommended to retrain the model from scratch and select the number of epochs based on where the previous best results were found. END"}
{"prompt":"What is the difference between resnet 50 and resnet101?\n\n###\n\n","completion":" The number 50 and 101 refer to the number of layers in the models. Therefore, ResNet101 is a larger model with more layers versus ResNet50. These model variants are commonly as there are ImageNet-pretrained weights available. END"}
{"prompt":"What does to_fp16 do?\n\n###\n\n","completion":" This enables mixed-precision training, in which less precise numbers are used in order to speed up training. END"}
{"prompt":"how could multi-label classification improve the usability of the bear classifier?\n\n###\n\n","completion":" This would allow for the classification of no bears present. Otherwise, a mutli-class classification model will predict the presence of a bear even if it’s not there (unless a separate class is explicitly added). END"}
{"prompt":"How do we encode the dependent variable in a multi-label classification problem?\n\n###\n\n","completion":" This is encoded as a one-hot encoded vector. Essentially, this means we have a zero vector of the same length of the number of classes, and ones are present at the indices for the classes that are present in the data. END"}
{"prompt":"How do you access the rows and columns of a DataFrame as if it was a matrix?\n\n###\n\n","completion":" You can use '.iloc'. For example, 'df.iloc[10,10]' will select the element in the 10th row and 10th column as if the DataFrame is a matrix. END"}
{"prompt":"How do you get a column by name from a DataFrame\n\n###\n\n","completion":" This is very simple. You can just index it! Ex: 'df['column_name']'. END"}
{"prompt":"What is the difference between a dataset and DataLoader?\n\n###\n\n","completion":" A 'Dataset' is a collection which returns a tuple of your independent and dependent variable for a single item. 'DataLoader' is an extension of the 'Dataset' functionality. It is an iterator which provides a stream of mini batches, where each mini batch is a couple of a batch of independent variables and a batch of dependent variables. END"}
{"prompt":"What does a Datasets object normally contain?\n\n###\n\n","completion":" A training set and validation set. END"}
{"prompt":"What does a DataLoaders object normally contain?\n\n###\n\n","completion":" A trainin dataloader and validation dataloader. END"}
{"prompt":"What does lambda do in Python?\n\n###\n\n","completion":" Lambdas are shortcuts for writing functions (writing one-liner functions). It is great for quick prototyping and iterating, but since it is not serializable, it cannot be used in deployment and production. END"}
{"prompt":"What are the methods to customise how the independent and dependent variables are created with the data block API?\n\n###\n\n","completion":" get_x and get_y\n\nget_x is used to specify how the independent variables are created.\nget_y is used to specify how the datapoints are labelled. END"}
{"prompt":"Why is softmax not an appropriate output activation function when using a one hot encoded target?\n\n###\n\n","completion":" Softmax wants to make the model predict only a single class, which may not be true in a multi-label classification problem. In multi-label classification problems, the input data could have multiple labels or even no labels. END"}
{"prompt":"Why is nll_loss not an appropriate loss function when using a one hot encoded target?\n\n###\n\n","completion":" Again, nll_loss only works for when the model only needs to predict one class, which is not the case here. END"}
{"prompt":"What is the difference between nn.BCELoss and nn.BCEWithLogitsLoss?\n\n###\n\n","completion":" nn.BCELoss does not include the initial sigmoid. It assumes that the appropriate activation function (ie. the sigmoid) has already been applied to the predictions. nn.BCEWithLogitsLoss, on the other hand, does both the sigmoid and cross entropy in a single function. END"}
{"prompt":"Why can’t we use regular accuracy in a multi-label problem?\n\n###\n\n","completion":" The regular accuracy function assumes that the final model-predicted class is the one with the highest activation. However, in multi-label problems, there can be multiple labels. Therefore, a threshold for the activations needs to be set for choosing the final predicted classes based on the activations, for comparing to the target claases. END"}
{"prompt":"When is it okay to tune an hyper-parameter on the validation set?\n\n###\n\n","completion":" It is okay to do so when the relationship between the hyper-parameter and the metric being observed is smooth. With such a smooth relationship, we would not be picking an inappropriate outlier. END"}
{"prompt":"How is y_range implemented in fastai? (See if you can implement it yourself and test it without peaking!)\n\n###\n\n","completion":" y_range is implemented using sigmoid_range in fastai.\n'def sigmoid_range(x, lo, hi): return x.sigmoid() * (hi-lo) + lo'. END"}
{"prompt":"What is a regression problem? What loss function should you use for such a problem?\n\n###\n\n","completion":" In a regression problem, the dependent variable or labels we are trying to predict are continuous values. For such problems, the mean squared error loss function is used. END"}
{"prompt":"What do you need to do to make sure the fastai library applies the same data augmentation to your inputs images and your target point coordinates?\n\n###\n\n","completion":" You need to use the correct DataBlock. In this case, it is the 'PointBlock'. This DataBlock automatically handles the application data augmentation to the input images and the target point coordinates. END"}
{"prompt":"What problem does collaborative filtering solve?\n\n###\n\n","completion":" It solves the problem of predicting the interests of users based on the interests of other users and recommending items based on these interests. END"}
{"prompt":"How does it solve it?\n\n###\n\n","completion":" The key idea of collaborative filtering is 'latent factors'. The idea is that the model can tell what kind of items you may like (ex: you like sci-fi movies\/books) and these kinds of factors are learned (via basic gradient descent) based on what items other users like. END"}
{"prompt":"Why might a collaborative filtering predictive model fail to be a very useful recommendation system?\n\n###\n\n","completion":" If there are not many recommendations to learn from, or enough data about the user to provide useful recommendations, then such collaborative filtering systems may not be useful. END"}
{"prompt":"What does a crosstab representation of collaborative filtering data look like?\n\n###\n\n","completion":" In the crosstab representation, the users and items are the rows and columns (or vice versa) of a large matrix with the values filled out based on the user’s rating of the item. END"}
{"prompt":"What is a latent factor? Why is it “latent”?\n\n###\n\n","completion":" As described above, a latent factor are factors that are important for the prediction of the recommendations, but are not explicitly given to the model and instead learned (hence “latent”). END"}
{"prompt":"What is a dot product? Calculate a dot product manually using pure python with lists.\n\n###\n\n","completion":" A dot product is when you multiply the corresponding elements of two vectors and add them up. If we represent the vectors as lists of the same size, here is how we can perform a dot product:\n```a = [1, 2, 3, 4]\nb = [5, 6, 7, 8]\ndot_product = sum(i[0]*i[1] for i in zip(a,b))\n``` END"}
{"prompt":"What does  'pandas.DataFrame.merge'  do?\n\n###\n\n","completion":" It allows you to merge DataFrames into one DataFrame. END"}
{"prompt":"What is an embedding matrix?\n\n###\n\n","completion":" It is what you multiply an embedding with, and in the case of this collaborative filtering problem, is learned through training. END"}
{"prompt":"What is the relationship between an embedding and a matrix of one-hot encoded vectors?\n\n###\n\n","completion":" An embedding is a matrix of one-hot encoded vectors that is computationally more efficient. END"}
{"prompt":"Why do we need  'Embedding'  if we could use one-hot encoded vectors for the same thing?\n\n###\n\n","completion":" 'Embedding' is computationally more efficient. The multiplication with one-hot encoded vectors is equivalent to indexing into the embedding matrix, and the 'Embedding' layer does this. However, the gradient is calculated such that it is equivalent to the multiplication with the one-hot encoded vectors. END"}
{"prompt":"What does an embedding contain before we start training (assuming we’re not using a prertained model)?\n\n###\n\n","completion":" The embedding is randomly initialized. END"}
{"prompt":"Rewrite the  'DotProduct'  class (without peeking, if possible!) and train a model with it\n\n###\n\n","completion":" Code provided in chapter:\n```class DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n``` END"}
{"prompt":"What is a good loss function to use for MovieLens? Why?\n\n###\n\n","completion":" We can use Mean Squared Error (MSE), which is a perfectly reasonable loss as we have numerical targets for the ratings and it is one possible way of representing the accuracy of the model. END"}
{"prompt":"What would happen if we used  'CrossEntropy'  loss with MovieLens? How would we need to change the model?\n\n###\n\n","completion":" We would need to ensure the model outputs 5 predictions. For example, with a neural network model, we need to change the last linear layer to output 5, not 1, predictions. Then this is passed into the Cross Entropy loss. END"}
{"prompt":"What is the use of bias in a dot product model?\n\n###\n\n","completion":" A bias will compensate for the fact that some movies are just amazing or pretty bad. It will also compensate for users who often have more positive or negative recommendations in general. END"}
{"prompt":"What is another name for weight decay?\n\n###\n\n","completion":" L2 regularization. END"}
{"prompt":"Write the equation for weight decay.\n\n###\n\n","completion":" 'loss_with_wd = loss + wd * (parameters**2).sum() '. END"}
{"prompt":"Write the equation for the gradient of weight decay. Why does it help reduce weights?\n\n###\n\n","completion":" We add to the gradients '2*wd*parameters'. This helps create more shallow, less bumpy\/sharp surfaces that generalize better and prevents overfitting. END"}
{"prompt":"What is the “bootstrapping problem” in collaborative filtering?\n\n###\n\n","completion":" That the model \/ system cannot make any recommendations or draw any inferences for users or items about which it has not yet gathered sufficient information. It’s also called the cold start problem. END"}
{"prompt":"How could you deal with the bootstrapping problem for new users? For new movies?\n\n###\n\n","completion":" You could solve this by coming up with an average embedding for a user or movie. Or select a particular user\/movie to represent the average user\/movie. Additionally, you could come up with some questions that could help initialize the embedding vectors for new users and movies. END"}
{"prompt":"How can feedback loops impact collaborative filtering systems?\n\n###\n\n","completion":" The recommendations may suffer from representation bias where a small number of people influence the system heavily. E.g.: Highly enthusiastic anime fans who rate movies much more frequently than others may cause the system to recommend anime more often than expected (incl. to non-anime fans). END"}
{"prompt":"When using a neural network in collaborative filtering, why can we have different number of factors for movie and user?\n\n###\n\n","completion":" In this case, we are not taking the dot product but instead concatenating the embedding matrices, so the number of factors can be different. END"}
{"prompt":"Why is there a  'nn.Sequential'  in the  'CollabNN'  model?\n\n###\n\n","completion":" This allows us to couple multiple nn.Module layers together to be used. In this case, the two linear layers are coupled together and the embeddings can be directly passed into the linear layers. END"}
{"prompt":"What kind of model should be use if we want to add metadata about users and items, or information such as date and time, to a collaborative filter model?\n\n###\n\n","completion":" We should use a tabular model, which is discussed in the next chapter!. END"}
{"prompt":"How do entity embeddings reduce memory usage and speed up neural networks?\n\n###\n\n","completion":" Especially for large datasets, representing the data as one-hot encoded vectors can be very inefficient (and also sparse). On the other hand, using entity embeddings allows the data to have a much more memory-efficient (dense) representation of the data. This will also lead to speed-ups for the model. END"}
{"prompt":"What kind of datasets are entity embeddings especially useful for?\n\n###\n\n","completion":" It is especially useful for datasets with features that have high levels of cardinality (the features have lots of possible categories). Other methods often overfit to data like this. END"}
{"prompt":"What are the two main families of machine learning algorithms?\n\n###\n\n","completion":" Ensemble of decision trees are best for structured (tabular data)\nMultilayered neural networks are best for unstructured data (audio, vision, text, etc.). END"}
{"prompt":"Why do some categorical columns need a special ordering in their classes? How do you do this in pandas?\n\n###\n\n","completion":" Ordinal categories may inherently have some order and by using 'set_categories' with the argument 'ordered=True' and passing in the ordered list, this information represented in the pandas DataFrame. END"}
{"prompt":"Summarize what a decision tree algorithm does.\n\n###\n\n","completion":" The basic idea of what a decision tree algorithm does is to determine how to group the data based on “questions” that we ask about the data. That is, we keep splitting the data based on the levels or values of the features and generate predictions based on the average target value of the data points in that group. Here is the algorithm:\n\nLoop through each column of the dataset in turn\nFor each column, loop through each possible level of that column in turn\nTry splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable)\nFind the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple “model” where our predictions are simply the average sale price of the item’s group\nAfter looping through all of the columns and possible levels for each, pick the split point which gave the best predictions using our very simple model\nWe now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each, by going back to step one for each group\nContinue this process recursively, and until you have reached some stopping criterion for each group — for instance, stop splitting a group further when it has only 20 items in it. END"}
{"prompt":"Why is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model?\n\n###\n\n","completion":" Some dates are different to others (ex: some are holidays, weekends, etc.) that cannot be described as just an ordinal variable. Instead, we can generate many different categorical features about the properties of the given date (ex: is it a weekday? is it the end of the month?, etc.). END"}
{"prompt":"Should you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick?\n\n###\n\n","completion":" No, the validation set should be as similar to the test set as possible. In this case, the test set contains data from later data, so we should split the data by the dates and include the later dates in the validation set. END"}
{"prompt":"How are mse, samples, and values calculated in the decision tree drawn in this chapter?\n\n###\n\n","completion":" By traversing the tree based on answering questions about the data, we reach the nodes that tell us the average value of the data in that group, the mse, and the number of samples in that group. END"}
{"prompt":"How do we deal with outliers, before building a decision tree?\n\n###\n\n","completion":" Finding out of domain data (Outliers)\nSometimes it is hard to even know whether your test set is distributed in the same way as your training data or, if it is different, then what columns reflect that difference. There’s actually a nice easy way to figure this out, which is to use a random forest!\nBut in this case we don’t use a random forest to predict our actual dependent variable. Instead we try to predict whether a row is in the validation set, or the training set. END"}
{"prompt":"How do we handle categorical variables in a decision tree?\n\n###\n\n","completion":" We convert the categorical variables to integers, where the integers correspond to the discrete levels of the categorical variable. Apart from that, there is nothing special that needs to be done to get it to work with decision trees (unlike neural networks, where we use embedding layers). END"}
{"prompt":"What is bagging?\n\n###\n\n","completion":" Train multiple models on random subsets of the data, and use the ensemble of models for prediction. END"}
{"prompt":"What is the difference between max_samples and max_features when creating a random forest?\n\n###\n\n","completion":" When training random forests, we train multiple decision trees on random subsets of the data. 'max_samples' defines how many samples, or rows of the tabular dataset, we use for each decision tree. 'max_features' defines how many features, or columns of the tabular dataset, we use for each decision tree. END"}
{"prompt":"If you increase n_estimators to a very high value, can that lead to overfitting? Why or why not?\n\n###\n\n","completion":" A higher 'n_estimators' mean more decision trees are being used. However, since the trees are independent of each other, using higher 'n_estimators' does not lead to overfitting. END"}
{"prompt":"What is <em>out of bag error<\/em> ?\n\n###\n\n","completion":" Only use the models not trained on the row of data when going through the data and evaluating the dataset. No validation set is needed. END"}
{"prompt":"Make a list of reasons why a model’s validation set error might be worse than the OOB error. How could you test your hypotheses?\n\n###\n\n","completion":" The major reason could be because the model does not generalize well. Related to this is the possibility that the validation data has a slightly different distribution than the data the model was trained on. END"}
{"prompt":"How can you answer each of these things with a random forest? How do they work?:\n\n###\n\n","completion":" How confident are we in our projections using a particular row of data?\nLook at standard deviation between the estimators\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nUsing the 'treeinterpreter' package to check how the prediction changes as it goes through the tree, adding up the contributions from each split\/feature. Use waterfall plot to visualize.\nWhich columns are the strongest predictors?\nLook at feature importance\nHow do predictions vary, as we vary these columns?\nLook at partial dependence plots. END"}
{"prompt":"What is the <em>extrapolation problem<\/em> ?\n\n###\n\n","completion":" Hard for a model to extrapolate to data that’s outside the domain of the training data. This is particularly important for random forests. On the other hand, neural networks have underlying Linear layers so it could potentially generalize better. END"}
{"prompt":"How can you tell if your test or validation set is distributed in a different way to your training set?\n\n###\n\n","completion":" We can do so by training a model to classify if the data is training or validation data. If the data is of different distributions (out-of-domain data), then the model can properly classify between the two datasets. END"}
{"prompt":"Why do we make saleElapsed a continuous variable, even although it has less than 9000 distinct values?\n\n###\n\n","completion":" This is a variable that changes over time, and since we want our model to extrapolate for future results, we make this a continuous variable. END"}
{"prompt":"What is boosting?\n\n###\n\n","completion":" We train a model that underfits the dataset, and train subsequent models that predicts the error of the original model. We then add the predictions of all the models to get the final prediction. END"}
{"prompt":"How could we use embeddings with a random forest? Would we expect this to help?\n\n###\n\n","completion":" Entity embeddings contains richer representations of the categorical features and definitely can improve the performance of other models like random forests. Instead of passing in the raw categorical columns, the entity embeddings can be passed into the random forest model. END"}
{"prompt":"Why might we not always use a neural net for tabular modeling?\n\n###\n\n","completion":" We might not use them because they are the hardest to train and longest to train, and less well-understood. Instead, random forests should be the first choice\/baseline, and neural networks could be tried to improve these results or add to an ensemble. END"}
{"prompt":"What is a language model?\n\n###\n\n","completion":" A language model is a self-supervised model that tries to predict the next word of a given passage of text. END"}
{"prompt":"Why is a language model considered self-supervised learning?\n\n###\n\n","completion":" There are no labels (ex: sentiment) provided during training. Instead, the model learns to predict the next word by reading lots of provided text with no labels. END"}
{"prompt":"What are self-supervised models usually used for?\n\n###\n\n","completion":" Sometimes, they are used by themselves. For example, a language model can be used for autocomplete algorithms! But often, they are used as a pre-trained model for transfer learning. END"}
{"prompt":"Why do we fine-tune language models?\n\n###\n\n","completion":" We can fine-tune the language model on the corpus of the desired downstream task, since the original pre-trained language model was trained on a corpus that is slightly different than the one for the current task. END"}
{"prompt":"What are the three steps to create a state-of-the-art text classifier?\n\n###\n\n","completion":" Train a language model on a large corpus of text (already done for ULM-FiT by Sebastian Ruder and Jeremy!)\nFine-tune the language model on text classification dataset\nFine-tune the language model as a text classifier instead. END"}
{"prompt":"How do the 50,000 unlabeled movie reviews help create a better text classifier for the IMDb dataset?\n\n###\n\n","completion":" By learning how to predict the next word of a movie review, the model better understands the language style and structure of the text classification dataset and can, therefore, perform better when fine-tuned as a classifier. END"}
{"prompt":"What are the three steps to prepare your data for a language model?\n\n###\n\n","completion":" Tokenization\nNumericalization\nLanguage model DataLoader. END"}
{"prompt":"What is tokenization? Why do we need it?\n\n###\n\n","completion":" Tokenization is the process of converting text into a list of words. It is not as simple as splitting on the spaces. Therefore, we need a tokenizer that deals with complicated cases like punctuation, hypenated words, etc. END"}
{"prompt":"Name three different approaches to tokenization.\n\n###\n\n","completion":" Word-based tokenization\nSubword-based tokenization\nCharacter-based tokenization. END"}
{"prompt":"What is ‘xxbos’?\n\n###\n\n","completion":" This is a special token added by fastai that indicated the beginning of the text. END"}
{"prompt":"List 4 rules that fastai applies to text during tokenization.\n\n###\n\n","completion":" Here are all the rules:\n\n\n'fix_html' :: replace special HTML characters by a readable version (IMDb reviews have quite a few of them for instance) ;\n\n'replace_rep' :: replace any character repeated three times or more by a special token for repetition (xxrep), the number of times it’s repeated, then the character ;\n\n'replace_wrep' :: replace any word repeated three times or more by a special token for word repetition (xxwrep), the number of times it’s repeated, then the word ;\n\n'spec_add_spaces' :: add spaces around \/ and # ;\n\n'rm_useless_spaces' :: remove all repetitions of the space character ;\n\n'replace_all_caps' :: lowercase a word written in all caps and adds a special token for all caps (xxcap) in front of it ;\n\n'replace_maj' :: lowercase a capitalized word and adds a special token for capitalized (xxmaj) in front of it ;\n\n'lowercase' :: lowercase all text and adds a special token at the beginning (xxbos) and\/or the end (xxeos). END"}
{"prompt":"Why are repeated characters replaced with a token showing the number of repetitions, and the character that’s repeated?\n\n###\n\n","completion":" We can expect that repeated characters could have special or different meaning than just a single character. By replacing them with a special token showing the number of repetitions, the model’s embedding matrix can encode information about general concepts such as repeated characters rather than requiring a separate token for every number of repetitions of every character. END"}
{"prompt":"What is numericalization?\n\n###\n\n","completion":" This refers to the mapping of the tokens to integers to be passed into the model. END"}
{"prompt":"Why might there be words that are replaced with the “unknown word” token?\n\n###\n\n","completion":" If all the words in the dataset have a token associated with them, then the embedding matrix will be very large, increase memory usage, and slow down training. Therefore, only words with more than 'min_freq' occurrence are assigned a token and finally a number, while others are replaced with the “unknown word” token. END"}
{"prompt":"With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer against the book website.)\n\n###\n\n","completion":" a. The dataset is split into 64 mini-streams (batch size)\nb. Each batch has 64 rows (batch size) and 64 columns (sequence length)\nc. The first row of the first batch contains the beginning of the first mini-stream (tokens 1-64)\nd. The second row of the first batch contains the beginning of the second mini-stream\ne. The first row of the second batch contains the second chunk of the first mini-stream (tokens 65-128). END"}
{"prompt":"Why do we need padding for text classification? Why don’t we need it for language modeling?\n\n###\n\n","completion":" Since the documents have variable sizes, padding is needed to collate the batch. Other approaches. like cropping or squishing, either to negatively affect training or do not make sense in this context. Therefore, padding is used. It is not required for language modeling since the documents are all concatenated. END"}
{"prompt":"What does an embedding matrix for NLP contain? What is its shape?\n\n###\n\n","completion":" It contains vector representations of all tokens in the vocabulary. The embedding matrix has the size (vocab_size x embedding_size), where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens. END"}
{"prompt":"What is perplexity?\n\n###\n\n","completion":" Perplexity is a commonly used metric in NLP for language models. It is the exponential of the loss. END"}
{"prompt":"Why do we have to pass the vocabulary of the language model to the classifier data block?\n\n###\n\n","completion":" This is to ensure the same correspondence of tokens to index so the model can appropriately use the embeddings learned during LM fine-tuning. END"}
{"prompt":"What is gradual unfreezing?\n\n###\n\n","completion":" This refers to unfreezing one layer at a time and fine-tuning the pretrained model. END"}
{"prompt":"Why is text generation always likely to be ahead of automatic identification of machine generated texts?\n\n###\n\n","completion":" The classification models could be used to improve text generation algorithms (evading the classifier) so the text generation algorithms will always be ahead. END"}
{"prompt":"What is “tokenization”? Why do we need it?\n\n###\n\n","completion":" I don’t think the book really dwells with why this is needed, only with some of the challenges. Tokenization is needed because we’re examining texts for patterns. It won’t be possible to find such patterns if we only consider a text at the sentence level, since patterns would only emerge if certain sentences frequently appeared together in sets, and this is a very rare phenomenon.  Needless to say, examining texts in their entirety would be even less productive.  We only find interesting patterns when we examine texts at the level of individual words, substrings, or individual characters. END"}
{"prompt":"What does an embedding matrix for NLP contain? What is its shape?\n\n###\n\n","completion":" It contains vector representations of all tokens in the vocabulary. The embedding matrix has the size (vocab_size x embedding_size), where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens.\nPlease let me know if this is wrong or unclear!. END"}
{"prompt":"With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer against the book website.)\n\n###\n\n","completion":" a. The batch has 64 rows (batch size) and also 64 columns (sequence length)\nb. The second row of the first batch contains the tokens beginning at 1\/64th of the dataset\nc. The first row of the second batch continues the first row of the first batch (tokens 65-128)\nI think the example would be a little clearer if we used different values for batch size and sequence length. For example:\nWith a batch size of 20, the first row of the tensor representing the first batch contains the first 50 tokens for the dataset.\na. The dataset is split into 20 mini-streams\nb. Each batch has 20 rows and 50 columns\nc. The first row of the first batch contains the beginning of the first mini-stream (tokens 1-50)\nd. The second row of the first batch contains the beginning of the second mini-stream\ne. The first row of the second batch contains the second chunk of the first mini-stream (tokens 51-100)\n@ilovescience, any thoughts?. END"}
{"prompt":"Why do we say that fastai has a “layered” API? What does it mean?\n\n###\n\n","completion":" fastai is designed following a layered architecture with 4 layers: an applications layer, a high-level API, a mid-level API and a low-level API. These offer higher and higher customizability as you make your way down the layers. The high-level of the API is most likely to be useful to beginners and to practitioners who are mainly in interested in applying pre-existing deep learning methods. It offers concise APIs over four main application areas: vision, text, tabular and time-series analysis, and collaborative filtering. These APIs choose intelligent default values and behaviors based on all available information. The mid-level API provides the core deep learning and data-processing methods for each of these applications, and low-level APIs provide a library of optimized primitives and functional and object-oriented foundations, which allows the mid-level to be developed and customised. END"}
{"prompt":"Why does a 'Transform' have a 'decode' method? What does it do?\n\n###\n\n","completion":" 'decode' is used by fastai’s 'show_batch' and 'show_results' , as well as some other inference methods, to convert predictions and mini-batches into a human-understandable representation. END"}
{"prompt":"Why does a 'Transform' have a 'setup' method? What does it do?\n\n###\n\n","completion":" In general, a 'Transform' is an object that behaves like a function and has an optional 'setup' method that will initialize some inner state and an optional 'decode' that will reverse the function. END"}
{"prompt":"How does a 'Transform' work when called on a tuple?\n\n###\n\n","completion":" A special behavior of 'Transform' s is that they always get applied over tuples. In general, our data is always a tuple '(input,target)' (sometimes with more than one input or more than one target). When applying a transform on an item like this, such as 'Resize' , we don’t want to resize the tuple as a whole; instead, we want to resize the input (if applicable) and the target (if applicable) separately. It’s the same for batch transforms that do data augmentation: when the input is an image and the target is a segmentation mask, the transform needs to be applied (the same way) to the input and the target. END"}
{"prompt":"Which methods do you need to implement when writing your own 'Transform'?\n\n###\n\n","completion":" If you want to write a custom transform to apply to your data, the easiest way is to write a function. You will need the 'encode' method and optionally the 'setup' or 'decode' methods. END"}
{"prompt":"Write a 'Normalize' transform that fully normalizes items (subtract the mean and divide by the standard deviation of the dataset), and that can decode that behavior. Try not to peek!\n\n###\n\n","completion":" ```class NormalizeMean(Transform):\n    def setups(self, items): self.mean = sum(items)\/len(items)\n    def encodes(self, x): return x-self.mean\n    def decodes(self, x): return x+self.mean\n``` END"}
{"prompt":"Write a 'Transform' that does the numericalization of tokenized texts (it should set its vocab automatically from the dataset seen and have a 'decode' method). Look at the source code of fastai if you need help.\n\n###\n\n","completion":" ```class Numericalize(Transform):\n    def setups(self, dsets):\n       if dsets is None: return\n       if self.vocab is None:\n           count = dsets.counter if getattr(dsets, 'counter', None) is not None else Counter(p for o in dsets for p in o)\n           if self.special_toks is None and hasattr(dsets, 'special_toks'):\n               self.special_toks = dsets.special_toks\n           self.vocab = make_vocab(count, min_freq=self.min_freq, max_vocab=self.max_vocab, special_toks=self.special_toks)\n           self.o2i = defaultdict(int, {v:k for k,v in enumerate(self.vocab) if v != 'xxfake'})\n    def encodes(self, o): return TensorText(tensor([self.o2i  [o_] for o_ in o]))\n    def decodes(self, o): return L(self.vocab[o_] for o_ in o)\n``` END"}
{"prompt":"What is a 'Pipeline'?\n\n###\n\n","completion":" To compose several transforms together, fastai provides the 'Pipeline' class. We define a 'Pipeline' by passing it a list of 'Transform' s; it will then compose the transforms inside it. When you call 'Pipeline' on an object, it will automatically call the transforms inside, in order. END"}
{"prompt":"What is a 'TfmdLists'?\n\n###\n\n","completion":" Your data is usually a set of raw items (like filenames, or rows in a DataFrame) to which you want to apply a succession of transformations. We just saw that a succession of transformations is represented by a 'Pipeline' in fastai. The class that groups together this 'Pipeline' with your raw items is called 'TfmdLists' . END"}
{"prompt":"What is a 'Datasets'? How is it different from a 'TfmdLists'?\n\n###\n\n","completion":" 'Datasets' will apply two (or more) pipelines in parallel to the same raw object and build a tuple with the result. Like 'TfmdLists' , it will automatically do the setup for us, and when we index into a 'Datasets' , it will return us a tuple with the results of each pipeline. 'TfmdLists' behaves differently in that it returns two separate objects for our inputs and targets. END"}
{"prompt":"Why are 'TfmdLists' and 'Datasets' named with an “s”?\n\n###\n\n","completion":" 'TfmdLists' and 'Datasets' are named with an “s” because they can handle a training and a validation set with a 'splits' argument. END"}
{"prompt":"How can you build a 'DataLoaders' from a 'TfmdLists' or a 'Datasets'?\n\n###\n\n","completion":" You can directly convert a 'TfmdLists' or a 'Datasets' to a 'DataLoaders' object with the 'dataloaders' method. END"}
{"prompt":"How do you pass 'item_tfms' and 'batch_tfms' when building a 'DataLoaders' from a 'TfmdLists' or a 'Datasets'?\n\n###\n\n","completion":" When building a 'DataLoaders' from a 'TfmdLists' or a 'Datasets', 'after_item' is the equivalent of 'item_tfms' in 'DataBlock' whereas 'after_batch' is the equivalent of 'batch_tfms' in 'DataBlock'. END"}
{"prompt":"What do you need to do when you want to have your custom items work with methods like 'show_batch' or 'show_results'?\n\n###\n\n","completion":" 'decode' is used by fastai’s 'show_batch' and 'show_results' , as well as some other inference methods, to convert predictions and mini-batches into a human-understandable representation. Your custom items should therefore have a 'decode' method in order for them to work with methods like 'show_batch' or 'show_results'. END"}
{"prompt":"Why can we easily apply fastai data augmentation transforms to the 'SiamesePair' we built?\n\n###\n\n","completion":" The mid-level API for data collection gives us two objects ('TfmdLists' and 'Datasets') that can help us easily apply transforms to the 'SiamesePair' we built. END"}
{"prompt":"What do you need to do when you want to have your custom items work with methods like 'show_batch' or 'show_results'?\n\n###\n\n","completion":" You must create a custom type by inherit 'fastuple' and implements a 'show' method.When you call the 'show' method on a 'TfmdLists' or a 'Datasets' object, it will decode items until it reaches a type that contains a 'show' method and use it to show the object.\nI copy the example to show the signature:class SiameseImage(fastuple):\n    def show(self, ctx=None, **kwargs): \n        img1,img2,same_breed = self\n        if not isinstance(img1, Tensor):\n            if img2.size != img1.size: img2 = img2.resize(img1.size)\n            t1,t2 = tensor(img1),tensor(img2)\n            t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1)\n        else: t1,t2 = img1,img2\n        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)\n        return show_image(torch.cat([t1,line,t2], dim=2), \n                          title=same_breed, ctx=ctx)\n'. END"}
{"prompt":"If the dataset for your project is so big and complicated that working with it takes a significant amount of time, what should you do?\n\n###\n\n","completion":" Perhaps create the simplest possible dataset that allow for quick and easy prototyping. For example, Jeremy created a “human numbers” dataset. END"}
{"prompt":"Why do we concatenate the documents in our dataset before creating a language model?\n\n###\n\n","completion":" To create a continuous stream of input\/target words, to be able to split it up in batches of significant size. END"}
{"prompt":"To use a standard fully connected network to predict the fourth word given the previous three words, what two tweaks do we need to make?\n\n###\n\n","completion":" Use the same weight matrix for the three layers.\nUse the first word’s embeddings as activations to pass to linear layer, add the second word’s embeddings to the first layer’s output activations, and continues for rest of words. END"}
{"prompt":"How can we share a weight matrix across multiple layers in PyTorch?\n\n###\n\n","completion":" Define one layer in the PyTorch model class, and use them multiple times in the 'forward' method. END"}
{"prompt":"Write a module which predicts the third word given the previous two words of a sentence, without peeking.\n\n###\n\n","completion":" Same code as in chapter:\n```class LMModel1(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        \n    def forward(self, x):\n        h = F.relu(self.h_h(self.i_h(x[:,0])))\n        h = h + self.i_h(x[:,1])\n        h = F.relu(self.h_h(h))\n        h = h + self.i_h(x[:,2])\n        h = F.relu(self.h_h(h))\n        return self.h_o(h)\n``` END"}
{"prompt":"Why can maintaining the hidden state in an RNN lead to memory and performance problems? How do we fix this problem?\n\n###\n\n","completion":" Since the hidden state is maintained through every single call of the model, when performing backpropagation with the model, it has to use the gradients from also all the past calls of the model. This can lead to high memory usage. So therefore after every call, the 'detach' method is called to delete the gradient history of previous calls of the model. END"}
{"prompt":"What is BPTT?\n\n###\n\n","completion":" Calculating backpropagation only for the given batch, and therefore only doing backprop for the defined sequence length of the batch. END"}
{"prompt":"What are the downsides of predicting just one output word for each three input words?\n\n###\n\n","completion":" There are words in between that are not being predicted and that is extra information for training the model that is not being used. To solve this, we apply the output layer to every hidden state produced to predict three output words for the three input words (offset by one). END"}
{"prompt":"Why do we need a custom loss function for  'LMModel4' ?\n\n###\n\n","completion":" CrossEntropyLoss expects flattened tensors. END"}
{"prompt":"Why is the training of  'LMModel4'  unstable?\n\n###\n\n","completion":" Because this network is effectively very deep and this can lead to very small or very large gradients that don’t train well. END"}
{"prompt":"In the unrolled representation, we can see that a recurrent neural network actually has many layers. So why do we need to stack RNNs to get better results?\n\n###\n\n","completion":" Because only one weight matrix is really being used. So multiple layers can improve this. END"}
{"prompt":"Why should we get better results in an RNN if we call  'detach'  less often? Why might this not happen in practice with a simple RNN?\n\n###\n\n","completion":" Numbers that are just slightly higher or lower than one can lead to the explosion or disappearance of numbers after repeated multiplications. In deep networks, we have repeated matrix multiplications, so this is a big problem. END"}
{"prompt":"Why do vanishing gradients prevent training?\n\n###\n\n","completion":" Gradients that are zero can’t contribute to training because they don’t change any weights. END"}
{"prompt":"Why do we scale the weights with dropout? Is this applied during training, inference, or both?\n\n###\n\n","completion":" a. The scale changes if we sum up activations, it makes a difference if all activations are present or they are dropped with probability p. To correct the scale, a division by (1-p) is applied.\nb. In the implementation in the book, it is applied during training\nc. It should be possible in both ways. END"}
{"prompt":"What is the purpose of this line from  'Dropout' ?:  'if not self.training: return x'\n\n\n###\n\n","completion":" When not in training mode, don’t apply dropout. END"}
{"prompt":"Experiment with  'bernoulli_'  to understand how it works.\n\n###\n\n","completion":" a. 'Module.train(), Module.eval()'. END"}
{"prompt":"Write the equation for activation regularization (in maths or code, as you prefer). How is it different to weight decay?\n\n###\n\n","completion":" 'loss += alpha * activations.pow(2).mean() '\nIt is different by not decreasing the weights but the activations. END"}
{"prompt":"Write the equation for temporal activation regularization (in maths or code, as you prefer). Why wouldn’t we use this for computer vision problems?\n\n###\n\n","completion":" This focuses on making the activations of consecutive tokens to be similar:\n'loss += alpha * activations.pow(2).mean()'. END"}
