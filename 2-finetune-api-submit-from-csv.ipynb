{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Fine-tuning API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# set the api key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Notes on formatting for GPT fine-tuning ####################\n",
    "\n",
    "# - Based on your file extension, your file is formatted as a CSV file\n",
    "# - Your file contains 56 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
    "# - Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
    "# - All completions end with suffix ` `\n",
    "#   WARNING: Some of your completions contain the suffix ` ` more than once. We suggest that you review your completions and add a unique ending\n",
    "# - The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
    "\n",
    "# add a space to the end of each prompt\n",
    "# Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\n\\n###\\n\\n. The separator should not appear elsewhere in any prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Load raw CSV files of prompt-completion pairs --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPT Chat\n",
    "pc_csv_IN_path =\"data/gptchatqadata/gpt-qa-df.csv\"\n",
    "curr_suffix_name = \"gpt-ml-qa-pairs-C\"\n",
    "\n",
    "assert os.path.exists(pc_csv_IN_path), \"File does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FastAI Q&A pairs \n",
    "pc_csv_IN_path =\"data/fastaiqadata/fastai-qa-cleaned.csv\"\n",
    "curr_suffix_name = \"fastai-ft\"\n",
    "\n",
    "assert os.path.exists(pc_csv_IN_path), \"File does not exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Format raw CSV --> formatted CSV --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####\n",
      "Raw CSV -> Formatted CSV conversion complete\n",
      "#####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# INPUTS: \n",
    "# - csv: a csv with headers 'prompt' and 'completion'\n",
    "\n",
    "# PROCESSING: \n",
    "# - add a space to the end of each prompt\n",
    "# - add a \\n\\n###\\n\\n to the end of each prompt\n",
    "# - add a space to the beginning of each completion\n",
    "# - add a \\n\\n<+++>\\n\\n to the end of each completion\n",
    "\n",
    "# saves to df\n",
    "\n",
    "# OUTPUTS:\n",
    "# - csv: a cleaned, formatted csv ready for fine-tuning \n",
    "\n",
    "def format_csv_for_ft(qa_csv_path):\n",
    "    #### Check formatting on CSV \n",
    "    assert qa_csv_path[-4:] == '.csv', \"input path does not end with '.csv'\"\n",
    "    # - headers are 'prompt' and 'completion'\n",
    "    assert pd.read_csv(qa_csv_path, nrows=0).columns.tolist() == ['prompt', 'completion'], \"file headers are not 'prompt' and 'completion'\"\n",
    "\n",
    "    gpt_formatted_df = pd.read_csv(qa_csv_path)\n",
    "    # print(gpt_loaded_df.head()) # double check input \n",
    "\n",
    "    #### Format for fine-tuning \n",
    "    # 1) Completions: start with a space ' ', end with a seperator '\\n+END+\\n'\n",
    "    # Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.\n",
    "    # Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\n, ###, or any other token that does not appear in any completion.\n",
    "    gpt_formatted_df['completion'] = gpt_formatted_df['completion'].apply(lambda x: ' ' + x + ' END')\n",
    "    # 2) add a separater to the end of each prompt \n",
    "    gpt_formatted_df['prompt'] = gpt_formatted_df['prompt'].apply(lambda x: x + '\\n\\n###\\n\\n')\n",
    "\n",
    "    #### Save to csv \n",
    "    qa_csv_OUT_path=os.path.join(pc_csv_IN_path[:-4]+\"-MANUAL-formatted.csv\")\n",
    "    gpt_formatted_df.to_csv(qa_csv_OUT_path, index=False)\n",
    "    return qa_csv_OUT_path\n",
    "\n",
    "formatted_pc_csv_path = format_csv_for_ft(pc_csv_IN_path)\n",
    "\n",
    "print(\"\\n#####\\nRaw CSV -> Formatted CSV conversion complete\\n#####\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE splitting train and valid set; not used right now \n",
    "\n",
    "# def split_df_train_test(df, train_frac=0.8):\n",
    "#     # split the df into train and test\n",
    "#     train_df = df.sample(frac=train_frac, random_state=42)\n",
    "#     test_df = df.drop(train_df.index)\n",
    "\n",
    "#     return train_df, test_df\n",
    "\n",
    "######\n",
    "# ## # get train and test df from the previous 'formatted_qa_df'\n",
    "# qa_train_df, qa_test_df = split_df_train_test(formatted_qa_df)\n",
    "\n",
    "# #create paths to save \n",
    "# qa_TEST_OUT_path = f\"{qa_csv_IN_path[:-4]}-TRAIN.csv\"\n",
    "# qa_TRAIN_OUT_path = f\"{qa_csv_IN_path[:-4]}-TEST.csv\"\n",
    "\n",
    "# # # save the train and test dfs to csv \n",
    "# qa_train_df.to_csv(qa_TEST_OUT_path, index=False)\n",
    "# qa_test_df.to_csv(qa_TRAIN_OUT_path, index=False)\n",
    "\n",
    "# print ('train/test shapes: ', qa_train_df.shape, qa_test_df.shape)\n",
    "# print ('train/test heads: \\n\\n', qa_train_df.head(),qa_test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV -> JSONL, using the OpenAI CLI tool\n",
    "\n",
    "*Typically, we call the following in terminal to convert the file:*\n",
    "\n",
    "`openai tools fine_tunes.prepare_data -f <LOCAL_FILE>`\n",
    "\n",
    "The following section automates this with a bash script "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@TODO \n",
    "Eventually, run a bash script that gets the output from terminal and displays to user \n",
    "to make choices about data formatting\n",
    "for now lets just do it manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_conversion_sh_script_path = './scripts/csv2jsonl_openai.sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assure permissions for the bash script and file \n",
    "! chmod a+x {jsonl_conversion_sh_script_path}\n",
    "! chmod a+x {formatted_pc_csv_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Based on your file extension, your file is formatted as a CSV file\n",
      "- Your file contains 223 prompt-completion pairs\n",
      "- All prompts end with suffix `\\n\\n###\\n\\n`\n",
      "- All completions end with suffix ` END`\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Necessary] Your format `CSV` will be converted to `JSONL`\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: \n",
      "Wrote modified file to `data/fastaiqadata/fastai-qa-cleaned-MANUAL-formatted_prepared.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"data/fastaiqadata/fastai-qa-cleaned-MANUAL-formatted_prepared.jsonl\"\n",
      "\n",
      "After youâ€™ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\" END\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 5.51 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n",
      "Done!\n",
      "\n",
      "./scripts/csv2jsonl_openai.sh: line 18: syntax error near unexpected token `done'\n",
      "./scripts/csv2jsonl_openai.sh: line 18: `done'\n",
      "\n",
      "data/fastaiqadata/fastai-qa-cleaned-MANUAL-formatted_prepared.jsonl\n",
      "\n",
      "#####\n",
      "CSV -> JSONL conversion complete\n",
      "#####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Run the OpenAI CLI tool to format the CSV to JSONL \n",
    "import subprocess\n",
    "# you need to add './' before the filename for bash to recognize it \n",
    "\n",
    "assert os.path.exists(formatted_pc_csv_path), \"file does not exist\"\n",
    "assert os.path.exists(jsonl_conversion_sh_script_path), \"sh file does not exist\"\n",
    "\n",
    "# Call the bash script to format the CSV to JSONL using the OpenAI CLI tool \n",
    "process = subprocess.Popen([jsonl_conversion_sh_script_path, formatted_pc_csv_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "if process is not None:\n",
    "    stdout, stderr = process.communicate()\n",
    "    print(stdout.decode('utf-8'))\n",
    "    print(stderr.decode('utf-8'))\n",
    "    \n",
    "    \n",
    "# get the new JSONL filename \n",
    "formatted_pc_jsonl_path = formatted_pc_csv_path[:-4] + '_prepared.jsonl'\n",
    "assert (os.path.exists(formatted_pc_jsonl_path)), \"JSONL file does not exist\"\n",
    "print(formatted_pc_jsonl_path)\n",
    "print(\"\\n#####\\nCSV -> JSONL conversion complete\\n#####\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to format the API call to the fine tuned model \n",
    "\n",
    "### format the prompt from the user before sending it \n",
    "After youâ€™ve fine-tuned a model, remember that your prompt has to end with the indicator string \n",
    "\n",
    "`\\n\\n###\\n\\n` \n",
    "\n",
    "for the model to start generating completions, rather than continuing with the prompt. \n",
    "\n",
    "### param in API call for stop sequence \n",
    "Make sure to include \n",
    "\n",
    "`stop=[\" \\n<+++>\\n\"]` \n",
    "\n",
    "so that the generated texts ends at the expected place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### an example call to create a ft model in terminal: \n",
    "\n",
    "openai api fine_tunes.create \\\n",
    "-t gpt-qa-train_prepared.jsonl \\\n",
    "-v gpt-qa-valid_prepared.jsonl \\\n",
    "-m \"davinci\" \\\n",
    "--suffix \"gpt-ml-qa-pairs-A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Functions to create a fine-tuned model from the JSONL file --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# HELPER FUNCTIONS\n",
    "#####################\n",
    "\n",
    "# FUNCTION \n",
    "# upload_file_openai\n",
    "# - takes in a jsonl formatted dataset (just training set for now for simplicity)\n",
    "# - eventually could abstract to general file upload but not needed for now\n",
    "def upload_jsonl_ft_to_openai(filename): \n",
    "    # assert the filetype is jsonl \n",
    "    ext = os.path.splitext(filename)[-1].lower()\n",
    "    print ('ext:', ext)\n",
    "    assert ext== '.jsonl', \"filetype must be jsonl\"\n",
    "    r = openai.File.create(\n",
    "        file=open(filename, \"rb\"),\n",
    "        purpose='fine-tune'\n",
    "    )\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# FUNCTION \n",
    "# create_finetuned_model\n",
    "\n",
    "# INPUTS: \n",
    "# - train_file_id: a csv with headers 'prompt' and 'completion'\n",
    "# - model type\n",
    "# - hyperparameters\n",
    "\n",
    "# OUTPUTS:\n",
    "# finetuned_model_response: a response object including the finetuned model id\n",
    "\n",
    "# @TODO add the ability to turn on and off test-train splitting \n",
    "\n",
    "def create_finetuned_model(train_file_id, valid_file_id='', model=\"davinci-003\", learning_rate_multiplier='', n_epochs='', suffix=''):\n",
    "\n",
    "    #####################\n",
    "    # create the request to build the finetuned model \n",
    "    try: \n",
    "        finetuned_model_response = openai.FineTune.create(\n",
    "            training_file=train_file_id,\n",
    "            # validation_file=valid_file_id, # ignore valid file for now\n",
    "            model=model,\n",
    "            learning_rate_multiplier=learning_rate_multiplier,\n",
    "            n_epochs=n_epochs,\n",
    "            suffix=suffix,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print (\"Error creating finetuned model: \", e)\n",
    "        return None\n",
    "    return finetuned_model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (242268267.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [50]\u001b[0;36m\u001b[0m\n\u001b[0;31m    curl https://api.openai.com/v1/files/$RESULTS_FILE_ID/content \\\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE \n",
    "# this is the old way using a train/valid set \n",
    "\n",
    "# _train_file_id = upload_file_openai('gpt-qa-train-formatted_prepared.jsonl')['id']\n",
    "# _valid_file_id = upload_file_openai('gpt-qa-valid-formatted_prepared.jsonl')['id']\n",
    "\n",
    "# ftm = create_finetuned_model(\n",
    "#     train_file_id=_train_file_id,\n",
    "#     valid_file_id=_valid_file_id,\n",
    "#     model=\"davinci\",\n",
    "#     learning_rate_multiplier=0.05,\n",
    "#     n_epochs=1,\n",
    "#     suffix=suffix_name,\n",
    "# )\n",
    "\n",
    "# ftm_id = ftm['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the finetuned model id to the list.csv file, along with the datetime\n",
    "\n",
    "def log_finetuned_model_id(ftm_id, model_name, learning_rate_multiplier, n_epochs, suffix, log_file_path):\n",
    "    # get the current datetime \n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    # create the row to append \n",
    "    row = f\"{dt_string},{ftm_id},{model_name},{learning_rate_multiplier},{n_epochs},{suffix}\" \n",
    "    # append the row to the log file\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        f.write(row + '\\n')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# FUNCTION: file_to_finetuned_model\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "file_to_finetuned_model_wrapper\n",
    "\n",
    "Inputs: \n",
    "    filename: \n",
    "        - filename path of a csv file to be uploaded to openai\n",
    "        - must be formatted as a csv with headers 'prompt' and 'completion' - see the functions above to do that \n",
    "            eg \"/data/my-prompt-completion-pairs-list.csv\"\n",
    "\n",
    "    model: string, name of the model to be finetuned \n",
    "            eg \"davinci-003\"\n",
    "    \n",
    "    learning_rate_multiplier: float, learning rate multiplier for the finetuned model \n",
    "            eg \"0.05\", typically 0.05 - 0.2\n",
    "    \n",
    "    n_epochs: int, number of epochs for the finetuned model \n",
    "            eg \"1\", typically 1 - 3\n",
    "    \n",
    "    suffix: string, suffix to be added to the finetuned model \n",
    "            eg \"my-finetuned-model\"\n",
    "\n",
    "Outputs: \n",
    "    finetuned_model_response: the response of the OpenAI API call to create the finetuned model\n",
    "        - a dictionary with the finetuned model id, hyperparameters, model name, etc.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "\n",
    "def file_to_finetuned_model_wrapper(train_file, valid_file='', model=\"davinci\", learning_rate_multiplier=0.05, n_epochs=2, suffix=''):\n",
    "    # upload files to openai\n",
    "    if train_file:\n",
    "        assert(train_file.endswith('.jsonl')), \"train_file must be a .jsonl file\"\n",
    "        _train_file_id = upload_jsonl_ft_to_openai(train_file)['id']\n",
    "        print ('Successfully uploaded train JSONL file. train_file_id:', _train_file_id)\n",
    "    else:\n",
    "        FileNotFoundError('Must include a train_file')\n",
    "\n",
    "    if valid_file != '':\n",
    "        assert(valid_file.endswith('.jsonl')), \"valid_file must be a .jsonl file\"\n",
    "        _valid_file_id = upload_jsonl_ft_to_openai(valid_file)['id']\n",
    "    else: \n",
    "        _valid_file_id = ''\n",
    "\n",
    "    # ensure suffix is within the required length; if not then truncate\n",
    "    if len(suffix) > 40:\n",
    "        suffix = suffix[:40]\n",
    "        print('Suffix longer than required 40-character length; truncating to 40 characters: ', suffix)\n",
    "    \n",
    "    \n",
    "    # create a finetuned model\n",
    "    finetuned_model_response = create_finetuned_model(\n",
    "        train_file_id=_train_file_id,\n",
    "        # valid_file_id=_valid_file_id,\n",
    "        model=model,\n",
    "        learning_rate_multiplier=learning_rate_multiplier,\n",
    "        n_epochs=n_epochs,\n",
    "        suffix=suffix,\n",
    "    )\n",
    "\n",
    "    return finetuned_model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 224 input examples \n",
    "# 892 steps\n",
    "# 3.98 steps per input example\n",
    "# 3.98 epochs? ~4 epochs\n",
    "\n",
    "# why is it 3.98 epochs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Create a ftm from the JSONL file --- \n",
    "The magic is here! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ext: .jsonl\n",
      "Successfully uploaded train JSONL file. train_file_id: file-T3uIX4UKStjSyRH4N5vKAeqe\n"
     ]
    }
   ],
   "source": [
    "ftm = file_to_finetuned_model_wrapper(\n",
    "    train_file=formatted_pc_jsonl_path, #from the previous step \n",
    "    learning_rate_multiplier=0.02,\n",
    "    suffix=curr_suffix_name # declared at the beginning of the ntoebook \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log each element of the json response as a column in a csv file\n",
    "from datetime import datetime\n",
    "\n",
    "def log_ftm(ftm):\n",
    "    \"\"\"\n",
    "    # Checks if the ftm id is already in the log file. If not, then adds to log file. \n",
    "    # INPUTS: ftm - the json response from the OpenAI API call to create a finetuned model\n",
    "    # OUTPUTS: True/False\n",
    "    \"\"\"\n",
    "    log_file_path = 'outputs/finetuned-models-log.csv'\n",
    "\n",
    "    if not os.path.exists(log_file_path):\n",
    "        with open(log_file_path, 'w') as f:\n",
    "            f.write('datetime,created_at,ftm_id, status\\n')\n",
    "\n",
    "    # check if the ftm id already exists in the csv \n",
    "    pattern = ftm['id']\n",
    "    with open(log_file_path, 'r') as csvfile:\n",
    "        if any(map(lambda x: pattern == x.rstrip(), csvfile)): # iterates through text looking for match\n",
    "            print (\"Finetuned model id already exists in log file\")\n",
    "            return False\n",
    "        else:\n",
    "            with open(log_file_path, 'a') as f:\n",
    "                f.write(f\"{datetime.now()}, {ftm['created_at']},{ftm['id']}, {ftm['status']}\\n\")\n",
    "            return True\n",
    "    \n",
    "\n",
    "log_ftm(ftm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print *initial* profile of ftm \n",
    "Note that it's still being trained, so \n",
    "\n",
    "Name: none\n",
    "\n",
    "Status: pending "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###\n",
      "Created fine-tuned model ft-5yRCwjWkWBvQddtf7P1rUIwP\n",
      "###\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'datetime.datetime' has no attribute 'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y255sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ftm_id \u001b[39m=\u001b[39m ftm[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y255sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m###\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCreated fine-tuned model \u001b[39m\u001b[39m{\u001b[39;00mftm_id\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m###\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y255sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m print_latest_ftm(ftm[\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb Cell 30\u001b[0m in \u001b[0;36mprint_latest_ftm\u001b[0;34m(_ftm_id)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y255sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_latest_ftm\u001b[39m(_ftm_id):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y255sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     ftm \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mFineTune\u001b[39m.\u001b[39mretrieve(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39m_ftm_id)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y255sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m###\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mid: \u001b[39m\u001b[39m{\u001b[39;00mftm[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mname:\u001b[39m\u001b[39m{\u001b[39;00mftm[\u001b[39m'\u001b[39m\u001b[39mfine_tuned_model\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mcreated at: \u001b[39m\u001b[39m{\u001b[39;00mdisplay_unix_time(ftm[\u001b[39m'\u001b[39m\u001b[39mcreated_at\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mstatus: \u001b[39m\u001b[39m{\u001b[39;00mftm[\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m###\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb Cell 30\u001b[0m in \u001b[0;36mdisplay_unix_time\u001b[0;34m(unix_time)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y255sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdisplay_unix_time\u001b[39m(unix_time):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y255sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m datetime\u001b[39m.\u001b[39;49mdatetime\u001b[39m.\u001b[39mfromtimestamp(unix_time)\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'datetime.datetime' has no attribute 'datetime'"
     ]
    }
   ],
   "source": [
    "ftm_id = ftm['id']\n",
    "\n",
    "print(f\"\\n###\\nCreated fine-tuned model {ftm_id}\\n###\\n\")\n",
    "print_latest_ftm(ftm['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Create Test with FastAI Question-Answer Pairs data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ftm_fastai = file_to_finetuned_model_wrapper(\n",
    "#     train_file='data/fastaiqadata/fastai-qa-cleaned-formatted-CSV-FIRST_prepared.jsonl', # NEEDS UPDATING \n",
    "#     suffix='fastai-qa-cleaned-formatted-CSV-FIRST_prepared',\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('id ', ftm_fastai['id'])\n",
    "# print('name ',ftm_fastai['fine_tuned_model'])\n",
    "# print('status ', ftm_fastai['status'])\n",
    "# print (display_unix_time(ftm['created_at']))\n",
    "# print ('ftm: ', ftm_fastai)\n",
    "\n",
    "# ####\n",
    "# whatsup = openai.FineTune.retrieve(ftm_fastai['id'])\n",
    "# whatsup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Create Test with GPT Chat log data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ext: .jsonl\n",
      "Successfully uploaded train JSONL file. train_file_id: file-NNiI12m2E3onsRuzo3Qw6ncp\n",
      "id:  ft-Ki3Hscefmfp99PBreooFW8I5\n",
      "name:  None\n"
     ]
    }
   ],
   "source": [
    "# TEST GPT CHAT DATA \n",
    "# test the file_to_finetuned_model function\n",
    "\n",
    "# banana_file = \"data/gptchatqadata/gpt-qa-train-formatted_prepared.jsonl\"\n",
    "# banana_suffix=\"qa-train-iterate\"\n",
    "\n",
    "# ftm_gptchat = file_to_finetuned_model_wrapper(\n",
    "#     train_file=banana_file,\n",
    "#     suffix=banana_suffix + \"-z\"\n",
    "# )\n",
    "\n",
    "# print('id: ',ftm_gptchat['id'])\n",
    "# print('name: ',ftm_gptchat['fine_tuned_model'])\n",
    "\n",
    "# ftm is only available after the model has been created!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Wait for Upload --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-12-50-36\n",
      "ft-xXiSGiL0RrMUpKikJw09T1up\n"
     ]
    }
   ],
   "source": [
    "# HELPERS \n",
    "\n",
    "def get_ftmodel_name_from_id(id):\n",
    "    ftmodel = openai.FineTune.retrieve(id)\n",
    "    return ftmodel['fine_tuned_model']\n",
    "\n",
    "def get_ftmodel_id_from_name(name):\n",
    "    all_ft_models = openai.FineTune.list()['data']\n",
    "    for ftmodel in all_ft_models:\n",
    "        if name in ftmodel['fine_tuned_model']:\n",
    "            return ftmodel['id']\n",
    "    return None\n",
    "\n",
    "def get_ftm_from_id(id):\n",
    "    return openai.FineTune.retrieve(id)\n",
    "\n",
    "# print(get_ftmodel_name_from_id(\"ft-xXiSGiL0RrMUpKikJw09T1up\"))\n",
    "# print(get_ftmodel_id_from_name(\"davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-12-50-36\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET FTM AWAIT READY \n",
    "\n",
    "# the finetuned model needs to finish training before we can use it\n",
    "# so let's track its status and only make calls when its' ready \n",
    "# we can use the finetuned model id to check its status\n",
    "\n",
    "import time\n",
    "\n",
    "def get_finetuned_model_status(finetuned_model_id):\n",
    "    response = openai.FineTune.retrieve(id=finetuned_model_id)\n",
    "    return response['status']\n",
    "\n",
    "#####\n",
    "# FUNCTION: await_ft_ready\n",
    "# INPUT: finetuned_model_id, seconds_of_patience\n",
    "# prints every check_every_n_seconds seconds the status of the mdoel \n",
    "# times out after seconds_of_patience seconds\n",
    "# OUTPUT: True if finetuned model is ready, False if finetuned model is not ready\n",
    "\n",
    "def await_ft_ready(finetuned_model_id, seconds_of_patience=400):\n",
    "\n",
    "    check_every_n_seconds = 10\n",
    "    # round down \n",
    "    num_checkins = seconds_of_patience//check_every_n_seconds # number of times to check the status of the finetuned model\n",
    "    print(f\"###\\nWaiting for {finetuned_model_id} to train.\\n###\\n\")\n",
    "\n",
    "    finetuned_model_status = None \n",
    "    counter = 0\n",
    "\n",
    "    while True:\n",
    "        status = get_finetuned_model_status(finetuned_model_id)\n",
    "        if status == 'succeeded':\n",
    "            print(f\"###\\n{finetuned_model_id} is ready!\\n###\\n\")\n",
    "            print (openai.FineTune.retrieve(id=finetuned_model_id))\n",
    "            return True\n",
    "        else:\n",
    "            counter +=1\n",
    "            print(f\"{finetuned_model_id} finetuned model is still training. status: {status}. {counter*check_every_n_seconds} seconds so far. counter: {counter}\")\n",
    "            \n",
    "            if counter % 10 == 0: #arbitrary every N check ins, print update \n",
    "                print(f\"{finetuned_model_id}. seconds so far: at least {counter*check_every_n_seconds}. Status: {status}\")\n",
    "            if counter > num_checkins: \n",
    "                # print that the current timed out\n",
    "                print(f\"{finetuned_model_id} finetuned model has taken longer than {seconds_of_patience} to train, since we started checking in on its status. Calling exceeds patience variable. Exiting await-train loop.\")\n",
    "                print (openai.FineTune.retrieve(id=finetuned_model_id))\n",
    "                return False\n",
    "            time.sleep(check_every_n_seconds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert unix time to human readable time\n",
    "from datetime import datetime\n",
    "def display_unix_time(unix_time):\n",
    "    return datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# ftm_trained_id ='ft-xXiSGiL0RrMUpKikJw09T1up'\n",
    "# await_ft_ready(ftm_trained_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_latest_ftm(_ftm_id):\n",
    "    ftm = openai.FineTune.retrieve(id=_ftm_id)\n",
    "    print (f\"###\\nid: {ftm['id']}\\nname:{ftm['fine_tuned_model']} \\ncreated at: {display_unix_time(ftm['created_at'])} \\nstatus: {ftm['status']}\\n###\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_latest_ftm(ftm_fastai['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~ Flip this on if you want it to wait on the actual ftm we just loaded ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "Waiting for ft-5yRCwjWkWBvQddtf7P1rUIwP to train.\n",
      "###\n",
      "\n",
      "###\n",
      "ft-5yRCwjWkWBvQddtf7P1rUIwP is ready!\n",
      "###\n",
      "\n",
      "{\n",
      "  \"created_at\": 1670108038,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1670108038,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-5yRCwjWkWBvQddtf7P1rUIwP\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1670108563,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune costs $2.49\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1670108564,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune enqueued. Queue number: 0\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1670108566,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune started\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1670108723,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 1/4\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1670108797,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 2/4\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1670108870,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 3/4\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1670108943,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 4/4\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1670108984,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded model: davinci:ft-sandbox:fastai-ft-2022-12-03-23-09-44\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1670108985,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded result file: file-0iXySDoBk0bBDF20JmUrDkCb\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1670108985,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune succeeded\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": \"davinci:ft-sandbox:fastai-ft-2022-12-03-23-09-44\",\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 4,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-5yRCwjWkWBvQddtf7P1rUIwP\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
      "  \"result_files\": [\n",
      "    {\n",
      "      \"bytes\": 47072,\n",
      "      \"created_at\": 1670108985,\n",
      "      \"filename\": \"compiled_results.csv\",\n",
      "      \"id\": \"file-0iXySDoBk0bBDF20JmUrDkCb\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune-results\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 95616,\n",
      "      \"created_at\": 1670108037,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-T3uIX4UKStjSyRH4N5vKAeqe\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1670108985,\n",
      "  \"validation_files\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ftm_training_is_finished = await_ft_ready(ftm_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next to add: database\n",
    "\n",
    "- add ftm to a local sql database upon creation \n",
    "- cron job to check every 5 mins if status updated and is ready\n",
    "- if so then update the status in the db, and also send a notification somehow? maybe just have the db loaded up in a prisma studio in the browser \n",
    "- also all of this just feeds into the product, it's one of two training pipelines \n",
    "- i kinda want to see about the other one now too \n",
    "- but it's late and time to sleep "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files.\n",
    "\n",
    "The _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. \n",
    "\n",
    "openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- After training complete: Log the training-loss file as a csv  --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_dir_path = \"tests/\"\n",
    "tests_output_dir_path = \"tests-output/\"\n",
    "\n",
    "def get_training_results_obj(ftm_id):\n",
    "    \"\"\" \n",
    "    INPUT: finetuned model id\n",
    "    OUTPUT: bytes object of results \n",
    "    \"\"\"\n",
    "    ftm=get_ftm_from_id(ftm_id)\n",
    "    # get the results file id\n",
    "    results_files= ftm['result_files']\n",
    "    print('results_files', results_files)\n",
    "    # for now just get the first one\n",
    "    first_results_file_id = results_files[0]['id']\n",
    "    print('first_results_file_id', first_results_file_id)\n",
    "    # get the results file\n",
    "    return openai.File.download(first_results_file_id)\n",
    "\n",
    "def get_training_results_df(ftm_id):\n",
    "    \"\"\" \n",
    "    INPUT: finetuned model id\n",
    "    - calls get_training_results_obj to download results object\n",
    "    - converts results object to csv and to df \n",
    "    OUTPUT: df of results \n",
    "    \"\"\"\n",
    "    # call the previous function \n",
    "    results_file_content = get_training_results_obj(ftm_id)\n",
    "    # Will also want a CSV file of the results\n",
    "    filepath = os.path.join(tests_output_dir_path, f\"training-results-{ftm_id}_results.csv\")\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(results_file_content)\n",
    "    # create df\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ftm_training_is_finished' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb Cell 54\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y104sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y104sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y104sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m ftm_training_is_finished:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y104sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     df \u001b[39m=\u001b[39m get_training_results_df(ftm_id)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y104sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ftm_training_is_finished' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_loss(df):\n",
    "    plt.plot(df['training_loss'])\n",
    "    plt.title('training loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "\n",
    "if ftm_training_is_finished:\n",
    "    df = get_training_results_df(ftm_id)\n",
    "    print(df.head())\n",
    "    plot_training_loss(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training_loss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminal outputs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "(ml) SF-mbp:gpt-fine-tuning stephen$ openai api fine_tunes.create \\\n",
    "> -t gpt-qa-train_prepared.jsonl \\\n",
    "> -v gpt-qa-valid_prepared.jsonl\n",
    "-m \"davinci\" \\\n",
    "--suffix \"gpt-ml-qa-pairs-A\"\n",
    "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50.8k/50.8k [00:00<00:00, 22.6Mit/s]\n",
    "Uploaded file from gpt-qa-train_prepared.jsonl: file-5fYU80lXy5FQgfmfVU56k8nb\n",
    "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.2k/12.2k [00:00<00:00, 8.80Mit/s]\n",
    "Uploaded file from gpt-qa-valid_prepared.jsonl: file-99tOIqCaPbNwL90bI1K1yq23\n",
    "Created fine-tune: ft-zHk8gGvZdaBNRVheXq5fZFqt\n",
    "Streaming events until fine-tuning is complete...\n",
    "\n",
    "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
    "[2022-11-20 00:36:48] Created fine-tune: ft-zHk8gGvZdaBNRVheXq5fZFqt\n",
    "[2022-11-20 00:36:52] Fine-tune costs $0.14\n",
    "[2022-11-20 00:36:52] Fine-tune enqueued. Queue number: 0\n",
    "[2022-11-20 00:36:54] Fine-tune started\n",
    "[2022-11-20 00:37:54] Completed epoch 1/4\n",
    "[2022-11-20 00:38:06] Completed epoch 2/4\n",
    "[2022-11-20 00:38:18] Completed epoch 3/4\n",
    "[2022-11-20 00:38:30] Completed epoch 4/4\n",
    "[2022-11-20 00:38:46] Uploaded model: curie:ft-personal-2022-11-20-08-38-46\n",
    "[2022-11-20 00:38:47] Uploaded result file: file-EZy5duulRUswzYdqj6LuW76a\n",
    "[2022-11-20 00:38:47] Fine-tune succeeded\n",
    "\n",
    "Job complete! Status: succeeded ðŸŽ‰\n",
    "Try out your fine-tuned model:\n",
    "\n",
    "openai api completions.create -m curie:ft-personal-2022-11-20-08-38-46 -p <YOUR_PROMPT>\n",
    "(ml) SF-mbp:gpt-fine-tuning stephen$ -m \"davinci\" \\\n",
    "> --suffix \"gpt-ml-qa-pairs-A\"\n",
    "-bash: -m: command not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying again \n",
    "\n",
    "$ openai api fine_tunes.create \\\n",
    "> -t gpt-qa-train_prepared.jsonl \\\n",
    "> -v gpt-qa-valid_prepared.jsonl \\\n",
    "> -m \"davinci\" \\\n",
    "> --suffix \"gpt-ml-qa-pairs-A\"\n",
    "Found potentially duplicated files with name 'gpt-qa-train_prepared.jsonl', purpose 'fine-tune' and size 50782 bytes\n",
    "file-5fYU80lXy5FQgfmfVU56k8nb\n",
    "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway:\n",
    "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50.8k/50.8k [00:00<00:00, 21.8Mit/s]\n",
    "Uploaded file from gpt-qa-train_prepared.jsonl: file-XCo6Sza9cVyfIWjPmoatpK3s\n",
    "Found potentially duplicated files with name 'gpt-qa-valid_prepared.jsonl', purpose 'fine-tune' and size 12227 bytes\n",
    "file-99tOIqCaPbNwL90bI1K1yq23\n",
    "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway:\n",
    "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.2k/12.2k [00:00<00:00, 6.49Mit/s]\n",
    "Uploaded file from gpt-qa-valid_prepared.jsonl: file-HbOaJkGRRu4hp4C8DQ4PtZ0t\n",
    "Created fine-tune: ft-xNzKl8ORDdASWIALy5WoSLcY\n",
    "Streaming events until fine-tuning is complete...\n",
    "\n",
    "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
    "[2022-11-20 00:45:39] Created fine-tune: ft-xNzKl8ORDdASWIALy5WoSLcY\n",
    "[2022-11-20 00:45:46] Fine-tune costs $1.42\n",
    "[2022-11-20 00:45:47] Fine-tune enqueued. Queue number: 0\n",
    "[2022-11-20 00:45:48] Fine-tune started\n",
    "[2022-11-20 00:47:30] Completed epoch 1/4\n",
    "[2022-11-20 00:47:50] Completed epoch 2/4\n",
    "[2022-11-20 00:48:10] Completed epoch 3/4\n",
    "[2022-11-20 00:48:30] Completed epoch 4/4\n",
    "[2022-11-20 00:49:22] Uploaded model: davinci:ft-sandbox:gpt-ml-qa-pairs-a-2022-11-20-08-49-22\n",
    "[2022-11-20 00:49:23] Uploaded result file: file-rGIPU3ndPAZXVRSBT4xhjDKy\n",
    "[2022-11-20 00:49:23] Fine-tune succeeded\n",
    "\n",
    "Job complete! Status: succeeded ðŸŽ‰\n",
    "Try out your fine-tuned model:\n",
    "\n",
    "openai api completions.create -m davinci:ft-sandbox:gpt-ml-qa-pairs-a-2022-11-20-08-49-22 -p <YOUR_PROMPT>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Test prompts --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_answer(prompt, ftm_name_, printit=False): \n",
    "\n",
    "    # prompt += \"\\n\\n###\\n\\n\"\n",
    "\n",
    "    if printit:\n",
    "        print(f\"creating call from {ftm_name_} with prompt: {prompt}\")\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=ftm_name_,\n",
    "        # engine=\"text-davinci-002\", \n",
    "        prompt=prompt,\n",
    "        temperature=0.9,\n",
    "        max_tokens=500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0.6,\n",
    "        stop=[\" END\"]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "# davinci:ft-sandbox:gpt-ml-qa-pairs-b-2022-11-20-09-29-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Run with the current model ---      \n",
    "\n",
    "--> Loads the model again from `ftm_id`\n",
    "\n",
    "Why? Only after it has trained will it have a `name`, which is needed to send as the `model` param to the OpenAI Completion endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "davinci:ft-sandbox:fastai-ft-2022-12-03-21-04-37 \n",
      "status: succeeded\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ftm = get_ftm_from_id(ftm_id)\n",
    "ftm_name = ftm['fine_tuned_model']\n",
    "ftm_status = ftm['status']\n",
    "\n",
    "print (f\"###\\n{ftm_name} \\nstatus: {ftm_status}\\n###\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run a single test prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating call from davinci:ft-sandbox:fastai-ft-2022-12-03-21-04-37 with prompt: What is the difference between a neural network and a deep learning model?\n",
      "\n",
      "\n",
      "A deep learning model is the neural network that has more than one layer.\n",
      "Deep learning is the application of neural networks for more than just classifying tabular data.\n",
      "What is a deep neural network?\n",
      "A neural network with more than one hidden layer.\n",
      "How can you implement a deep neural network in TensorFlow?\n",
      "TensorFlow provides the two functions multi_rnn and decoders in the 'encoders' module to implement a deep neural network with multiple layers of RNNs. \n",
      "\n",
      "Questions in part 2:\n",
      "What are the major differences between the implementations of multi-head and multi-rnn?\n",
      "The multi_rnn implementation requires a separate batch generator, ran_sentence , and vocabulary to be passed in, whereas the multi_head implementation combines the two functions into one. The multi_rnn implementation handles parallel output whereas the multi_head implementation concatenates the output from both models.\n",
      "The multi_rnn implementation requires a separate batch generator, ran_sentence , and vocabulary to be passed in, whereas the multi_head implementation combines the two functions into one. The multi_rnn implementation handles parallel output whereas the multi_head implementation concatenates the output from both models.\n",
      "What is the difference between the two differ_arrays and the two merge_op methods of the merge_decoders function?\n",
      "In both the differ_arrays and merge_decoders functions, there is one method to merge the output arrays of the two head models and one method to merge the output decoders of the two head models. In the former case, the two output arrays are first cast as tensors and then the two tensors are merged. In the latter case, the two output decoders are first cast into tensors and then the two tensors are merged.\n",
      "What is the difference between the two diff_arrays and the two merge_op methods of the merge_decoders function?\n",
      "In both the diff_arrays and merge_decoders functions, there are two methods to merge the output arrays of the two multi-rnn models and two methods to merge the output decoders of the two multi-rnn models. In the former case, the two output arrays are first cast as tensors and then the two tensors are merged. In the latter case, the two output decoders are first cast\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the difference between a neural network and a deep learning model?\" \n",
    "#ftm_name is defined after creating the ftm\n",
    "print(get_gpt_answer(prompt, ftm_name, printit=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run a battery of test prompts on the model from \"test-prompts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0: What is the difference between a CNN and a DNN? \n",
      "\n",
      "Answer 0: \n",
      "Generally, in deep neural networks (DNNs), several layers of neural networks are stacked together. In contrast, in Convolutional Neural Networks (CNNs), the neural network is split into two separate layers, the feature layer and the concatenation layer. Both are connected to the current input, but the feature layer usually has its weights randomly initialized in order to provide better initial prediction.\n",
      " \n",
      "How do we optimize a DNN? \n",
      " \n",
      "Instead of using simple mini-batch SGD with random initialization, momentum and various weight updates are used. \n",
      " \n",
      "In gradient descent, each parameter is updated based on the derivative of the loss function with respect to the parameter. For example, if we have a multilayered neural network with 3 layers, then we calculate the derivatives with respect to the first parameter for layer 1, the second parameter for layer 2 and the final parameter for layer 3. The same is done for the second parameter for layer 1 and the first for layer 2, and the final for layer 3. \n",
      " \n",
      "Momentum involves adding a fraction of the previous update to the current one. Concisely, the idea is that a good update in the past leads to future good updates. This can speed up training a lot.\n",
      " \n",
      "Instead of updating all parameters by the same amount, weight decay randomly alters the parameter updates to introduce more stability.\n",
      " \n",
      "Adadelta is an extension of momentum but with a different update. For each parameter, the update is the sum of the momentum, some part of the past update, and some part of the weight decay term. \n",
      " \n",
      "Rmsprop is another extension of momentum where the momentum is calculated exponentially. \n",
      " \n",
      "The key to successful Training of deep neural networks is to add regularization. \n",
      "The two key ideas are:\n",
      "1) Early stopping: stop training when the validation set loss stops improving.\n",
      "2) Random restarts: restart the model with a new random initialization at regular intervals. \n",
      " \n",
      "Other approaches include:\n",
      "- Dropout in which some random neurons are turned off.\n",
      "- Baseline models: use multiple simpler models and compare performance.\n",
      "- Adaptive learning rates.\n",
      "- Adversarial training: train two models on the same problem in order to speed up training by providing competing gradients.\n",
      "What is a good way to visualize data?  \n",
      "Use semantic embedding.  \n",
      "\n",
      "\n",
      "\n",
      "Question 1: How many roads must a man walk down? \n",
      "\n",
      "Answer 1: How many roads must a man walk down?\n",
      "How many roads must a man walk down? \n",
      "About seven times.\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Question 2: What does `DataLoaders` do in the fast.ai library? \n",
      "\n",
      "Answer 2: \n",
      "Define DataLoaders for reading files from disk\n",
      "Define serializers for converting objects to strings for passing to the DataLoaders\n",
      "Define a training procedure that iterates over batches of samples and calls the appropriate 'train' function of the model \n",
      "Define a D History classes for keeping track of various metrics for evaluating the effectiveness of training\n",
      "Define a TensorBoard class for visualizing learning progress through TensorBoard\n",
      "What is a DataLoader in the fast.ai library? \n",
      "Defines how to read in data from disk in batches\n",
      "Implements serialization into tensors for passing to the appropriate model function\n",
      "Defines a training procedure that iterates over batches of samples and calls the appropriate 'train' function of the model\n",
      "Defines a D History classes for keeping track of various metrics for evaluating the effectiveness of training\n",
      "Defines a TensorBoard class for visualizing learning progress through TensorBoard\n",
      "The book mentions that basic DataLoaders have been defined in the fastai library. What are they?\n",
      "What is a Resis tor in the fastai library? \n",
      "Stores previously loaded batches \n",
      "What are BatchedDataLoaders in the fastai library? \n",
      "Defines how to read in data from disk in batches (similar to DataLoaders)\n",
      "But also provides a way to evaluate the model on each batch (pass the 'eval_data' argument to the 'train' call) that is independent of the training step (pass the 'train_step' argument to the 'run' call)\n",
      "Allows separate tensorboards for train and eval\n",
      "What is a TrainStep in the fastai library? \n",
      "As mentioned, a step of training that includes both training of the gradient descent process as well as the model.\n",
      "How do you visualise progress through TensorBoard in the fastai library? \n",
      "Pass an additional argument to the 'run' call, 'tensorboard=tb'. Any number of DataLoaders and TrainSteps may be passed to the 'run' call.\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Question 3: Please write python code for a list comprehension\n",
      "\n",
      "Answer 3: \n",
      "that applies ix to element i of a list, with optional if\n",
      "else statements.\n",
      "for x in y :\n",
      "yield x\n",
      "yield x\n",
      "elif genfunc :\n",
      "else :\n",
      "elif else:\n",
      "elif else:\n",
      "def conditional ( test , when_true = None , when_false = None ):\n",
      "def conditional ( test , when_true = None , when_false = None ):\n",
      "def select ( indices , genfuncs ):\n",
      "def select_if ( pred , indices , genfuncs ):\n",
      "for ( index , expression ) in enumerate ( zip ( indices , genfuncs )):\n",
      "def chained ( genfuncs ):\n",
      "return chain ([ expr () for i in genfuncs ])\n",
      "def chained ( genfuncs ):\n",
      "return chain ([ expr () for i in genfuncs ])\n",
      "def choose ( * preds ):\n",
      "for pred in preds :\n",
      "def dropwhile ( pred , genfuncs ):\n",
      "def any ( pred , genfuncs ):\n",
      "return any ( pred , genfuncs )\n",
      "def all ( pred , genfuncs ):\n",
      "return all ( pred , genfuncs )\n",
      "def enumerate ( genfuncs ):\n",
      "def zip ( * iterables ):\n",
      "def izip ( * iterables ):\n",
      "def zip_eq ( * iterables ):\n",
      "def ifilter ( pred , genfuncs ):\n",
      "return ifilter ( pred , genfuncs )\n",
      "def ifilterfalse ( pred , genfuncs ):\n",
      "return ifilterfalse ( pred , genfuncs )\n",
      "def filterfalse ( pred , genfuncs ):\n",
      "def map ( genfuncs ):\n",
      "def map_dw ( genfuncs ):\n",
      "def imap ( * genfuncs ):\n",
      "def irmap ( * genfuncs ):\n",
      "genfuncs , else :( expr ,)\n",
      "kinds = [ 'elif' , 'else' ] + [ 'for' , 'while' , 'if' , 'else' ]\n",
      "for kind in kinds :\n",
      "for kind in kinds :\n",
      "def ifilter ( pred , genfuncs ):\n",
      "return ifilter ( pred , genfuncs )\n",
      "def ifilterfalse ( pred , genfuncs ):\n",
      "return ifilterfalse ( pred , genfuncs )\n",
      "def filterfalse ( pred , genfuncs ):\n",
      "def map ( genfuncs \n",
      "\n",
      "\n",
      "\n",
      "Question 4: Please write a bash script to save the output of a terminal command to a json file\n",
      "\n",
      "Answer 4: \n",
      "to create a dataset for machine learning.\n",
      "\n",
      "def save_json(output):\n",
      "forthefilepath,foriinrange(len(output)):\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Question 5: What are the steps for stochastic gradient descent? \n",
      "Answer 5:    Initialize the parameters (usually random).  Run for one step.  Update the parameters based on the current loss and current parameters.  Repeat forever.\n",
      "  Which are the components of a Tensor ?\n",
      "Dimensions, shape, dtype.\n",
      "How to create a tensor ?    tf.tensor(d_input,shape=d_shape, dtype=d_dtype)\n",
      "How to change the size of a tensor ?    new_shape = tensor.get_shape()\n",
      "new_data = tensâ€™*self.data\n",
      "self.data = self.data.rename(index=new_data.get_rr() )\n",
      "What are the basic operations of a tensor ?    Matrix product (elementwise multiplication).\n",
      "Forces (sum of products of gradients).    Dot product (addition of vectors).\n",
      "Which are the linear algebra operations in tensors ?     Matrix multiplication\n",
      "Einsum or outer product\n",
      "Sparse matrix ops (like Sparselinear)\n",
      "What is a Pushforward ?    \n",
      "Which Linear Algebra Operations are implemented in TensorFlow ?   Matrix multiplication (TensorFlow.matmul).\n",
      "Outer product (TensorFlow.tensordot).\n",
      "Sparse matrix ops (TensorFlow.sparse_xrays).\n",
      "What are the conditional operations that can be performed in TensorFlow?     Equal to (TensorFlow.eq).\n",
      "Less than or equal to (TensorFlow.le).\n",
      "Less than (TensorFlow.lt).\n",
      "Greater than or equal to (TensorFlow.ge).\n",
      "Greater than (TensorFlow.gt).\n",
      "Not equal to (TensorFlow.ne).\n",
      "Conditional expression involving several conditions with or (TensorFlow.or), xor (TensorFlow.xor), and or not (TensorFlow.not).\n",
      "When Conditional Ops are implemented ?      If-else (always)\n",
      "Select\n",
      "Average\n",
      "Maximum\n",
      "Minimum\n",
      "Subtract\n",
      "Add\n",
      "Divide\n",
      "Power\n",
      "When NumPy Ops are implemented ?    Arithmetic ops (sin, cos, etc)\n",
      "Basic mathematical functions (abs, sqrt, etc).\n",
      "When Basic Math Ops are implemented ?    Ar \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# save all the question, answer pairs to a csv\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('tests/test-prompts.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    with open('test-outputs/gpt-completions-to-test-prompts.csv', 'w',newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['prompt','completion'])\n",
    "        for i, line in enumerate(lines):\n",
    "            print(f'Question {i}: {line}')\n",
    "            answer = get_gpt_answer(line, ftm_name)\n",
    "            print(f'Answer {i}: {answer} \\n\\n\\n')\n",
    "            writer.writerow([line, answer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Run a previously trained model for testing (1) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for testing for now \n",
    "\n",
    "# ftm_id = \"ft-xXiSGiL0RrMUpKikJw09T1up\" # this model in particular was returning results with weird formatting, maybe something with the START and END sequences \n",
    "# ftm_name = get_ftmodel_name_from_id(ftm_id)\n",
    "\n",
    "# ftm_name = \"davinci:ft-sandbox:gpt-ml-qa-pairs-a-2022-11-20-08-49-22\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Previously trained example (2) --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the openai api with a prompt and get a response\n",
    "\n",
    "ftm_name = \"davinci:ft-sandbox:gpt-ml-qa-pairs-b-2022-11-20-09-29-25\"\n",
    "\n",
    "prompt = \"What is the difference between a neural network and a deep learning model?\" \n",
    "\n",
    "print(get_gpt_answer(prompt, ftm_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Gather Training results --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "def gather_training_results(ftm_trained):\n",
    "    \"\"\" \n",
    "    INPUT: finetuned model id\n",
    "    - calls get_training_results_obj to download results object\n",
    "    - converts results object to csv and to df \n",
    "    OUTPUT:\n",
    "    results \n",
    "    - results obj\n",
    "    - results df\n",
    "    - results csv path\n",
    "    losses\n",
    "    - train loss\n",
    "    - token loss\n",
    "    - valid loss\n",
    "    plots\n",
    "    - train, token, valid\n",
    "    \n",
    "    \"\"\"\n",
    "    # call the previous function \n",
    "    results_file_content = get_training_results_obj(ftm_trained)\n",
    "    # Will also want a CSV file of the results\n",
    "    # os path join a b \n",
    "\n",
    "    filepath = os.path.join(tests_dir_path, f\"training-results-{ftm_trained}_results.csv\")\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(results_file_content)\n",
    "    # create df\n",
    "    df = pd.DataFrame(results_file_content)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Hyperparameter Exploration --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparam loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparam job results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i could generate over several hyperparameters, \n",
    "# create a function that makes the call \n",
    "# saves the output \n",
    "# gets the finetuned model id\n",
    "# and then calls the next function with the finetuned model id as an argument\n",
    "\n",
    "\n",
    "# can i create a df of openai models and then iterate through it?\n",
    "# or do i need to create a function that takes the model id as an argument?\n",
    "# or do i need to create a function that takes the model id as an argument and then calls the next function with the finetuned model id as an argument?\n",
    "\n",
    "# i think i need to create a function that takes the model id as an argument and then calls the next function with the finetuned model id as an argument\n",
    "\n",
    "# function: takes in a dataset (just training set for now for simplicity) \n",
    "# outputs a finetuned model \"return\" \n",
    "\n",
    "# HYPERPARAMETER EXPLORATION \n",
    "\n",
    "EXPLORE_HYPERPARAMS=False\n",
    "\n",
    "# create a set of various hyperparameters to iterate through\n",
    "# create a ftmodel for each one \n",
    "# save the ftmodel id to a list\n",
    "# then iterate through the list and generate responses to the test-prompts for each one\n",
    "# save the responses to a list\n",
    "# then compare the responses to the test answers\n",
    "import time \n",
    "\n",
    "# wrap this in tqdm to show progress\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "# currsuffix = banana_suffix+suffix\n",
    "currsuffix = \"curie:ft-personal-2022-11-20-08-38-46\"\n",
    "\n",
    "def create_ftm_suffix(base,**kwargs):\n",
    "    str = base + \":\" + \"_\".join([f\"{k}-{v}, \" for k,v in kwargs.items()])\n",
    "    return str\n",
    "\n",
    "def already_exists_ftm(suffix):\n",
    "    all_ft_models = openai.FineTune.list()['data']\n",
    "\n",
    "    # this checks if the current name is already in the list of ft models\n",
    "    # any( ) returns True if any element in the list is True\n",
    "    exists = any([True for i in all_ft_models if suffix in i['fine_tuned_model']])\n",
    "    print('ftmodel already exists: ', suffix)\n",
    "    return exists\n",
    " \n",
    "\n",
    "banana_file = \"gpt-qa-train-formatted_prepared.jsonl\"\n",
    "\n",
    "base_suffix=\"qa-train-iterate\"\n",
    "# create a set of various hyperparameters to iterate through\n",
    "# epochs=[1,2,3,4]\n",
    "# learning_rate_multiplier=[0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "epochs=[1,2]\n",
    "learning_rate_multiplier=[0.1, 0.05, 0.01]\n",
    "\n",
    "suffixes=['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "ftmodels=pd.DataFrame(columns=['id', 'epochs', 'learning_rate_multiplier', 'suffix'])\n",
    "\n",
    "ftmodelcompletions = pd.DataFrame(columns=['id', 'ftm_name', 'epochs', 'learning_rate_multiplier', 'suffix', 'prompt','completion'])\n",
    "\n",
    "# create a ftmodel for each one\n",
    "\n",
    "def explore_hyperparams():\n",
    "\n",
    "    for epoch in tqdm(epochs):\n",
    "    # for epoch in epochs:\n",
    "\n",
    "        for lrm in tqdm(learning_rate_multiplier):\n",
    "\n",
    "            #check if the current model already exists. if so, skip it\n",
    "            curr_suffix = create_ftm_suffix(base_suffix,lrm=lrm, epoch=epoch)\n",
    "\n",
    "            # if already_exists_ftm(curr_suffix): continue\n",
    "\n",
    "            # @TODO actually this should say: \n",
    "            # if already exists, then don't create the finetuned model, just get the id\n",
    "            # if doesn't exist then create it, and then get the id \n",
    "\n",
    "            # create a ft model with these parameters \n",
    "            ftm = file_to_finetuned_model(\n",
    "                train_file=banana_file,\n",
    "                suffix=curr_suffix,\n",
    "                n_epochs=epoch,\n",
    "                learning_rate_multiplier=lrm\n",
    "            )\n",
    "\n",
    "            ftm_id = ftm['id']\n",
    "\n",
    "            print(f\"\\n\\n======================= Initiated ftm:  ======================= \\n ftm_id: {ftm_id}. \\n Now we wait for the model to train... \\n\")\n",
    "\n",
    "            # wait for the ft model to finish training\n",
    "            current_ft_model_is_ready = await_ft_ready(ftm_id)\n",
    "            if not current_ft_model_is_ready: \n",
    "                print(f\"finetuned model timed out: {ftm_id}\")\n",
    "                continue\n",
    "\n",
    "            # the model has passed the await test, so it can be called now\n",
    "            # it's also been given a name, so it can be called by ftm_name now\n",
    "            # ftm_name = ftm['fine_tuned_model'] # the name is stored as ftm['fine_tuned_model']\n",
    "\n",
    "            ftm = openai.FineTune.retrieve(ftm_id) # need to retrieve the fresh version with its name! \n",
    "            \n",
    "            ftm_name = ftm['fine_tuned_model']\n",
    "\n",
    "            print (ftm_name, curr_suffix)\n",
    "            #also we could derive the name from the suffix! although they add a timestamp to the end of the name, which we'd want to emulate \n",
    "\n",
    "\n",
    "            print (f\"{ftm_id} finetuned model is now ready for testing. \\nIts name is: {ftm_name}\\n\")\n",
    "\n",
    "            # append this model to the df \n",
    "            ftmodels = ftmodels.append({'id': ftm_id, 'ftm_name': ftm_name, 'epochs': epoch, 'learning_rate_multiplier': lrm, 'suffix': curr_suffix}, ignore_index=True)\n",
    "\n",
    "            # now test every prompt with every ftmodel\n",
    "            with open ('test-prompts.txt') as f:\n",
    "                print(f\"starting test-prompts on finetuned model {ftm_id}\")\n",
    "                prompts = f.readlines()\n",
    "                for prompt in tqdm(prompts):\n",
    "                    # get the completion for this prompt\n",
    "                    completion = get_gpt_answer(prompt, ftm_name, printit=True)\n",
    "                    # append this completion to the df\n",
    "                    ftmodelcompletions = ftmodelcompletions.append(\n",
    "                        {'id': ftm_id, \n",
    "                        'ftm_name': ftm_name,\n",
    "                        'epochs': epoch, \n",
    "                        'learning_rate_multiplier': lrm, \n",
    "                        'suffix': curr_suffix, \n",
    "                        'prompt': prompt, \n",
    "                        'completion': completion}, \n",
    "                        ignore_index=True)\n",
    "                print('\\n\\n\\nCompleted test prompts for ftmodel: ', ftm_id)\n",
    "\n",
    "    ftmodels.to_csv('ftmodels.csv')\n",
    "    ftmodelcompletions.to_csv('ftmodelcompletions.csv')\n",
    "\n",
    "    print(ftmodels.head())\n",
    "    print(ftmodelcompletions.head())\n",
    "\n",
    "\n",
    "if EXPLORE_HYPERPARAMS: \n",
    "    explore_hyperparams()\n",
    "\n",
    "    # SUCCESS! \n",
    "    print(ftmodels.head())\n",
    "    print(ftmodelcompletions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Analyze results --\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTune fine-tune id=ft-LAgPtBHLN5GvgQ23TaH3lCt4 at 0x7ff9c1710e00> JSON: {\n",
       "  \"created_at\": 1670101035,\n",
       "  \"events\": [\n",
       "    {\n",
       "      \"created_at\": 1670101035,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Created fine-tune: ft-LAgPtBHLN5GvgQ23TaH3lCt4\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    },\n",
       "    {\n",
       "      \"created_at\": 1670101047,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune costs $2.49\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    },\n",
       "    {\n",
       "      \"created_at\": 1670101048,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune enqueued. Queue number: 0\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    },\n",
       "    {\n",
       "      \"created_at\": 1670101050,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune started\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    },\n",
       "    {\n",
       "      \"created_at\": 1670101206,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Completed epoch 1/4\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    },\n",
       "    {\n",
       "      \"created_at\": 1670101280,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Completed epoch 2/4\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    },\n",
       "    {\n",
       "      \"created_at\": 1670101355,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Completed epoch 3/4\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    },\n",
       "    {\n",
       "      \"created_at\": 1670101428,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Completed epoch 4/4\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    },\n",
       "    {\n",
       "      \"created_at\": 1670101477,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Uploaded model: davinci:ft-sandbox:fastai-ft-2022-12-03-21-04-37\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    },\n",
       "    {\n",
       "      \"created_at\": 1670101479,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Uploaded result file: file-TFO1w2R7BX5ypdbyU30AYFJu\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    },\n",
       "    {\n",
       "      \"created_at\": 1670101479,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune succeeded\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    }\n",
       "  ],\n",
       "  \"fine_tuned_model\": \"davinci:ft-sandbox:fastai-ft-2022-12-03-21-04-37\",\n",
       "  \"hyperparams\": {\n",
       "    \"batch_size\": 1,\n",
       "    \"learning_rate_multiplier\": 0.1,\n",
       "    \"n_epochs\": 4,\n",
       "    \"prompt_loss_weight\": 0.01\n",
       "  },\n",
       "  \"id\": \"ft-LAgPtBHLN5GvgQ23TaH3lCt4\",\n",
       "  \"model\": \"davinci\",\n",
       "  \"object\": \"fine-tune\",\n",
       "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
       "  \"result_files\": [\n",
       "    {\n",
       "      \"bytes\": 46961,\n",
       "      \"created_at\": 1670101478,\n",
       "      \"filename\": \"compiled_results.csv\",\n",
       "      \"id\": \"file-TFO1w2R7BX5ypdbyU30AYFJu\",\n",
       "      \"object\": \"file\",\n",
       "      \"purpose\": \"fine-tune-results\",\n",
       "      \"status\": \"processed\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ],\n",
       "  \"status\": \"succeeded\",\n",
       "  \"training_files\": [\n",
       "    {\n",
       "      \"bytes\": 95616,\n",
       "      \"created_at\": 1670101035,\n",
       "      \"filename\": \"file\",\n",
       "      \"id\": \"file-3RyNWMqW4m6Eu6lq7Do9MkuI\",\n",
       "      \"object\": \"file\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"status\": \"processed\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ],\n",
       "  \"updated_at\": 1670101479,\n",
       "  \"validation_files\": []\n",
       "}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myftm = get_ftm_from_id(ftm_id)\n",
    "myftm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Chain it all together --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load params\n",
    "\n",
    "pc_csv_IN_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw CSV -> formatted JSONL for fine-tuning \n",
    "formatted_pc_jsonl_path = format_csv_for_ft()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d905c6a59c21f0f46be93fdc832728644d115a3fdfd57971d06d899b53e0576e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
