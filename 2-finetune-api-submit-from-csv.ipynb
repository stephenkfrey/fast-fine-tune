{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Fine-tuning API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# set the api key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Notes on formatting for GPT fine-tuning ####################\n",
    "\n",
    "# - Based on your file extension, your file is formatted as a CSV file\n",
    "# - Your file contains 56 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
    "# - Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
    "# - All completions end with suffix ` `\n",
    "#   WARNING: Some of your completions contain the suffix ` ` more than once. We suggest that you review your completions and add a unique ending\n",
    "# - The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
    "\n",
    "# add a space to the end of each prompt\n",
    "# Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\n\\n###\\n\\n. The separator should not appear elsewhere in any prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Load raw CSV files of prompt-completion pairs --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPT Chat\n",
    "pc_csv_IN_path =\"data/gpt-qa-df.csv\"\n",
    "curr_suffix_name = \"gpt-ml-qa-pairs-C\"\n",
    "\n",
    "assert os.path.exists(pc_csv_IN_path), \"File does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FastAI Q&A pairs \n",
    "pc_csv_IN_path =\"data/fastai-qa-cleaned.csv\"\n",
    "curr_suffix_name = \"fastai-ft\"\n",
    "\n",
    "assert os.path.exists(pc_csv_IN_path), \"File does not exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Format raw CSV --> formatted CSV --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####\n",
      "Raw CSV -> Formatted CSV conversion complete\n",
      "#####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# INPUTS: \n",
    "# - csv: a csv with headers 'prompt' and 'completion'\n",
    "\n",
    "# PROCESSING: \n",
    "# - add a space to the end of each prompt\n",
    "# - add a \\n\\n###\\n\\n to the end of each prompt\n",
    "# - add a space to the beginning of each completion\n",
    "# - add a \\n\\n<+++>\\n\\n to the end of each completion\n",
    "\n",
    "# saves to df\n",
    "\n",
    "# OUTPUTS:\n",
    "# - csv: a cleaned, formatted csv ready for fine-tuning \n",
    "\n",
    "def format_csv_for_ft(qa_csv_path):\n",
    "    #### Check formatting on CSV \n",
    "    assert qa_csv_path[-4:] == '.csv', \"input path does not end with '.csv'\"\n",
    "    # - headers are 'prompt' and 'completion'\n",
    "    assert pd.read_csv(qa_csv_path, nrows=0).columns.tolist() == ['prompt', 'completion'], \"file headers are not 'prompt' and 'completion'\"\n",
    "\n",
    "    gpt_formatted_df = pd.read_csv(qa_csv_path)\n",
    "    # print(gpt_loaded_df.head()) # double check input \n",
    "\n",
    "    #### Format for fine-tuning \n",
    "    # 1) Completions: start with a space ' ', end with a seperator '\\n+END+\\n'\n",
    "    # Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.\n",
    "    # Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\n, ###, or any other token that does not appear in any completion.\n",
    "    gpt_formatted_df['completion'] = gpt_formatted_df['completion'].apply(lambda x: ' ' + x + '\\n+END+\\n')\n",
    "    # 2) add a separater to the end of each prompt \n",
    "    gpt_formatted_df['prompt'] = gpt_formatted_df['prompt'].apply(lambda x: x + '\\n\\n###\\n\\n')\n",
    "\n",
    "    #### Save to csv \n",
    "    qa_csv_OUT_path=os.path.join(pc_csv_IN_path[:-4]+\"-formatted-CSV-FIRST.csv\")\n",
    "    gpt_formatted_df.to_csv(qa_csv_OUT_path, index=False)\n",
    "    return qa_csv_OUT_path\n",
    "\n",
    "formatted_pc_csv_path = format_csv_for_ft(pc_csv_IN_path)\n",
    "\n",
    "print(\"\\n#####\\nRaw CSV -> Formatted CSV conversion complete\\n#####\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE splitting train and valid set; not used right now \n",
    "\n",
    "# def split_df_train_test(df, train_frac=0.8):\n",
    "#     # split the df into train and test\n",
    "#     train_df = df.sample(frac=train_frac, random_state=42)\n",
    "#     test_df = df.drop(train_df.index)\n",
    "\n",
    "#     return train_df, test_df\n",
    "\n",
    "######\n",
    "# ## # get train and test df from the previous 'formatted_qa_df'\n",
    "# qa_train_df, qa_test_df = split_df_train_test(formatted_qa_df)\n",
    "\n",
    "# #create paths to save \n",
    "# qa_TEST_OUT_path = f\"{qa_csv_IN_path[:-4]}-TRAIN.csv\"\n",
    "# qa_TRAIN_OUT_path = f\"{qa_csv_IN_path[:-4]}-TEST.csv\"\n",
    "\n",
    "# # # save the train and test dfs to csv \n",
    "# qa_train_df.to_csv(qa_TEST_OUT_path, index=False)\n",
    "# qa_test_df.to_csv(qa_TRAIN_OUT_path, index=False)\n",
    "\n",
    "# print ('train/test shapes: ', qa_train_df.shape, qa_test_df.shape)\n",
    "# print ('train/test heads: \\n\\n', qa_train_df.head(),qa_test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV -> JSONL, using the OpenAI CLI tool\n",
    "\n",
    "*Typically, we call the following in terminal to convert the file:*\n",
    "\n",
    "`openai tools fine_tunes.prepare_data -f <LOCAL_FILE>`\n",
    "\n",
    "The following section automates this with a bash script "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@TODO \n",
    "Eventually, run a bash script that gets the output from terminal and displays to user \n",
    "to make choices about data formatting\n",
    "for now lets just do it manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_conversion_sh_script_path = './scripts/csv2jsonl_openai.sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assure permissions for the bash script and file \n",
    "! chmod a+x {jsonl_conversion_sh_script_path}\n",
    "! chmod a+x {formatted_pc_csv_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Based on your file extension, your file is formatted as a CSV file\n",
      "- Your file contains 223 prompt-completion pairs\n",
      "- All prompts end with suffix `\\n\\n###\\n\\n`\n",
      "- All completions end with suffix `\\n+END+\\n`\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Necessary] Your format `CSV` will be converted to `JSONL`\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: \n",
      "Wrote modified file to `data/fastai-qa-cleaned-formatted-CSV-FIRST_prepared (1).jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"data/fastai-qa-cleaned-formatted-CSV-FIRST_prepared (1).jsonl\"\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"\\n+END+\\n\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 5.51 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n",
      "Done!\n",
      "\n",
      "./scripts/csv2jsonl_openai.sh: line 18: syntax error near unexpected token `done'\n",
      "./scripts/csv2jsonl_openai.sh: line 18: `done'\n",
      "\n",
      "data/fastai-qa-cleaned-formatted-CSV-FIRST_prepared.jsonl\n",
      "\n",
      "#####\n",
      "CSV -> JSONL conversion complete\n",
      "#####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Run the OpenAI CLI tool to format the CSV to JSONL \n",
    "import subprocess\n",
    "# you need to add './' before the filename for bash to recognize it \n",
    "\n",
    "assert os.path.exists(formatted_pc_csv_path), \"file does not exist\"\n",
    "assert os.path.exists(jsonl_conversion_sh_script_path), \"sh file does not exist\"\n",
    "\n",
    "# Call the bash script to format the CSV to JSONL using the OpenAI CLI tool \n",
    "process = subprocess.Popen([jsonl_conversion_sh_script_path, formatted_pc_csv_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "if process is not None:\n",
    "    stdout, stderr = process.communicate()\n",
    "    print(stdout.decode('utf-8'))\n",
    "    print(stderr.decode('utf-8'))\n",
    "    \n",
    "    \n",
    "# get the new JSONL filename \n",
    "formatted_pc_jsonl_path = formatted_pc_csv_path[:-4] + '_prepared.jsonl'\n",
    "assert (os.path.exists(formatted_pc_jsonl_path)), \"JSONL file does not exist\"\n",
    "print(formatted_pc_jsonl_path)\n",
    "print(\"\\n#####\\nCSV -> JSONL conversion complete\\n#####\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to format the API call to the fine tuned model \n",
    "\n",
    "### format the prompt from the user before sending it \n",
    "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string \n",
    "\n",
    "`\\n\\n###\\n\\n` \n",
    "\n",
    "for the model to start generating completions, rather than continuing with the prompt. \n",
    "\n",
    "### param in API call for stop sequence \n",
    "Make sure to include \n",
    "\n",
    "`stop=[\" \\n<+++>\\n\"]` \n",
    "\n",
    "so that the generated texts ends at the expected place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### an example call to create a ft model in terminal: \n",
    "\n",
    "openai api fine_tunes.create \\\n",
    "-t gpt-qa-train_prepared.jsonl \\\n",
    "-v gpt-qa-valid_prepared.jsonl \\\n",
    "-m \"davinci\" \\\n",
    "--suffix \"gpt-ml-qa-pairs-A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Functions to create a fine-tuned model from the JSONL file --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# HELPER FUNCTIONS\n",
    "#####################\n",
    "\n",
    "# FUNCTION \n",
    "# upload_file_openai\n",
    "# - takes in a jsonl formatted dataset (just training set for now for simplicity)\n",
    "# - eventually could abstract to general file upload but not needed for now\n",
    "def upload_jsonl_ft_to_openai(filename): \n",
    "    # assert the filetype is jsonl \n",
    "    ext = os.path.splitext(filename)[-1].lower()\n",
    "    print ('ext:', ext)\n",
    "    assert ext== '.jsonl', \"filetype must be jsonl\"\n",
    "    r = openai.File.create(\n",
    "        file=open(filename, \"rb\"),\n",
    "        purpose='fine-tune'\n",
    "    )\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# FUNCTION \n",
    "# create_finetuned_model\n",
    "\n",
    "# INPUTS: \n",
    "# - train_file_id: a csv with headers 'prompt' and 'completion'\n",
    "# - model type\n",
    "# - hyperparameters\n",
    "\n",
    "# OUTPUTS:\n",
    "# finetuned_model_response: a response object including the finetuned model id\n",
    "\n",
    "# @TODO add the ability to turn on and off test-train splitting \n",
    "\n",
    "def create_finetuned_model(train_file_id, valid_file_id='', model=\"davinci-003\", learning_rate_multiplier='', n_epochs='', suffix=''):\n",
    "\n",
    "    #####################\n",
    "    # create the request to build the finetuned model \n",
    "    try: \n",
    "        finetuned_model_response = openai.FineTune.create(\n",
    "            training_file=train_file_id,\n",
    "            # validation_file=valid_file_id, # ignore valid file for now\n",
    "            model=model,\n",
    "            learning_rate_multiplier=learning_rate_multiplier,\n",
    "            n_epochs=n_epochs,\n",
    "            suffix=suffix,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print (\"Error creating finetuned model: \", e)\n",
    "        return None\n",
    "    return finetuned_model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE \n",
    "# this is the old way using a train/valid set \n",
    "\n",
    "# _train_file_id = upload_file_openai('gpt-qa-train-formatted_prepared.jsonl')['id']\n",
    "# _valid_file_id = upload_file_openai('gpt-qa-valid-formatted_prepared.jsonl')['id']\n",
    "\n",
    "# ftm = create_finetuned_model(\n",
    "#     train_file_id=_train_file_id,\n",
    "#     valid_file_id=_valid_file_id,\n",
    "#     model=\"davinci\",\n",
    "#     learning_rate_multiplier=0.05,\n",
    "#     n_epochs=1,\n",
    "#     suffix=suffix_name,\n",
    "# )\n",
    "\n",
    "# ftm_id = ftm['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the finetuned model id to the list.csv file, along with the datetime\n",
    "\n",
    "def log_finetuned_model_id(ftm_id, model_name, learning_rate_multiplier, n_epochs, suffix, log_file_path):\n",
    "    # get the current datetime \n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    # create the row to append \n",
    "    row = f\"{dt_string},{ftm_id},{model_name},{learning_rate_multiplier},{n_epochs},{suffix}\" \n",
    "    # append the row to the log file\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        f.write(row + '\\n')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# FUNCTION: file_to_finetuned_model\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "file_to_finetuned_model_wrapper\n",
    "\n",
    "Inputs: \n",
    "    filename: \n",
    "        - filename path of a csv file to be uploaded to openai\n",
    "        - must be formatted as a csv with headers 'prompt' and 'completion' - see the functions above to do that \n",
    "            eg \"/data/my-prompt-completion-pairs-list.csv\"\n",
    "\n",
    "    model: string, name of the model to be finetuned \n",
    "            eg \"davinci-003\"\n",
    "    \n",
    "    learning_rate_multiplier: float, learning rate multiplier for the finetuned model \n",
    "            eg \"0.05\", typically 0.05 - 0.2\n",
    "    \n",
    "    n_epochs: int, number of epochs for the finetuned model \n",
    "            eg \"1\", typically 1 - 3\n",
    "    \n",
    "    suffix: string, suffix to be added to the finetuned model \n",
    "            eg \"my-finetuned-model\"\n",
    "\n",
    "Outputs: \n",
    "    finetuned_model_response: the response of the OpenAI API call to create the finetuned model\n",
    "        - a dictionary with the finetuned model id, hyperparameters, model name, etc.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "\n",
    "def file_to_finetuned_model_wrapper(train_file, valid_file='', model=\"davinci\", learning_rate_multiplier=0.1, n_epochs=4, suffix=''):\n",
    "    # upload files to openai\n",
    "    if train_file:\n",
    "        assert(train_file.endswith('.jsonl')), \"train_file must be a .jsonl file\"\n",
    "        _train_file_id = upload_jsonl_ft_to_openai(train_file)['id']\n",
    "        print ('Successfully uploaded train JSONL file. train_file_id:', _train_file_id)\n",
    "    else:\n",
    "        FileNotFoundError('Must include a train_file')\n",
    "\n",
    "    if valid_file != '':\n",
    "        assert(valid_file.endswith('.jsonl')), \"valid_file must be a .jsonl file\"\n",
    "        _valid_file_id = upload_jsonl_ft_to_openai(valid_file)['id']\n",
    "    else: \n",
    "        _valid_file_id = ''\n",
    "\n",
    "    # ensure suffix is within the required length; if not then truncate\n",
    "    if len(suffix) > 40:\n",
    "        suffix = suffix[:40]\n",
    "        print('Suffix longer than required 40-character length; truncating to 40 characters: ', suffix)\n",
    "    \n",
    "    \n",
    "    # create a finetuned model\n",
    "    finetuned_model_response = create_finetuned_model(\n",
    "        train_file_id=_train_file_id,\n",
    "        # valid_file_id=_valid_file_id,\n",
    "        model=model,\n",
    "        learning_rate_multiplier=learning_rate_multiplier,\n",
    "        n_epochs=n_epochs,\n",
    "        suffix=suffix,\n",
    "    )\n",
    "\n",
    "    return finetuned_model_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Create a ftm from the JSONL file --- \n",
    "The magic is here! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ext: .jsonl\n",
      "Successfully uploaded train JSONL file. train_file_id: file-AGJfhVqwZDX8uDcXtH0GcX46\n",
      "\n",
      "###\n",
      "Created fine-tuned model None\n",
      "###\n",
      "\n",
      "ftm_name: None\n",
      "ftm_id: None\n",
      "ftm: {\n",
      "  \"created_at\": 1670062197,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1670062197,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-NM7Jg1qiG2Fco1Wnbl6BoDOn\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.1,\n",
      "    \"n_epochs\": 4,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-NM7Jg1qiG2Fco1Wnbl6BoDOn\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 96731,\n",
      "      \"created_at\": 1670062197,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-AGJfhVqwZDX8uDcXtH0GcX46\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1670062197,\n",
      "  \"validation_files\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ftm = file_to_finetuned_model_wrapper(\n",
    "    train_file=formatted_pc_jsonl_path, #from the previous step \n",
    "    suffix=curr_suffix_name # declared at the beginning of the ntoebook \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log each element of the json response as a column in a csv file\n",
    "from datetime import datetime\n",
    "\n",
    "def log_ftm(ftm):\n",
    "    \"\"\"\n",
    "    # Checks if the ftm id is already in the log file. If not, then adds to log file. \n",
    "    # INPUTS: ftm - the json response from the OpenAI API call to create a finetuned model\n",
    "    # OUTPUTS: True/False\n",
    "    \"\"\"\n",
    "    log_file_path = 'outputs/finetuned-models-log.csv'\n",
    "\n",
    "    if not os.path.exists(log_file_path):\n",
    "        with open(log_file_path, 'w') as f:\n",
    "            f.write('datetime,created_at,ftm_id, status\\n')\n",
    "\n",
    "    # check if the ftm id already exists in the csv \n",
    "    pattern = ftm['id']\n",
    "    with open(log_file_path, 'r') as csvfile:\n",
    "        if any(map(lambda x: pattern == x.rstrip(), csvfile)): # iterates through text looking for match\n",
    "            print (\"Finetuned model id already exists in log file\")\n",
    "            return False\n",
    "        else:\n",
    "            with open(log_file_path, 'a') as f:\n",
    "                f.write(f\"{datetime.now()}, {ftm['created_at']},{ftm['id']}, {ftm['status']}\\n\")\n",
    "            return True\n",
    "    \n",
    "\n",
    "log_ftm(ftm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###\n",
      "Created fine-tuned model ft-NM7Jg1qiG2Fco1Wnbl6BoDOn\n",
      "###\n",
      "\n",
      "ftm_name:  None\n",
      "ftm_id:  ft-NM7Jg1qiG2Fco1Wnbl6BoDOn\n",
      "status:  pending\n",
      "ftm:  {\n",
      "  \"created_at\": 1670062197,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1670062197,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-NM7Jg1qiG2Fco1Wnbl6BoDOn\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.1,\n",
      "    \"n_epochs\": 4,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-NM7Jg1qiG2Fco1Wnbl6BoDOn\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 96731,\n",
      "      \"created_at\": 1670062197,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-AGJfhVqwZDX8uDcXtH0GcX46\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1670062197,\n",
      "  \"validation_files\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ftm_name = ftm['fine_tuned_model']\n",
    "ftm_id = ftm['id']\n",
    "\n",
    "print(f\"\\n###\\nCreated fine-tuned model {ftm_id}\\n###\\n\")\n",
    "print ('ftm_name: ', ftm_name)\n",
    "print ('ftm_id: ', ftm_id)\n",
    "print ('status: ', ftm['status'])\n",
    "print ('ftm: ', ftm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Test with FastAI Question-Answer Pairs data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ext: .jsonl\n",
      "Successfully uploaded train JSONL file. train_file_id: file-xUD0WfFc5NSvjEcdSPjmhFjN\n",
      "Suffix longer than required 40-character length; truncating to 40 characters:  fastai-qa-cleaned-formatted-CSV-FIRST_pr\n"
     ]
    }
   ],
   "source": [
    "ftm_fastai = file_to_finetuned_model_wrapper(\n",
    "    train_file='data/fastai-qa-cleaned-formatted-CSV-FIRST_prepared.jsonl', \n",
    "    suffix='fastai-qa-cleaned-formatted-CSV-FIRST_prepared',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id  ft-WIixwlrf7V7FS5hpHj9SG9QY\n",
      "name  None\n",
      "status  pending\n",
      "ftm:  {\n",
      "  \"created_at\": 1670062226,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1670062226,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-WIixwlrf7V7FS5hpHj9SG9QY\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.1,\n",
      "    \"n_epochs\": 4,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-WIixwlrf7V7FS5hpHj9SG9QY\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 96731,\n",
      "      \"created_at\": 1670062226,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-U7HzG2MrVw7TpMPzcWzEmS4A\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1670062226,\n",
      "  \"validation_files\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('id ', ftm_fastai['id'])\n",
    "print('name ',ftm_fastai['fine_tuned_model'])\n",
    "print('status ', ftm_fastai['status'])\n",
    "print ('ftm: ', ftm_fastai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Test with GPT Chat log data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ext: .jsonl\n",
      "Successfully uploaded train JSONL file. train_file_id: file-eNPg3CZs8FhJAyUAK5l3wRWl\n",
      "id:  ft-pLLITeGYtGtpmdbJRIrhDEaf\n",
      "name:  None\n"
     ]
    }
   ],
   "source": [
    "# TEST GPT CHAT DATA \n",
    "# test the file_to_finetuned_model function\n",
    "\n",
    "banana_file = \"data/gpt-qa-train-formatted_prepared.jsonl\"\n",
    "banana_suffix=\"qa-train-iterate\"\n",
    "\n",
    "ftm_gptchat = file_to_finetuned_model_wrapper(\n",
    "    train_file=banana_file,\n",
    "    suffix=banana_suffix + \"-z\"\n",
    ")\n",
    "\n",
    "print('id: ',ftm_gptchat['id'])\n",
    "print('name: ',ftm_gptchat['fine_tuned_model'])\n",
    "\n",
    "# ftm is only available after the model has been created!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-12-50-36\n",
      "ft-xXiSGiL0RrMUpKikJw09T1up\n"
     ]
    }
   ],
   "source": [
    "# HELPERS \n",
    "\n",
    "def get_ftmodel_name_from_id(id):\n",
    "    ftmodel = openai.FineTune.retrieve(id)\n",
    "    return ftmodel['fine_tuned_model']\n",
    "\n",
    "def get_ftmodel_id_from_name(name):\n",
    "    all_ft_models = openai.FineTune.list()['data']\n",
    "    for ftmodel in all_ft_models:\n",
    "        if name in ftmodel['fine_tuned_model']:\n",
    "            return ftmodel['id']\n",
    "    return None\n",
    "\n",
    "print(get_ftmodel_name_from_id(\"ft-xXiSGiL0RrMUpKikJw09T1up\"))\n",
    "print(get_ftmodel_id_from_name(\"davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-12-50-36\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET FTM AWAIT READY \n",
    "\n",
    "# the finetuned model needs to finish training before we can use it\n",
    "# so let's track its status and only make calls when its' ready \n",
    "# we can use the finetuned model id to check its status\n",
    "\n",
    "import time\n",
    "\n",
    "def get_finetuned_model_status(finetuned_model_id):\n",
    "    response = openai.FineTune.retrieve(id=finetuned_model_id)\n",
    "    return response['status']\n",
    "\n",
    "#####\n",
    "# FUNCTION: await_ft_ready\n",
    "# INPUT: finetuned_model_id, seconds_of_patience\n",
    "# prints every check_every_n_seconds seconds the status of the mdoel \n",
    "# times out after seconds_of_patience seconds\n",
    "# OUTPUT: True if finetuned model is ready, False if finetuned model is not ready\n",
    "\n",
    "def await_ft_ready(finetuned_model_id, seconds_of_patience=400):\n",
    "\n",
    "    check_every_n_seconds = 10\n",
    "    # round down \n",
    "    num_checkins = seconds_of_patience//check_every_n_seconds # number of times to check the status of the finetuned model\n",
    "    print(f\"###\\nWaiting for {finetuned_model_id} to train.\\n###\\n\")\n",
    "\n",
    "    finetuned_model_status = None \n",
    "    counter = 0\n",
    "\n",
    "    while True:\n",
    "        status = get_finetuned_model_status(finetuned_model_id)\n",
    "        if status == 'succeeded':\n",
    "            print(f\"###\\n{finetuned_model_id} is ready!\\n###\\n\")\n",
    "            print (openai.FineTune.retrieve(id=finetuned_model_id))\n",
    "            return True\n",
    "        else:\n",
    "            # print (f\"{finetuned_model_id} status: {status}\")\n",
    "            # print(f\"{finetuned_model_id} finetuned model is still training. {patience*10} seconds so far.\")\n",
    "            # print ('counter',counter)\n",
    "            counter +=1\n",
    "            if counter % check_every_n_seconds == 0: \n",
    "                print(f\"{finetuned_model_id} has trained for at least {counter*check_every_n_seconds} seconds so far. Status: {status}\")\n",
    "            if counter > num_checkins: \n",
    "                # print that the current timed out\n",
    "                print(f\"{finetuned_model_id} finetuned model has taken longer than {seconds_of_patience} to train, since we started checking in on its status. Calling exceeds patience variable. Exiting await-train loop.\")\n",
    "                print (openai.FineTune.retrieve(id=finetuned_model_id))\n",
    "                return False\n",
    "            time.sleep(check_every_n_seconds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "Waiting for ft-xXiSGiL0RrMUpKikJw09T1up to train.\n",
      "###\n",
      "\n",
      "###\n",
      "ft-xXiSGiL0RrMUpKikJw09T1up is ready!\n",
      "###\n",
      "\n",
      "{\n",
      "  \"created_at\": 1668948495,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1668948495,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-xXiSGiL0RrMUpKikJw09T1up\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668948501,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune costs $0.35\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668948502,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune enqueued. Queue number: 0\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668948503,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune started\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668948604,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 1/1\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668948636,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded model: davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-12-50-36\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668948637,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded result file: file-sl65W26kQDpovGBIDKW51rGv\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668948637,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune succeeded\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": \"davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-12-50-36\",\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"learning_rate_multiplier\": 0.1,\n",
      "    \"n_epochs\": 1,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-xXiSGiL0RrMUpKikJw09T1up\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
      "  \"result_files\": [\n",
      "    {\n",
      "      \"bytes\": 2411,\n",
      "      \"created_at\": 1668948637,\n",
      "      \"filename\": \"compiled_results.csv\",\n",
      "      \"id\": \"file-sl65W26kQDpovGBIDKW51rGv\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune-results\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 50561,\n",
      "      \"created_at\": 1668948494,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-JkoWyo1PAKeYeNbGmWTIftuE\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1668948637,\n",
      "  \"validation_files\": []\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "ftm_trained_id ='ft-xXiSGiL0RrMUpKikJw09T1up'\n",
    "await_ft_ready(ftm_trained_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~ Flip this on if you want it to wait on the actual ftm we just loaded ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "Waiting for ft-NM7Jg1qiG2Fco1Wnbl6BoDOn to train.\n",
      "###\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y242sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m await_ft_ready(ftm_id)\n",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb Cell 38\u001b[0m in \u001b[0;36mawait_ft_ready\u001b[0;34m(finetuned_model_id, seconds_of_patience)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y242sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mprint\u001b[39m (openai\u001b[39m.\u001b[39mFineTune\u001b[39m.\u001b[39mretrieve(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39mfinetuned_model_id))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y242sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/2-finetune-api-submit-from-csv.ipynb#Y242sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m time\u001b[39m.\u001b[39;49msleep(check_every_n_seconds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "await_ft_ready(ftm_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files.\n",
    "\n",
    "The _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. \n",
    "\n",
    "openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminal outputs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "(ml) SF-mbp:gpt-fine-tuning stephen$ openai api fine_tunes.create \\\n",
    "> -t gpt-qa-train_prepared.jsonl \\\n",
    "> -v gpt-qa-valid_prepared.jsonl\n",
    "-m \"davinci\" \\\n",
    "--suffix \"gpt-ml-qa-pairs-A\"\n",
    "Upload progress: 100%|███████████████████████| 50.8k/50.8k [00:00<00:00, 22.6Mit/s]\n",
    "Uploaded file from gpt-qa-train_prepared.jsonl: file-5fYU80lXy5FQgfmfVU56k8nb\n",
    "Upload progress: 100%|███████████████████████| 12.2k/12.2k [00:00<00:00, 8.80Mit/s]\n",
    "Uploaded file from gpt-qa-valid_prepared.jsonl: file-99tOIqCaPbNwL90bI1K1yq23\n",
    "Created fine-tune: ft-zHk8gGvZdaBNRVheXq5fZFqt\n",
    "Streaming events until fine-tuning is complete...\n",
    "\n",
    "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
    "[2022-11-20 00:36:48] Created fine-tune: ft-zHk8gGvZdaBNRVheXq5fZFqt\n",
    "[2022-11-20 00:36:52] Fine-tune costs $0.14\n",
    "[2022-11-20 00:36:52] Fine-tune enqueued. Queue number: 0\n",
    "[2022-11-20 00:36:54] Fine-tune started\n",
    "[2022-11-20 00:37:54] Completed epoch 1/4\n",
    "[2022-11-20 00:38:06] Completed epoch 2/4\n",
    "[2022-11-20 00:38:18] Completed epoch 3/4\n",
    "[2022-11-20 00:38:30] Completed epoch 4/4\n",
    "[2022-11-20 00:38:46] Uploaded model: curie:ft-personal-2022-11-20-08-38-46\n",
    "[2022-11-20 00:38:47] Uploaded result file: file-EZy5duulRUswzYdqj6LuW76a\n",
    "[2022-11-20 00:38:47] Fine-tune succeeded\n",
    "\n",
    "Job complete! Status: succeeded 🎉\n",
    "Try out your fine-tuned model:\n",
    "\n",
    "openai api completions.create -m curie:ft-personal-2022-11-20-08-38-46 -p <YOUR_PROMPT>\n",
    "(ml) SF-mbp:gpt-fine-tuning stephen$ -m \"davinci\" \\\n",
    "> --suffix \"gpt-ml-qa-pairs-A\"\n",
    "-bash: -m: command not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying again \n",
    "\n",
    "$ openai api fine_tunes.create \\\n",
    "> -t gpt-qa-train_prepared.jsonl \\\n",
    "> -v gpt-qa-valid_prepared.jsonl \\\n",
    "> -m \"davinci\" \\\n",
    "> --suffix \"gpt-ml-qa-pairs-A\"\n",
    "Found potentially duplicated files with name 'gpt-qa-train_prepared.jsonl', purpose 'fine-tune' and size 50782 bytes\n",
    "file-5fYU80lXy5FQgfmfVU56k8nb\n",
    "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway:\n",
    "Upload progress: 100%|███████████████████████████████| 50.8k/50.8k [00:00<00:00, 21.8Mit/s]\n",
    "Uploaded file from gpt-qa-train_prepared.jsonl: file-XCo6Sza9cVyfIWjPmoatpK3s\n",
    "Found potentially duplicated files with name 'gpt-qa-valid_prepared.jsonl', purpose 'fine-tune' and size 12227 bytes\n",
    "file-99tOIqCaPbNwL90bI1K1yq23\n",
    "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway:\n",
    "Upload progress: 100%|███████████████████████████████| 12.2k/12.2k [00:00<00:00, 6.49Mit/s]\n",
    "Uploaded file from gpt-qa-valid_prepared.jsonl: file-HbOaJkGRRu4hp4C8DQ4PtZ0t\n",
    "Created fine-tune: ft-xNzKl8ORDdASWIALy5WoSLcY\n",
    "Streaming events until fine-tuning is complete...\n",
    "\n",
    "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
    "[2022-11-20 00:45:39] Created fine-tune: ft-xNzKl8ORDdASWIALy5WoSLcY\n",
    "[2022-11-20 00:45:46] Fine-tune costs $1.42\n",
    "[2022-11-20 00:45:47] Fine-tune enqueued. Queue number: 0\n",
    "[2022-11-20 00:45:48] Fine-tune started\n",
    "[2022-11-20 00:47:30] Completed epoch 1/4\n",
    "[2022-11-20 00:47:50] Completed epoch 2/4\n",
    "[2022-11-20 00:48:10] Completed epoch 3/4\n",
    "[2022-11-20 00:48:30] Completed epoch 4/4\n",
    "[2022-11-20 00:49:22] Uploaded model: davinci:ft-sandbox:gpt-ml-qa-pairs-a-2022-11-20-08-49-22\n",
    "[2022-11-20 00:49:23] Uploaded result file: file-rGIPU3ndPAZXVRSBT4xhjDKy\n",
    "[2022-11-20 00:49:23] Fine-tune succeeded\n",
    "\n",
    "Job complete! Status: succeeded 🎉\n",
    "Try out your fine-tuned model:\n",
    "\n",
    "openai api completions.create -m davinci:ft-sandbox:gpt-ml-qa-pairs-a-2022-11-20-08-49-22 -p <YOUR_PROMPT>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Test prompts --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_answer(prompt, ftm_name_, printit=False): \n",
    "\n",
    "    prompt += \"\\n\\n###\\n\\n\"\n",
    "\n",
    "    if printit:\n",
    "        print(f\"creating call from {ftm_name_} with prompt: {prompt}\")\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=ftm_name_,\n",
    "        # engine=\"text-davinci-002\", \n",
    "        prompt=prompt,\n",
    "        temperature=0.9,\n",
    "        max_tokens=500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0.6,\n",
    "        stop=[\"\\n<+++>\\n\"]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "# davinci:ft-sandbox:gpt-ml-qa-pairs-b-2022-11-20-09-29-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Running with a prevoiusly trained model to test for now ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for testing for now \n",
    "\n",
    "# ftm_id = \"ft-xXiSGiL0RrMUpKikJw09T1up\" # this model in particular was returning results with weird formatting, maybe something with the START and END sequences \n",
    "# ftm_name = get_ftmodel_name_from_id(ftm_id)\n",
    "\n",
    "ftm_name = \"davinci:ft-sandbox:gpt-ml-qa-pairs-a-2022-11-20-08-49-22\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run a single test prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating call from davinci:ft-sandbox:gpt-ml-qa-pairs-a-2022-11-20-08-49-22 with prompt: What is the difference between a neural network and a deep learning model?\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      " The terms \"neural network\" and \"deep learning model\" are often used interchangeably to refer to a class of machine learning models that use a layered architecture with nonlinear transformation functions.   In general, a neural network or deep learning model can be defined as a machine learning model that is composed of multiple layers of processing elements (or \"hidden layers\") interposed between the input and output layers. A common characteristic of these models is that they use algorithms that involve backpropagation or other gradient-based optimization techniques to learn the parameters of the model by minimizing a loss function.   In practice, there are some subtle differences between these two terms. For example, the term \"neural network\" may be used to refer to models that use biologically-inspired architectures and functions, such as multiple layers of interconnected neurons. Alternatively, the term \"deep learning model\" may be used to refer to models that use more sophisticated machine learning techniques, such as deep neural networks, to solve complex problem domains.   Overall, the distinction between these two terms is not always clear and may depend on the context and application of the model. In general, the terms \"neural network\" and \"deep learning model\" can be used interchangeably to refer to a class of machine learning models that use a layered architecture with nonlinear transformation functions.   # ‍<+++====+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++<+++>+++ ‍ ‍\n",
      "<+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>+++>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the difference between a neural network and a deep learning model?\" \n",
    "#ftm_name is defined after creating the ftm\n",
    "print(get_gpt_answer(prompt, ftm_name, printit=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run a battery of test prompts on the model from \"test-prompts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0: What is the difference between a CNN and a DNN? \n",
      "\n",
      "Answer 0:  In general, the terms \"CNN\" and \"DNN\" are used to refer to different architectures for a class of machine learning models known as neural networks.    In general, a CNN (or convolutional neural network) is a type of DNN that uses a particular architecture inspired by the visual processing system of mammals (e.g. the retina and visual cortex). This architecture is characterized by an alternating column-wise and row-wise structure, with connections (or \"weights\") between layers that are organized in a \"convolutional\" (or cross-channel) manner. This architecture is well-suited to analyzing images or other data that have a 2D structure, and can be used for tasks such as image classification, object detection, and image processing.    In contrast, a DNN (or deep neural network) is a general term for any neural network architecture that uses a large number of layers or \"hidden units\" in its architecture. In general, the deeper the architecture (i.e. the more layers or hidden units), the more complex the model and the more powerful it is at processing data and solving complex tasks.    Note that there is not a strict distinction between CNNs and DNNs, and many models combine features of both architectures. For example, one common architecture uses a ConvNet (i.e. a CNN) as the feature extractor for a larger, traditional (e.g. fully connected) DNN.     \n",
      "\n",
      "\n",
      "\n",
      "Question 1: How many roads must a man walk down? \n",
      "\n",
      "Answer 1:  In the song \"Crossroads\" by Cream, the singer (W. Bruce Kramer) asks \"How many roads must a man walk down, before you call him a man?\" The answer given by the chorus is \"Yes, 'n' how many seas must a white dove sail, before she sleeps in the sand?\" In other words, there is no set number of roads a man must walk down or seas a white dove must sail before they are considered to be a man or a real (\"true\") dove. This phrase is used to suggest that there is no universal standard for what makes someone a man or a true dove, but rather, these characteristics are determined by individual people or cultures. ‍ ‍  \n",
      "\n",
      "\n",
      "\n",
      "Question 2: What does `DataLoaders` do in the fast.ai library? \n",
      "\n",
      "Answer 2:  In the fast.ai library, `DataLoaders` are objects that are used to load data during machine learning training and testing.   There are a number of different types of DataLoaders in the fast.ai library, each with a different purpose. For example, the `LabelsLoader` is used to load data for supervised learning, where the goal is to predict labels or class values for objects (e.g. images) based on other data (e.g. image features). The `FeaturesLoader` is used to load feature data (e.g. image features) used for unsupervised or semi-supervised learning. There is also a `LabelsAndFeaturesLoader` loader that can be used to load data for both supervised and unsupervised learning.   In general, the different DataLoaders in the fast.ai library can be used as part of a machine learning workflow, where they are used to load and prepare data used during training and testing. Note that you don't need to use all of the DataLoaders in the library, and there are other tools and libraries (e.g. Pandas) that can be used to load data in a similar way. ‍ ‍  \n",
      "\n",
      "\n",
      "\n",
      "Question 3: Please write python code for a list comprehension\n",
      "\n",
      "Answer 3:  Here is some example Python code that uses a list comprehension to create a new list: ``` sorted_names = [name for name in names if name.lower() != name.lower()] ``` Note that the syntax for a list comprehension can vary depending on the programming language. For example, in Python the syntax for a list comprehension typically involves a `for` statement followed by an ``if`` or ``if-else`` statement (or another conditional statement) and a generator expression enclosed in curly braces (such as `{}`).   In Java, the syntax for a list comprehension typically involves a `forEach` statement followed by an `if` or `if-else` statement and a lambda expression enclosed in curly braces. ‍ ‍  \n",
      "\n",
      "\n",
      "\n",
      "Question 4: Please write a bash script to save the output of a terminal command to a json file\n",
      "\n",
      "Answer 4:   In Bash, you can use the ` tee` command to save the output of a terminal command to a file, as well as displaying the output in the terminal. For example, if you want to save the output of a command to a JSON file, you can run a command like this:\n",
      "<+++> # tee SomeCommand.json some_output.txt This will run the ` SomeCommand ` command, saving the output to a JSON file named ` SomeCommand.json `, while also displaying the output in the terminal in a file named ` some_output.txt `. ‍ ‍   Note that the ` tee` command is somewhat different from other tools you may be familiar with, such as ` >` (to redirect output to a file) or ` 2> ` (to redirect error output to a file). The ` tee` command essentially allows you to pipe the output of a command to multiple destinations at once, such as a file and the terminal.   In addition to saving the output of a command to a file, you can also use the ` tee` command to save the output to the terminal. For example, if you want to save the output of a command in the terminal, but also display the output in a new terminal window, you can run a command like this: <+++> # tee -a SomeCommand.json some_output.txt This will run the ` SomeCommand ` command, saving the output to a JSON file named ` SomeCommand.json `, while also displaying the output in a new terminal window. ‍ ‍ \n",
      "<+++>  ‍ In Bash, you can use the ` ` (backtick) character to run a command in the shell's command line, even if the shell is not active (for example, if the shell is inactive in the desktop environment). For example, if you want to run a command in the shell when you are working in an application, you can press the ` ` key to open the shell's command line, and then run the command.   If you are working in a graphical desktop environment, you can use the ` ` key to bring up a \"command window\" that displays the shell's command line. Pressing the ` ` key again will close the command window, and return you to the application you were working in. ‍ ‍ \n",
      "<+++>  Once you have written the Bash script, you can \n",
      "\n",
      "\n",
      "\n",
      "Question 5: What are the steps for stochastic gradient descent? \n",
      "Answer 5:  In general, the steps for performing stochastic gradient descent on a machine learning problem using mini-batch stochastic gradient descent can be described as follows: 1. Compute the learning rate and the batch size used for the mini-batch update of the parameter vector. 2. Randomly initialize the parameter vector (or the model) to a random vector. 3. For each mini-batch in the batch of mini-batches:    A. Compute the gradient of the loss function with respect to the parameter vector at the current batch of mini-batres. B. Update the parameter vector in the direction of the negative gradient, using the learning rate and the batch size. C. Compute the accuracy of the model on a held-out validation set at the end of each mini-batch. D. If the model does not achieve a desired level of performance (e.g. loss or accuracy) on the validation set by the end of the mini-batch, restart the mini-batch with a new initializer for the parameter vector. ‍ ‍  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# save all the question, answer pairs to a csv\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('tests/test-prompts.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    with open('test-outputs/gpt-completions-to-test-prompts.csv', 'w',newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['prompt','completion'])\n",
    "        for i, line in enumerate(lines):\n",
    "            print(f'Question {i}: {line}')\n",
    "            answer = get_gpt_answer(line, ftm_name)\n",
    "            print(f'Answer {i}: {answer} \\n\\n\\n')\n",
    "            writer.writerow([line, answer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Previously trained example --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the openai api with a prompt and get a response\n",
    "\n",
    "ftm_name = \"davinci:ft-sandbox:gpt-ml-qa-pairs-b-2022-11-20-09-29-25\"\n",
    "\n",
    "prompt = \"What is the difference between a neural network and a deep learning model?\" \n",
    "\n",
    "print(get_gpt_answer(prompt, ftm_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Hyperparameter Exploration --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i could generate over several hyperparameters, \n",
    "# create a function that makes the call \n",
    "# saves the output \n",
    "# gets the finetuned model id\n",
    "# and then calls the next function with the finetuned model id as an argument\n",
    "\n",
    "\n",
    "# can i create a df of openai models and then iterate through it?\n",
    "# or do i need to create a function that takes the model id as an argument?\n",
    "# or do i need to create a function that takes the model id as an argument and then calls the next function with the finetuned model id as an argument?\n",
    "\n",
    "# i think i need to create a function that takes the model id as an argument and then calls the next function with the finetuned model id as an argument\n",
    "\n",
    "# function: takes in a dataset (just training set for now for simplicity) \n",
    "# outputs a finetuned model \"return\" \n",
    "\n",
    "# HYPERPARAMETER EXPLORATION \n",
    "\n",
    "EXPLORE_HYPERPARAMS=False\n",
    "\n",
    "# create a set of various hyperparameters to iterate through\n",
    "# create a ftmodel for each one \n",
    "# save the ftmodel id to a list\n",
    "# then iterate through the list and generate responses to the test-prompts for each one\n",
    "# save the responses to a list\n",
    "# then compare the responses to the test answers\n",
    "import time \n",
    "\n",
    "# wrap this in tqdm to show progress\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "# currsuffix = banana_suffix+suffix\n",
    "currsuffix = \"curie:ft-personal-2022-11-20-08-38-46\"\n",
    "\n",
    "def create_ftm_suffix(base,**kwargs):\n",
    "    str = base + \":\" + \"_\".join([f\"{k}-{v}, \" for k,v in kwargs.items()])\n",
    "    return str\n",
    "\n",
    "def already_exists_ftm(suffix):\n",
    "    all_ft_models = openai.FineTune.list()['data']\n",
    "\n",
    "    # this checks if the current name is already in the list of ft models\n",
    "    # any( ) returns True if any element in the list is True\n",
    "    exists = any([True for i in all_ft_models if suffix in i['fine_tuned_model']])\n",
    "    print('ftmodel already exists: ', suffix)\n",
    "    return exists\n",
    " \n",
    "\n",
    "banana_file = \"gpt-qa-train-formatted_prepared.jsonl\"\n",
    "\n",
    "base_suffix=\"qa-train-iterate\"\n",
    "# create a set of various hyperparameters to iterate through\n",
    "# epochs=[1,2,3,4]\n",
    "# learning_rate_multiplier=[0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "epochs=[1,2]\n",
    "learning_rate_multiplier=[0.1, 0.05, 0.01]\n",
    "\n",
    "suffixes=['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "ftmodels=pd.DataFrame(columns=['id', 'epochs', 'learning_rate_multiplier', 'suffix'])\n",
    "\n",
    "ftmodelcompletions = pd.DataFrame(columns=['id', 'ftm_name', 'epochs', 'learning_rate_multiplier', 'suffix', 'prompt','completion'])\n",
    "\n",
    "# create a ftmodel for each one\n",
    "\n",
    "def explore_hyperparams():\n",
    "\n",
    "    for epoch in tqdm(epochs):\n",
    "    # for epoch in epochs:\n",
    "\n",
    "        for lrm in tqdm(learning_rate_multiplier):\n",
    "\n",
    "            #check if the current model already exists. if so, skip it\n",
    "            curr_suffix = create_ftm_suffix(base_suffix,lrm=lrm, epoch=epoch)\n",
    "\n",
    "            # if already_exists_ftm(curr_suffix): continue\n",
    "\n",
    "            # @TODO actually this should say: \n",
    "            # if already exists, then don't create the finetuned model, just get the id\n",
    "            # if doesn't exist then create it, and then get the id \n",
    "\n",
    "            # create a ft model with these parameters \n",
    "            ftm = file_to_finetuned_model(\n",
    "                train_file=banana_file,\n",
    "                suffix=curr_suffix,\n",
    "                n_epochs=epoch,\n",
    "                learning_rate_multiplier=lrm\n",
    "            )\n",
    "\n",
    "            ftm_id = ftm['id']\n",
    "\n",
    "            print(f\"\\n\\n======================= Initiated ftm:  ======================= \\n ftm_id: {ftm_id}. \\n Now we wait for the model to train... \\n\")\n",
    "\n",
    "            # wait for the ft model to finish training\n",
    "            current_ft_model_is_ready = await_ft_ready(ftm_id)\n",
    "            if not current_ft_model_is_ready: \n",
    "                print(f\"finetuned model timed out: {ftm_id}\")\n",
    "                continue\n",
    "\n",
    "            # the model has passed the await test, so it can be called now\n",
    "            # it's also been given a name, so it can be called by ftm_name now\n",
    "            # ftm_name = ftm['fine_tuned_model'] # the name is stored as ftm['fine_tuned_model']\n",
    "\n",
    "            ftm = openai.FineTune.retrieve(ftm_id) # need to retrieve the fresh version with its name! \n",
    "            \n",
    "            ftm_name = ftm['fine_tuned_model']\n",
    "\n",
    "            print (ftm_name, curr_suffix)\n",
    "            #also we could derive the name from the suffix! although they add a timestamp to the end of the name, which we'd want to emulate \n",
    "\n",
    "\n",
    "            print (f\"{ftm_id} finetuned model is now ready for testing. \\nIts name is: {ftm_name}\\n\")\n",
    "\n",
    "            # append this model to the df \n",
    "            ftmodels = ftmodels.append({'id': ftm_id, 'ftm_name': ftm_name, 'epochs': epoch, 'learning_rate_multiplier': lrm, 'suffix': curr_suffix}, ignore_index=True)\n",
    "\n",
    "            # now test every prompt with every ftmodel\n",
    "            with open ('test-prompts.txt') as f:\n",
    "                print(f\"starting test-prompts on finetuned model {ftm_id}\")\n",
    "                prompts = f.readlines()\n",
    "                for prompt in tqdm(prompts):\n",
    "                    # get the completion for this prompt\n",
    "                    completion = get_gpt_answer(prompt, ftm_name, printit=True)\n",
    "                    # append this completion to the df\n",
    "                    ftmodelcompletions = ftmodelcompletions.append(\n",
    "                        {'id': ftm_id, \n",
    "                        'ftm_name': ftm_name,\n",
    "                        'epochs': epoch, \n",
    "                        'learning_rate_multiplier': lrm, \n",
    "                        'suffix': curr_suffix, \n",
    "                        'prompt': prompt, \n",
    "                        'completion': completion}, \n",
    "                        ignore_index=True)\n",
    "                print('\\n\\n\\nCompleted test prompts for ftmodel: ', ftm_id)\n",
    "\n",
    "    ftmodels.to_csv('ftmodels.csv')\n",
    "    ftmodelcompletions.to_csv('ftmodelcompletions.csv')\n",
    "\n",
    "    print(ftmodels.head())\n",
    "    print(ftmodelcompletions.head())\n",
    "\n",
    "\n",
    "if EXPLORE_HYPERPARAMS: \n",
    "    explore_hyperparams()\n",
    "\n",
    "    # SUCCESS! \n",
    "    print(ftmodels.head())\n",
    "    print(ftmodelcompletions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d905c6a59c21f0f46be93fdc832728644d115a3fdfd57971d06d899b53e0576e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
