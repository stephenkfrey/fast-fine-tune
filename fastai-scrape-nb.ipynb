{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a scraper that reads the main body of a forum post and returns the text of the post\n",
    "import re\n",
    "import pandas as pd \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def find_answer_nodes(soup):\n",
    "    \"\"\"\n",
    "    Extract the answer html between the question and the next question\n",
    "\n",
    "    Args:\n",
    "        soup: The soup of the question\n",
    "\n",
    "    Returns:\n",
    "        The formatted html of the answer\n",
    "    \n",
    "    \"\"\"\n",
    "    position = soup\n",
    "    answer_nodes = []\n",
    "    while True:\n",
    "        try:\n",
    "            position = position.find_next_sibling()\n",
    "            if 'start' in position.attrs:\n",
    "                break\n",
    "\n",
    "            answer_nodes.append(position.decode_contents())\n",
    "        except AttributeError:\n",
    "            break\n",
    "\n",
    "    return ''.join(answer_nodes).strip()\n",
    "\n",
    "\n",
    "def extract_qa(item):\n",
    "    \"\"\"\n",
    "    Extract the question and answer by an answer soup\n",
    "\n",
    "    Args: \n",
    "        item: The soup of the qa post \n",
    "\n",
    "    Returns:\n",
    "        A dictionary of the question and answer pairs \n",
    "    \"\"\"\n",
    "    title_tmp = item.select_one('li').decode_contents()\n",
    "    title = title_tmp.replace('<code>', '\\'').replace('</code>', '\\'')\n",
    "\n",
    "    # if 'What is the difference between tensor rank and shape?' in title:\n",
    "    #     print(111)\n",
    "\n",
    "    # Get the answer content\n",
    "    answer_inner_html = find_answer_nodes(item)  # Get original html\n",
    "\n",
    "    # Replace the code blocks\n",
    "    answer_inner_html = re.sub(r'<pre><code( class=\"[\\w-]+\")?>([\\s\\S]*?)</code></pre>', r'```\\2```', answer_inner_html)\n",
    "    answer_inner_html = answer_inner_html.replace('<code>', '\\'').replace('</code>', '\\'')\n",
    "\n",
    "    # Get content\n",
    "    answer_soup = BeautifulSoup(answer_inner_html, \"html.parser\")\n",
    "    answer = answer_soup.text\n",
    "\n",
    "    return {'title': title, 'answer': answer.strip()}\n",
    "\n",
    "\n",
    "def get_qa_post(post_url):\n",
    "    \"\"\"\n",
    "    Reads the main body of a forum post and returns the text of the post.\n",
    "    \n",
    "    Args:\n",
    "        post_url: The URL of the forum post.\n",
    "    \n",
    "    Returns:\n",
    "        The text of the forum post.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the HTML of the forum post\n",
    "    response = requests.get(post_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    items = soup.find_all('ol')\n",
    "\n",
    "    # Get the valid question titles\n",
    "    QAs = []\n",
    "    for i, item in enumerate(items):\n",
    "        # Only get question pairs\n",
    "        if i == 0 or 'start' in item.attrs:\n",
    "            QAs.append(extract_qa(item))\n",
    "\n",
    "    return QAs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the forum post for 4 \n",
    "# url = \"https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253\"\n",
    "# qa_dict = get_qa_post(url)\n",
    "# # print(qa_dict)\n",
    "\n",
    "# pd.DataFrame(qa_dict).to_csv('fastai-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fastai-solutions-forum-pages.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load the csv file \"forum pages\" into a df and then loop through the pages and scrape the questions and answers\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m forum_urls_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mfastai-solutions-forum-pages.csv\u001b[39;49m\u001b[39m'\u001b[39;49m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m forum_urls_df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m     f,\n\u001b[1;32m   1220\u001b[0m     mode,\n\u001b[1;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1227\u001b[0m )\n\u001b[1;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    787\u001b[0m             handle,\n\u001b[1;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fastai-solutions-forum-pages.csv'"
     ]
    }
   ],
   "source": [
    "# load the csv file \"forum pages\" into a df and then loop through the pages and scrape the questions and answers\n",
    "forum_urls_df = pd.read_csv('fastai-solutions-forum-pages.csv') \n",
    "forum_urls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 https://forums.fast.ai/t/fastbook-chapter-1-questionnaire-solutions-wiki/65647\n",
      "https://forums.fast.ai/t/fastbook-chapter-1-questionnaire-solutions-wiki/65647\n",
      "1 https://forums.fast.ai/t/fastbook-chapter-2-questionnaire-solutions-wiki/66392\n",
      "https://forums.fast.ai/t/fastbook-chapter-2-questionnaire-solutions-wiki/66392\n",
      "2 https://forums.fast.ai/t/fastbook-chapter-3-questionnaire-solutions-wiki/68042\n",
      "https://forums.fast.ai/t/fastbook-chapter-3-questionnaire-solutions-wiki/68042\n",
      "3 https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253\n",
      "https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253\n",
      "4 https://forums.fast.ai/t/fastbook-chapter-5-questionnaire-solutions-wiki/69301\n",
      "https://forums.fast.ai/t/fastbook-chapter-5-questionnaire-solutions-wiki/69301\n",
      "5 https://forums.fast.ai/t/fastbook-chapter-6-questionnaire-solutions-wiki/69922\n",
      "https://forums.fast.ai/t/fastbook-chapter-6-questionnaire-solutions-wiki/69922\n",
      "6 nan\n",
      "7 https://forums.fast.ai/t/fastbook-chapter-8-questionnaire-solutions-wiki/69926\n",
      "https://forums.fast.ai/t/fastbook-chapter-8-questionnaire-solutions-wiki/69926\n",
      "8 https://forums.fast.ai/t/fastbook-chapter-9-questionnaire-solutions-wiki/69932\n",
      "https://forums.fast.ai/t/fastbook-chapter-9-questionnaire-solutions-wiki/69932\n",
      "9 https://forums.fast.ai/t/fastbook-chapter-10-questionnaire-solutions-wiki/70506\n",
      "https://forums.fast.ai/t/fastbook-chapter-10-questionnaire-solutions-wiki/70506\n",
      "10 https://forums.fast.ai/t/fastbook-chapter-11-questionnaire-solutions-wiki/88037\n",
      "https://forums.fast.ai/t/fastbook-chapter-11-questionnaire-solutions-wiki/88037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 https://forums.fast.ai/t/fastbook-chapter-12-questionnaire-wiki/70516\n",
      "https://forums.fast.ai/t/fastbook-chapter-12-questionnaire-wiki/70516\n",
      "12 nan\n",
      "13 nan\n",
      "14 nan\n",
      "15 nan\n",
      "16 nan\n",
      "17 nan\n",
      "18 nan\n",
      "19 nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# loop through the pages and scrape the questions and answers\n",
    "qa_dict = []\n",
    "for i, row in forum_urls_df.iterrows():\n",
    "    # print(i, row['URL'])\n",
    "\n",
    "    # if it contains nan or is empty or is not a url, skip\n",
    "    if not pd.isna(row['URL']) and row['URL'].startswith('https'):\n",
    "        # print(row['URL'])\n",
    "        qa_dict.extend(get_qa_post(row['URL']))\n",
    "\n",
    "# save the questions and answers to a csv file\n",
    "qadict = pd.DataFrame(qa_dict)\n",
    "qadict.to_csv('fastai-2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n"
     ]
    }
   ],
   "source": [
    "# print(qadict.head())\n",
    "print(len(qadict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on the book of the same name, what are t...</td>\n",
       "      <td>- A set of processing units\\n- A state of acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What were the two theoretical misunderstanding...</td>\n",
       "      <td>In 1969, Marvin Minsky and Seymour Papert demo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a GPU?</td>\n",
       "      <td>GPU stands for Graphics Processing Unit (also ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Why is it hard to use a traditional computer p...</td>\n",
       "      <td>For us humans, it is easy to identify images i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What term do we normally use in deep learning ...</td>\n",
       "      <td>We instead use the term parameters. In deep le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Based on the book of the same name, what are t...   \n",
       "1  What were the two theoretical misunderstanding...   \n",
       "2                                     What is a GPU?   \n",
       "6  Why is it hard to use a traditional computer p...   \n",
       "7  What term do we normally use in deep learning ...   \n",
       "\n",
       "                                              answer  \n",
       "0  - A set of processing units\\n- A state of acti...  \n",
       "1  In 1969, Marvin Minsky and Seymour Papert demo...  \n",
       "2  GPU stands for Graphics Processing Unit (also ...  \n",
       "6  For us humans, it is easy to identify images i...  \n",
       "7  We instead use the term parameters. In deep le...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the csv file, remove empty lines, and ensure every line has a question and answer, and ensure answers end with a '.' or ' 'ArithmeticError\n",
    "qa_df = pd.read_csv('fastai-qa-cropped.csv')\n",
    "qa_df = qa_df.dropna()\n",
    "# if an answer does not end with a '.' or ' ' or '`', add a '.'\n",
    "qa_df['answer'] = qa_df['answer'].apply(lambda x: x if x.endswith('.') or x.endswith(' ') or x.endswith('`') else x + '.')\n",
    "# save the questions and answers to a csv file\n",
    "qa_df.to_csv('fastai-qa-cleaned.csv', index=False)\n",
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "# create a new df with the questions and answers\n",
    "gpt_df = pd.DataFrame()\n",
    "\n",
    "# add new columns to the df\n",
    "gpt_df['prompt'] = ''\n",
    "gpt_df['completion'] = ''\n",
    "gpt_df['question_id'] = 0\n",
    "\n",
    "question_i = 0\n",
    "\n",
    "# set the initial entry to empty values and question_id to 0\n",
    "gpt_df.loc[question_i, 'prompt'] = ''\n",
    "gpt_df.loc[question_i, 'completion'] = ''\n",
    "gpt_df.loc[question_i, 'question_id'] = 0\n",
    "\n",
    "with open('gpt-chat-log-cleaned.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        # if it starts with \"Q: \", then it is a question\n",
    "        if line.startswith(\"Q: \"):\n",
    "            # add a new entry to the df with the question \n",
    "            gpt_df = gpt_df.append({'prompt': line[3:].strip(), 'completion': '', 'question_id': question_i}, ignore_index=True)\n",
    "            \n",
    "        # otherwise, append it to the answer for the current question\n",
    "            question_i += 1\n",
    "            # print(question_i)\n",
    "        else:\n",
    "            # add the answer to the df at the index question_i \n",
    "            # print(question_i)\n",
    "            # print(gpt_df.iloc[question_i]['answer'])\n",
    "            gpt_df.at[question_i, 'completion'] += line.strip() + ' '\n",
    "\n",
    "\n",
    "gpt_df.head()\n",
    "print(len(gpt_df))\n",
    "\n",
    "# save to csv \n",
    "# gpt_df.to_csv('gpt-qa.csv', index=False)\n",
    "\n",
    "# drop the question_id column\n",
    "gpt_df = gpt_df.drop(columns=['question_id'])\n",
    "# remove rows with empty questions\n",
    "gpt_df = gpt_df[gpt_df['prompt'] != '']\n",
    "gpt_df = gpt_df[gpt_df['completion'] != '']\n",
    "gpt_df = gpt_df.dropna()\n",
    "\n",
    "# replace all triple spaces with '\\n\\n' \n",
    "gpt_df['completion'] = gpt_df['completion'].apply(lambda x: x.replace('  ', ' \\n\\n'))\n",
    "\n",
    "gpt_df['completion'] = gpt_df['completion'].apply(lambda x: x.replace('```', '\\n```\\n'))\n",
    "\n",
    "\n",
    "# save to csv without the indices \n",
    "gpt_df.to_csv('gpt-qa-df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Fine-tuning API \n",
    "\n",
    "ok now let's combine these two and submit to the openai finetuning api \n",
    "-  first i'll upload the GPT chat one \n",
    "-  then i'll upload the fastai one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's format it for finetuning API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  whats the difference between ; and && in terminal   \n",
      "1                            how to auto jump in cli   \n",
      "2  how to list all files even hidden files in ter...   \n",
      "3         how to list files in human readable format   \n",
      "4               how to list details of files with ls   \n",
      "\n",
      "                                          completion  \n",
      "0  In the terminal, `;` and `&&` are used to sepa...  \n",
      "1  To enable auto jumping in the CLI, you can use...  \n",
      "2  To list all files, including hidden files, in ...  \n",
      "3  To list files in human readable format in the ...  \n",
      "4  To list details about files in the terminal us...  \n"
     ]
    }
   ],
   "source": [
    "# load the csv file for more editing\n",
    "gpt_loaded_df = pd.read_csv('gpt-qa-df.csv')\n",
    "print(gpt_loaded_df.head())\n",
    "\n",
    "# - Based on your file extension, your file is formatted as a CSV file\n",
    "# - Your file contains 56 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
    "# - Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
    "# - All completions end with suffix ` `\n",
    "#   WARNING: Some of your completions contain the suffix ` ` more than once. We suggest that you review your completions and add a unique ending\n",
    "# - The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
    "\n",
    "# add a space to the end of each prompt\n",
    "# Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\n\\n###\\n\\n. The separator should not appear elsewhere in any prompt.\n",
    "\n",
    "gpt_loaded_df['prompt'] = gpt_loaded_df['prompt'].apply(lambda x: x + '\\n\\n###\\n\\n')\n",
    "\n",
    "# start each completion with a space \n",
    "# Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.\n",
    "# Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\n, ###, or any other token that does not appear in any completion.\n",
    "\n",
    "gpt_loaded_df['completion'] = gpt_loaded_df['completion'].apply(lambda x: ' ' + x + '\\n<+++>\\n')\n",
    "\n",
    "\n",
    "# save csv \n",
    "\n",
    "# gpt_loaded_df.to_csv('gpt-qa-formatted.csv', index=False)\n",
    "\n",
    "# split the df into two dfs, one with the first 80% of data and one with the last 20% of data\n",
    "gpt_train_df, gpt_valid_df = np.split(gpt_loaded_df.sample(frac=1), [int(.8*len(gpt_loaded_df))])\n",
    "\n",
    "# save the train and test dfs to csv files\n",
    "gpt_train_df.to_csv('gpt-qa-train.csv', index=False)\n",
    "gpt_valid_df.to_csv('gpt-qa-valid.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to format the API call to the fine tuned model \n",
    "\n",
    "### format the prompt from the user before sending it \n",
    "After youâ€™ve fine-tuned a model, remember that your prompt has to end with the indicator string \n",
    "\n",
    "`\\n\\n###\\n\\n` \n",
    "\n",
    "for the model to start generating completions, rather than continuing with the prompt. \n",
    "\n",
    "### param in API call for stop sequence \n",
    "Make sure to include \n",
    "\n",
    "`stop=[\" \\n<+++>\\n\"]` \n",
    "\n",
    "so that the generated texts ends at the expected place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the output CSV file in terminal into a JSONL: \n",
    "\n",
    "openai tools fine_tunes.prepare_data -f <LOCAL_FILE>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"gpt-qa-valid.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !y| openai tools fine_tunes.prepare_data -f {filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai tools fine_tunes.prepare_data -f {filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the fine-tuned model \n",
    "- in CLI\n",
    "- using the files we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (819037427.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [116]\u001b[0;36m\u001b[0m\n\u001b[0;31m    openai api fine_tunes.create \\\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### in terminal: \n",
    "\n",
    "openai api fine_tunes.create \\\n",
    "-t gpt-qa-train_prepared.jsonl \\\n",
    "-v gpt-qa-valid_prepared.jsonl \\\n",
    "-m \"davinci\" \\\n",
    "--suffix \"gpt-ml-qa-pairs-A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2530784851.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [129]\u001b[0;36m\u001b[0m\n\u001b[0;31m    openai api fine_tunes.create \\\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### in terminal: \n",
    "\n",
    "# smaller learning rate because smaller batch \n",
    "\n",
    "openai api fine_tunes.create \\\n",
    "-t gpt-qa-train-formatted_prepared.jsonl \\\n",
    "-v gpt-qa-valid-formatted_prepared.jsonl \\\n",
    "-m \"davinci\" \\\n",
    "-learning_rate_multiplier 0.05 \\\n",
    "--suffix \"gpt-ml-qa-pairs-B\" \\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i could generate over several hyperparameters, \n",
    "# create a function that makes the call \n",
    "# saves the output \n",
    "# gets the finetuned model id\n",
    "# and then calls the next function with the finetuned model id as an argument\n",
    "\n",
    "\n",
    "# can i create a df of openai models and then iterate through it?\n",
    "# or do i need to create a function that takes the model id as an argument?\n",
    "# or do i need to create a function that takes the model id as an argument and then calls the next function with the finetuned model id as an argument?\n",
    "\n",
    "# i think i need to create a function that takes the model id as an argument and then calls the next function with the finetuned model id as an argument\n",
    "\n",
    "# function: takes in a dataset (just training set for now for simplicity) \n",
    "# outputs a finetuned model \"return\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def upload_file_openai(filename): \n",
    "    r = openai.File.create(\n",
    "        file=open(filename, \"rb\"),\n",
    "        purpose='fine-tune'\n",
    "    )\n",
    "    return r\n",
    "\n",
    "\n",
    "def create_finetuned_model(train_file_id, valid_file_id='', model=\"davinci\", learning_rate_multiplier='', n_epochs='', suffix=''):\n",
    "\n",
    "    # create a finetuned model\n",
    "    finetuned_model_response = openai.FineTune.create(\n",
    "        training_file=train_file_id,\n",
    "        # validation_file=valid_file_id,\n",
    "        model=model,\n",
    "        learning_rate_multiplier=learning_rate_multiplier,\n",
    "        n_epochs=n_epochs,\n",
    "        suffix=suffix,\n",
    "    )\n",
    "\n",
    "    return finetuned_model_response\n",
    "\n",
    "\n",
    "_train_file_id = upload_file_openai('gpt-qa-train-formatted_prepared.jsonl')['id']\n",
    "_valid_file_id = upload_file_openai('gpt-qa-valid-formatted_prepared.jsonl')['id']\n",
    "\n",
    "fm = create_finetuned_model(\n",
    "    train_file_id=_train_file_id,\n",
    "    valid_file_id=_valid_file_id,\n",
    "    model=\"davinci\",\n",
    "    learning_rate_multiplier=0.05,\n",
    "    n_epochs=1,\n",
    "    suffix=\"gpt-ml-qa-pairs-C\",\n",
    ")\n",
    "\n",
    "fm_id = fm['id']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTune fine-tune id=ft-CgHrXIX0OF8EsJhWLiOGUQvp at 0x7f91ee552c00> JSON: {\n",
       "  \"created_at\": 1668941450,\n",
       "  \"events\": [\n",
       "    {\n",
       "      \"created_at\": 1668941450,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Created fine-tune: ft-CgHrXIX0OF8EsJhWLiOGUQvp\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    }\n",
       "  ],\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"hyperparams\": {\n",
       "    \"batch_size\": null,\n",
       "    \"learning_rate_multiplier\": 0.1,\n",
       "    \"n_epochs\": 4,\n",
       "    \"prompt_loss_weight\": 0.01\n",
       "  },\n",
       "  \"id\": \"ft-CgHrXIX0OF8EsJhWLiOGUQvp\",\n",
       "  \"model\": \"davinci\",\n",
       "  \"object\": \"fine-tune\",\n",
       "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"pending\",\n",
       "  \"training_files\": [\n",
       "    {\n",
       "      \"bytes\": 50561,\n",
       "      \"created_at\": 1668941450,\n",
       "      \"filename\": \"file\",\n",
       "      \"id\": \"file-XOXTgbtE5qt0GRxEM7LazvGJ\",\n",
       "      \"object\": \"file\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"status\": \"uploaded\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ],\n",
       "  \"updated_at\": 1668941450,\n",
       "  \"validation_files\": []\n",
       "}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simplify all the above \n",
    "\"\"\"\"\n",
    "file_to_finetuned_model\n",
    "\n",
    "Inputs: \n",
    "    filename: string, name of the file to be uploaded to openai\n",
    "    model: string, name of the model to be finetuned\n",
    "    learning_rate_multiplier: float, learning rate multiplier\n",
    "    n_epochs: int, number of epochs\n",
    "    suffix: string, suffix to be added to the finetuned model name\n",
    "\n",
    "Outputs: \n",
    "    finetuned_model_id: string, id of the finetuned model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# takes in .jsonl file, returns a finetuned model response \n",
    "\n",
    "def file_to_finetuned_model(train_file, valid_file='', model=\"davinci\", learning_rate_multiplier=0.1, n_epochs=4, suffix=''):\n",
    "\n",
    "    # upload file to openai\n",
    "    if train_file:\n",
    "        assert(train_file.endswith('.jsonl')), \"train_file must be a .jsonl file\"\n",
    "        _train_file_id = upload_file_openai(train_file)['id']\n",
    "    else:\n",
    "        FileNotFoundError('Must include a train_file')\n",
    "\n",
    "    if valid_file != '':\n",
    "        assert(valid_file.endswith('.jsonl')), \"valid_file must be a .jsonl file\"\n",
    "        _valid_file_id = upload_file_openai(valid_file)['id']\n",
    "    else: \n",
    "        _valid_file_id = ''\n",
    "\n",
    "    # create a finetuned model\n",
    "    finetuned_model_response = create_finetuned_model(\n",
    "        train_file_id=_train_file_id,\n",
    "        # valid_file_id=_valid_file_id,\n",
    "        model=model,\n",
    "        learning_rate_multiplier=learning_rate_multiplier,\n",
    "        n_epochs=n_epochs,\n",
    "        suffix=suffix,\n",
    "    )\n",
    "\n",
    "    return finetuned_model_response\n",
    "\n",
    "\n",
    "banana_file = \"gpt-qa-train-formatted_prepared.jsonl\"\n",
    "banana_suffix=\"qa-train-iterate\"\n",
    "\n",
    "fm = file_to_finetuned_model(\n",
    "    train_file=banana_file,\n",
    "    suffix=banana_suffix\n",
    ")\n",
    "\n",
    "fm_id = fm['id']\n",
    "fm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ft-CgHrXIX0OF8EsJhWLiOGUQvp'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuned model id: ft-eI4AVMWnV9NZDZTi5tOWJ8vl\n",
      "waiting for ft-eI4AVMWnV9NZDZTi5tOWJ8vl to train.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [02:32<?, ?it/s]\n",
      "  0%|          | 0/5 [02:32<?, ?it/s]\n",
      "  0%|          | 0/4 [02:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft-eI4AVMWnV9NZDZTi5tOWJ8vl finetuned model is now ready for testing\n",
      "starting test-prompts on finetuned model {fm_id}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "That model does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m prompts \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     completion \u001b[39m=\u001b[39m get_gpt_response(prompt, fm_id)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     ftmodelcompletions \u001b[39m=\u001b[39m ftmodelcompletions\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m         {\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: fm_id, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m: epoch, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcompletion\u001b[39m\u001b[39m'\u001b[39m: completion}, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mCompleted test prompts for ftmodel: \u001b[39m\u001b[39m'\u001b[39m, fm_id)\n",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb Cell 25\u001b[0m in \u001b[0;36mget_gpt_response\u001b[0;34m(prompt, finetuned_model_id)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_gpt_response\u001b[39m(prompt, finetuned_model_id): \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     prompt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m###\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         model\u001b[39m=\u001b[39;49mfinetuned_model_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39m# engine=\"text-davinci-002\", \u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         top_p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.6\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         stop\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m<+++>\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X66sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:115\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    107\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39mAPIRequestor(\n\u001b[1;32m    108\u001b[0m     api_key,\n\u001b[1;32m    109\u001b[0m     api_base\u001b[39m=\u001b[39mapi_base,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m     organization\u001b[39m=\u001b[39morganization,\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[0;32m--> 115\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    116\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    117\u001b[0m     url,\n\u001b[1;32m    118\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    119\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    120\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    121\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    122\u001b[0m     request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    126\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/openai/api_requestor.py:181\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    161\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    162\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    170\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    171\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    172\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    173\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    180\u001b[0m     )\n\u001b[0;32m--> 181\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/openai/api_requestor.py:396\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    389\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    390\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    392\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    393\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 396\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    397\u001b[0m             result\u001b[39m.\u001b[39;49mcontent, result\u001b[39m.\u001b[39;49mstatus_code, result\u001b[39m.\u001b[39;49mheaders, stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    398\u001b[0m         ),\n\u001b[1;32m    399\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    400\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/openai/api_requestor.py:429\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    427\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    428\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    430\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: That model does not exist"
     ]
    }
   ],
   "source": [
    "# create a set of various hyperparameters to iterate through\n",
    "# create a ftmodel for each one \n",
    "# save the ftmodel id to a list\n",
    "# then iterate through the list and generate responses to the test-prompts for each one\n",
    "# save the responses to a list\n",
    "# then compare the responses to the test answers\n",
    "import time \n",
    "\n",
    "\n",
    "banana_file = \"gpt-qa-train-formatted_prepared.jsonl\"\n",
    "banana_suffix=\"qa-train-iterate\"\n",
    "# create a set of various hyperparameters to iterate through\n",
    "epochs=[1,2,3,4]\n",
    "learning_rate_multiplier=[0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "suffixes=['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "ftmodels=pd.DataFrame(columns=['id', 'epochs', 'learning_rate_multiplier', 'suffix'])\n",
    "\n",
    "ftmodelcompletions = pd.DataFrame(columns=['id', 'epochs', 'learning_rate_multiplier', 'suffix', 'prompt','completion'])\n",
    "\n",
    "# create a ftmodel for each one\n",
    "\n",
    "# wrap this in tqdm to show progress\n",
    "import tqdm \n",
    "\n",
    "for epoch in tqdm.tqdm(epochs):\n",
    "# for epoch in epochs:\n",
    "\n",
    "    for lrm in tqdm.tqdm(learning_rate_multiplier):\n",
    "        for suffix in tqdm.tqdm(suffixes):\n",
    "\n",
    "            \n",
    "\n",
    "            # create a ft model with these parameters \n",
    "            fm = file_to_finetuned_model(\n",
    "                train_file=banana_file,\n",
    "                suffix=banana_suffix+'__'+suffix,\n",
    "                n_epochs=epoch,\n",
    "                learning_rate_multiplier=lrm\n",
    "            )\n",
    "            fm_id = fm['id']\n",
    "            print(f\"finetuned model id: {fm_id}\")\n",
    "            ftmodels = ftmodels.append({'id': fm_id, 'epochs': epoch, 'learning_rate_multiplier': lrm, 'suffix': suffix}, ignore_index=True)\n",
    "\n",
    "            # wait for the ft model to finish training\n",
    "            current_ft_model_is_ready = await_ft_ready(fm_id)\n",
    "            if not current_ft_model_is_ready: \n",
    "                print(f\"finetuned model timed out: {fm_id}\")\n",
    "                continue\n",
    "            \n",
    "            print (f\"{fm_id} finetuned model is now ready for testing\")\n",
    "\n",
    "            # now test every prompt with every ftmodel\n",
    "            with open ('test-prompts.txt') as f:\n",
    "                print(\"starting test-prompts on finetuned model {fm_id}\")\n",
    "                prompts = f.readlines()\n",
    "                for prompt in prompts:\n",
    "                    completion = get_gpt_response(prompt, fm_id)\n",
    "                    ftmodelcompletions = ftmodelcompletions.append(\n",
    "                        {'id': fm_id, \n",
    "                        'epochs': epoch, \n",
    "                        'learning_rate_multiplier': lrm, \n",
    "                        'suffix': suffix, \n",
    "                        'prompt': prompt, \n",
    "                        'completion': completion}, \n",
    "                        ignore_index=True)\n",
    "                print('\\n\\n\\nCompleted test prompts for ftmodel: ', fm_id)\n",
    "\n",
    "\n",
    "ftmodels.to_csv('ftmodels.csv')\n",
    "ftmodelcompletions.to_csv('ftmodelcompletions.csv')\n",
    "\n",
    "print(ftmodels.head())\n",
    "print(ftmodelcompletions.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ft_models = openai.FineTune.list()['data']\n",
    "\n",
    "currsuffix = banana_suffix+suffix\n",
    "currsuffix = \"curie:ft-personal-2022-11-20-08-38-46\"\n",
    "\n",
    "# this checks if the current name is already in the list of ft models\n",
    "# any( ) returns True if any element in the list is True\n",
    "model_already_exists = any([True for i in all_ft_models if currsuffix in i['fine_tuned_model']])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succeeded\n",
      "waiting for ft-eI4AVMWnV9NZDZTi5tOWJ8vl to train.\n",
      "ft-eI4AVMWnV9NZDZTi5tOWJ8vl is ready!\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# the finetuned model needs to finish training before we can use it\n",
    "# so let's track its status and only make calls when its' ready \n",
    "# we can use the finetuned model id to check its status\n",
    "\n",
    "def get_finetuned_model_status(finetuned_model_id):\n",
    "    response = openai.FineTune.retrieve(id=finetuned_model_id)\n",
    "    return response['status']\n",
    "\n",
    "# check the status of the finetuned model\n",
    "print(get_finetuned_model_status(fm_id))\n",
    "\n",
    "# now let's create a function that will wait until the finetuned model is ready\n",
    "# and then return the response\n",
    "\n",
    "def await_ft_ready(finetuned_model_id):\n",
    "\n",
    "    patience = 40 # number of times to check the status of the finetuned model\n",
    "    counter=0\n",
    "\n",
    "    print(f\"waiting for {finetuned_model_id} to train.\")\n",
    "\n",
    "\n",
    "    while True:\n",
    "        status = get_finetuned_model_status(finetuned_model_id)\n",
    "        if status == 'succeeded':\n",
    "            print(f\"{finetuned_model_id} is ready!\")\n",
    "            return True\n",
    "        else:\n",
    "            # print(f\"{finetuned_model_id} finetuned model is still training. {patience*10} seconds so far.\")\n",
    "            counter +=1\n",
    "            if counter % 10 == 0: \n",
    "                print(f\"{finetuned_model_id} has trained for {counter*10} seconds so far.\")\n",
    "            if counter > patience: \n",
    "                # print that the current timed out\n",
    "                print(f\"{finetuned_model_id} finetuned model has taken longer than {patience*10} to train\")\n",
    "                return False\n",
    "            time.sleep(10) \n",
    "\n",
    "# now let's test it out\n",
    "print(await_ft_ready(fm_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files.\n",
    "\n",
    "The _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. \n",
    "\n",
    "openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal outputs \n",
    "\n",
    "\n",
    "(ml) SF-mbp:gpt-fine-tuning stephen$ openai api fine_tunes.create \\\n",
    "> -t gpt-qa-train_prepared.jsonl \\\n",
    "> -v gpt-qa-valid_prepared.jsonl\n",
    "-m \"davinci\" \\\n",
    "--suffix \"gpt-ml-qa-pairs-A\"\n",
    "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50.8k/50.8k [00:00<00:00, 22.6Mit/s]\n",
    "Uploaded file from gpt-qa-train_prepared.jsonl: file-5fYU80lXy5FQgfmfVU56k8nb\n",
    "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.2k/12.2k [00:00<00:00, 8.80Mit/s]\n",
    "Uploaded file from gpt-qa-valid_prepared.jsonl: file-99tOIqCaPbNwL90bI1K1yq23\n",
    "Created fine-tune: ft-zHk8gGvZdaBNRVheXq5fZFqt\n",
    "Streaming events until fine-tuning is complete...\n",
    "\n",
    "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
    "[2022-11-20 00:36:48] Created fine-tune: ft-zHk8gGvZdaBNRVheXq5fZFqt\n",
    "[2022-11-20 00:36:52] Fine-tune costs $0.14\n",
    "[2022-11-20 00:36:52] Fine-tune enqueued. Queue number: 0\n",
    "[2022-11-20 00:36:54] Fine-tune started\n",
    "[2022-11-20 00:37:54] Completed epoch 1/4\n",
    "[2022-11-20 00:38:06] Completed epoch 2/4\n",
    "[2022-11-20 00:38:18] Completed epoch 3/4\n",
    "[2022-11-20 00:38:30] Completed epoch 4/4\n",
    "[2022-11-20 00:38:46] Uploaded model: curie:ft-personal-2022-11-20-08-38-46\n",
    "[2022-11-20 00:38:47] Uploaded result file: file-EZy5duulRUswzYdqj6LuW76a\n",
    "[2022-11-20 00:38:47] Fine-tune succeeded\n",
    "\n",
    "Job complete! Status: succeeded ðŸŽ‰\n",
    "Try out your fine-tuned model:\n",
    "\n",
    "openai api completions.create -m curie:ft-personal-2022-11-20-08-38-46 -p <YOUR_PROMPT>\n",
    "(ml) SF-mbp:gpt-fine-tuning stephen$ -m \"davinci\" \\\n",
    "> --suffix \"gpt-ml-qa-pairs-A\"\n",
    "-bash: -m: command not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying again \n",
    "\n",
    "$ openai api fine_tunes.create \\\n",
    "> -t gpt-qa-train_prepared.jsonl \\\n",
    "> -v gpt-qa-valid_prepared.jsonl \\\n",
    "> -m \"davinci\" \\\n",
    "> --suffix \"gpt-ml-qa-pairs-A\"\n",
    "Found potentially duplicated files with name 'gpt-qa-train_prepared.jsonl', purpose 'fine-tune' and size 50782 bytes\n",
    "file-5fYU80lXy5FQgfmfVU56k8nb\n",
    "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway:\n",
    "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50.8k/50.8k [00:00<00:00, 21.8Mit/s]\n",
    "Uploaded file from gpt-qa-train_prepared.jsonl: file-XCo6Sza9cVyfIWjPmoatpK3s\n",
    "Found potentially duplicated files with name 'gpt-qa-valid_prepared.jsonl', purpose 'fine-tune' and size 12227 bytes\n",
    "file-99tOIqCaPbNwL90bI1K1yq23\n",
    "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway:\n",
    "Upload progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.2k/12.2k [00:00<00:00, 6.49Mit/s]\n",
    "Uploaded file from gpt-qa-valid_prepared.jsonl: file-HbOaJkGRRu4hp4C8DQ4PtZ0t\n",
    "Created fine-tune: ft-xNzKl8ORDdASWIALy5WoSLcY\n",
    "Streaming events until fine-tuning is complete...\n",
    "\n",
    "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
    "[2022-11-20 00:45:39] Created fine-tune: ft-xNzKl8ORDdASWIALy5WoSLcY\n",
    "[2022-11-20 00:45:46] Fine-tune costs $1.42\n",
    "[2022-11-20 00:45:47] Fine-tune enqueued. Queue number: 0\n",
    "[2022-11-20 00:45:48] Fine-tune started\n",
    "[2022-11-20 00:47:30] Completed epoch 1/4\n",
    "[2022-11-20 00:47:50] Completed epoch 2/4\n",
    "[2022-11-20 00:48:10] Completed epoch 3/4\n",
    "[2022-11-20 00:48:30] Completed epoch 4/4\n",
    "[2022-11-20 00:49:22] Uploaded model: davinci:ft-sandbox:gpt-ml-qa-pairs-a-2022-11-20-08-49-22\n",
    "[2022-11-20 00:49:23] Uploaded result file: file-rGIPU3ndPAZXVRSBT4xhjDKy\n",
    "[2022-11-20 00:49:23] Fine-tune succeeded\n",
    "\n",
    "Job complete! Status: succeeded ðŸŽ‰\n",
    "Try out your fine-tuned model:\n",
    "\n",
    "openai api completions.create -m davinci:ft-sandbox:gpt-ml-qa-pairs-a-2022-11-20-08-49-22 -p <YOUR_PROMPT>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, test the fine-tuned model with prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# set the api key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In general, the term \"neural network\" can refer to any algorithm that is inspired by the architecture of a biological neural network (i.e. a system of neurons connected by synapses). In the context of machine learning, the term \"neural network\" is often used to refer to a class of machine learning models, such as feed-forward neural networks or recurrent neural networks, that are built using a Representation Learning technique known as backpropagation.\n",
      "\n",
      " On the other hand, the term \"deep learning\" refers to a set of techniques for training neural networks (or other machine learning models) using large amounts of training data and computational resources, such as GPUs or distributed servers. Deep learning is a subset of machine learning and artificial intelligence (AI) that has gained popularity in recent years due to its success in accomplishing challenging tasks, such as image recognition and speech recognition. â€ â€\n"
     ]
    }
   ],
   "source": [
    "# davinci:ft-sandbox:gpt-ml-qa-pairs-b-2022-11-20-09-29-25\n",
    "\n",
    "# call the openai api with a prompt and get a response\n",
    "\n",
    "finetuned_model_id = \"davinci:ft-sandbox:gpt-ml-qa-pairs-b-2022-11-20-09-29-25\"\n",
    "\n",
    "prompt = \"What is the difference between a neural network and a deep learning model?\" \n",
    "\n",
    "def get_gpt_answer(prompt, finetuned_model_id): \n",
    "\n",
    "    prompt += \"\\n\\n###\\n\\n\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=finetuned_model_id,\n",
    "        # engine=\"text-davinci-002\", \n",
    "        prompt=prompt,\n",
    "        temperature=0.9,\n",
    "        max_tokens=500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0.6,\n",
    "        stop=[\" \\n<+++>\\n\"]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "\n",
    "print(get_gpt_answer(prompt, finetuned_model_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_gpt_answer() missing 1 required positional argument: 'finetuned_model_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(lines): \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     answer \u001b[39m=\u001b[39m get_gpt_answer(line)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mQuestion \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mline\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAnswer \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00manswer\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_gpt_answer() missing 1 required positional argument: 'finetuned_model_id'"
     ]
    }
   ],
   "source": [
    "# Test prompts \n",
    "\n",
    "# for each line in the file, call the openai api and get a response\n",
    "with open('test-prompts.txt', 'r') as f: \n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines): \n",
    "        answer = get_gpt_answer(line)\n",
    "        print(f'Question {i}: {line}')\n",
    "        print(f'Answer {i}: {answer} \\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        print(\"{} = {}\".format(key, value))\n",
    "    myb = kwargs[\"b\"] if \"b\" in kwargs\n",
    "    print(myb)\n",
    "        \n",
    "\n",
    "bar(a=1, b=2, c=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d905c6a59c21f0f46be93fdc832728644d115a3fdfd57971d06d899b53e0576e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
