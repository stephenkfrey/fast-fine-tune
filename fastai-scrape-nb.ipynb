{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a scraper that reads the main body of a forum post and returns the text of the post\n",
    "import re\n",
    "import pandas as pd \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def find_answer_nodes(soup):\n",
    "    \"\"\"\n",
    "    Extract the answer html between the question and the next question\n",
    "\n",
    "    Args:\n",
    "        soup: The soup of the question\n",
    "\n",
    "    Returns:\n",
    "        The formatted html of the answer\n",
    "    \n",
    "    \"\"\"\n",
    "    position = soup\n",
    "    answer_nodes = []\n",
    "    while True:\n",
    "        try:\n",
    "            position = position.find_next_sibling()\n",
    "            if 'start' in position.attrs:\n",
    "                break\n",
    "\n",
    "            answer_nodes.append(position.decode_contents())\n",
    "        except AttributeError:\n",
    "            break\n",
    "\n",
    "    return ''.join(answer_nodes).strip()\n",
    "\n",
    "\n",
    "def extract_qa(item):\n",
    "    \"\"\"\n",
    "    Extract the question and answer by an answer soup\n",
    "\n",
    "    Args: \n",
    "        item: The soup of the qa post \n",
    "\n",
    "    Returns:\n",
    "        A dictionary of the question and answer pairs \n",
    "    \"\"\"\n",
    "    title_tmp = item.select_one('li').decode_contents()\n",
    "    title = title_tmp.replace('<code>', '\\'').replace('</code>', '\\'')\n",
    "\n",
    "    # if 'What is the difference between tensor rank and shape?' in title:\n",
    "    #     print(111)\n",
    "\n",
    "    # Get the answer content\n",
    "    answer_inner_html = find_answer_nodes(item)  # Get original html\n",
    "\n",
    "    # Replace the code blocks\n",
    "    answer_inner_html = re.sub(r'<pre><code( class=\"[\\w-]+\")?>([\\s\\S]*?)</code></pre>', r'```\\2```', answer_inner_html)\n",
    "    answer_inner_html = answer_inner_html.replace('<code>', '\\'').replace('</code>', '\\'')\n",
    "\n",
    "    # Get content\n",
    "    answer_soup = BeautifulSoup(answer_inner_html, \"html.parser\")\n",
    "    answer = answer_soup.text\n",
    "\n",
    "    return {'title': title, 'answer': answer.strip()}\n",
    "\n",
    "\n",
    "def get_qa_post(post_url):\n",
    "    \"\"\"\n",
    "    Reads the main body of a forum post and returns the text of the post.\n",
    "    \n",
    "    Args:\n",
    "        post_url: The URL of the forum post.\n",
    "    \n",
    "    Returns:\n",
    "        The text of the forum post.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the HTML of the forum post\n",
    "    response = requests.get(post_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    items = soup.find_all('ol')\n",
    "\n",
    "    # Get the valid question titles\n",
    "    QAs = []\n",
    "    for i, item in enumerate(items):\n",
    "        # Only get question pairs\n",
    "        if i == 0 or 'start' in item.attrs:\n",
    "            QAs.append(extract_qa(item))\n",
    "\n",
    "    return QAs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the forum post for 4 \n",
    "# url = \"https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253\"\n",
    "# qa_dict = get_qa_post(url)\n",
    "# # print(qa_dict)\n",
    "\n",
    "# pd.DataFrame(qa_dict).to_csv('fastai-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fastai-solutions-forum-pages.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load the csv file \"forum pages\" into a df and then loop through the pages and scrape the questions and answers\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m forum_urls_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mfastai-solutions-forum-pages.csv\u001b[39;49m\u001b[39m'\u001b[39;49m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m forum_urls_df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m     f,\n\u001b[1;32m   1220\u001b[0m     mode,\n\u001b[1;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1227\u001b[0m )\n\u001b[1;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    787\u001b[0m             handle,\n\u001b[1;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fastai-solutions-forum-pages.csv'"
     ]
    }
   ],
   "source": [
    "# load the csv file \"forum pages\" into a df and then loop through the pages and scrape the questions and answers\n",
    "forum_urls_df = pd.read_csv('fastai-solutions-forum-pages.csv') \n",
    "forum_urls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 https://forums.fast.ai/t/fastbook-chapter-1-questionnaire-solutions-wiki/65647\n",
      "https://forums.fast.ai/t/fastbook-chapter-1-questionnaire-solutions-wiki/65647\n",
      "1 https://forums.fast.ai/t/fastbook-chapter-2-questionnaire-solutions-wiki/66392\n",
      "https://forums.fast.ai/t/fastbook-chapter-2-questionnaire-solutions-wiki/66392\n",
      "2 https://forums.fast.ai/t/fastbook-chapter-3-questionnaire-solutions-wiki/68042\n",
      "https://forums.fast.ai/t/fastbook-chapter-3-questionnaire-solutions-wiki/68042\n",
      "3 https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253\n",
      "https://forums.fast.ai/t/fastbook-chapter-4-questionnaire-solutions-wiki/67253\n",
      "4 https://forums.fast.ai/t/fastbook-chapter-5-questionnaire-solutions-wiki/69301\n",
      "https://forums.fast.ai/t/fastbook-chapter-5-questionnaire-solutions-wiki/69301\n",
      "5 https://forums.fast.ai/t/fastbook-chapter-6-questionnaire-solutions-wiki/69922\n",
      "https://forums.fast.ai/t/fastbook-chapter-6-questionnaire-solutions-wiki/69922\n",
      "6 nan\n",
      "7 https://forums.fast.ai/t/fastbook-chapter-8-questionnaire-solutions-wiki/69926\n",
      "https://forums.fast.ai/t/fastbook-chapter-8-questionnaire-solutions-wiki/69926\n",
      "8 https://forums.fast.ai/t/fastbook-chapter-9-questionnaire-solutions-wiki/69932\n",
      "https://forums.fast.ai/t/fastbook-chapter-9-questionnaire-solutions-wiki/69932\n",
      "9 https://forums.fast.ai/t/fastbook-chapter-10-questionnaire-solutions-wiki/70506\n",
      "https://forums.fast.ai/t/fastbook-chapter-10-questionnaire-solutions-wiki/70506\n",
      "10 https://forums.fast.ai/t/fastbook-chapter-11-questionnaire-solutions-wiki/88037\n",
      "https://forums.fast.ai/t/fastbook-chapter-11-questionnaire-solutions-wiki/88037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 https://forums.fast.ai/t/fastbook-chapter-12-questionnaire-wiki/70516\n",
      "https://forums.fast.ai/t/fastbook-chapter-12-questionnaire-wiki/70516\n",
      "12 nan\n",
      "13 nan\n",
      "14 nan\n",
      "15 nan\n",
      "16 nan\n",
      "17 nan\n",
      "18 nan\n",
      "19 nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# loop through the pages and scrape the questions and answers\n",
    "qa_dict = []\n",
    "for i, row in forum_urls_df.iterrows():\n",
    "    # print(i, row['URL'])\n",
    "\n",
    "    # if it contains nan or is empty or is not a url, skip\n",
    "    if not pd.isna(row['URL']) and row['URL'].startswith('https'):\n",
    "        # print(row['URL'])\n",
    "        qa_dict.extend(get_qa_post(row['URL']))\n",
    "\n",
    "# save the questions and answers to a csv file\n",
    "qadict = pd.DataFrame(qa_dict)\n",
    "qadict.to_csv('fastai-2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n"
     ]
    }
   ],
   "source": [
    "# print(qadict.head())\n",
    "print(len(qadict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on the book of the same name, what are t...</td>\n",
       "      <td>- A set of processing units\\n- A state of acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What were the two theoretical misunderstanding...</td>\n",
       "      <td>In 1969, Marvin Minsky and Seymour Papert demo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a GPU?</td>\n",
       "      <td>GPU stands for Graphics Processing Unit (also ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Why is it hard to use a traditional computer p...</td>\n",
       "      <td>For us humans, it is easy to identify images i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What term do we normally use in deep learning ...</td>\n",
       "      <td>We instead use the term parameters. In deep le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Based on the book of the same name, what are t...   \n",
       "1  What were the two theoretical misunderstanding...   \n",
       "2                                     What is a GPU?   \n",
       "6  Why is it hard to use a traditional computer p...   \n",
       "7  What term do we normally use in deep learning ...   \n",
       "\n",
       "                                              answer  \n",
       "0  - A set of processing units\\n- A state of acti...  \n",
       "1  In 1969, Marvin Minsky and Seymour Papert demo...  \n",
       "2  GPU stands for Graphics Processing Unit (also ...  \n",
       "6  For us humans, it is easy to identify images i...  \n",
       "7  We instead use the term parameters. In deep le...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the csv file, remove empty lines, and ensure every line has a question and answer, and ensure answers end with a '.' or ' 'ArithmeticError\n",
    "qa_df = pd.read_csv('fastai-qa-cropped.csv')\n",
    "qa_df = qa_df.dropna()\n",
    "# if an answer does not end with a '.' or ' ' or '`', add a '.'\n",
    "qa_df['answer'] = qa_df['answer'].apply(lambda x: x if x.endswith('.') or x.endswith(' ') or x.endswith('`') else x + '.')\n",
    "# save the questions and answers to a csv file\n",
    "qa_df.to_csv('fastai-qa-cleaned.csv', index=False)\n",
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "# create a new df with the questions and answers\n",
    "gpt_df = pd.DataFrame()\n",
    "\n",
    "# add new columns to the df\n",
    "gpt_df['prompt'] = ''\n",
    "gpt_df['completion'] = ''\n",
    "gpt_df['question_id'] = 0\n",
    "\n",
    "question_i = 0\n",
    "\n",
    "# set the initial entry to empty values and question_id to 0\n",
    "gpt_df.loc[question_i, 'prompt'] = ''\n",
    "gpt_df.loc[question_i, 'completion'] = ''\n",
    "gpt_df.loc[question_i, 'question_id'] = 0\n",
    "\n",
    "with open('gpt-chat-log-cleaned.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        # if it starts with \"Q: \", then it is a question\n",
    "        if line.startswith(\"Q: \"):\n",
    "            # add a new entry to the df with the question \n",
    "            gpt_df = gpt_df.append({'prompt': line[3:].strip(), 'completion': '', 'question_id': question_i}, ignore_index=True)\n",
    "            \n",
    "        # otherwise, append it to the answer for the current question\n",
    "            question_i += 1\n",
    "            # print(question_i)\n",
    "        else:\n",
    "            # add the answer to the df at the index question_i \n",
    "            # print(question_i)\n",
    "            # print(gpt_df.iloc[question_i]['answer'])\n",
    "            gpt_df.at[question_i, 'completion'] += line.strip() + ' '\n",
    "\n",
    "\n",
    "gpt_df.head()\n",
    "print(len(gpt_df))\n",
    "\n",
    "# save to csv \n",
    "# gpt_df.to_csv('gpt-qa.csv', index=False)\n",
    "\n",
    "# drop the question_id column\n",
    "gpt_df = gpt_df.drop(columns=['question_id'])\n",
    "# remove rows with empty questions\n",
    "gpt_df = gpt_df[gpt_df['prompt'] != '']\n",
    "gpt_df = gpt_df[gpt_df['completion'] != '']\n",
    "gpt_df = gpt_df.dropna()\n",
    "\n",
    "# replace all triple spaces with '\\n\\n' \n",
    "gpt_df['completion'] = gpt_df['completion'].apply(lambda x: x.replace('  ', ' \\n\\n'))\n",
    "\n",
    "gpt_df['completion'] = gpt_df['completion'].apply(lambda x: x.replace('```', '\\n```\\n'))\n",
    "\n",
    "\n",
    "# save to csv without the indices \n",
    "gpt_df.to_csv('gpt-qa-df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Fine-tuning API \n",
    "\n",
    "ok now let's combine these two and submit to the openai finetuning api \n",
    "-  first i'll upload the GPT chat one \n",
    "-  then i'll upload the fastai one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's format it for finetuning API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  whats the difference between ; and && in terminal   \n",
      "1                            how to auto jump in cli   \n",
      "2  how to list all files even hidden files in ter...   \n",
      "3         how to list files in human readable format   \n",
      "4               how to list details of files with ls   \n",
      "\n",
      "                                          completion  \n",
      "0  In the terminal, `;` and `&&` are used to sepa...  \n",
      "1  To enable auto jumping in the CLI, you can use...  \n",
      "2  To list all files, including hidden files, in ...  \n",
      "3  To list files in human readable format in the ...  \n",
      "4  To list details about files in the terminal us...  \n"
     ]
    }
   ],
   "source": [
    "# load the csv file for more editing\n",
    "gpt_loaded_df = pd.read_csv('gpt-qa-df.csv')\n",
    "print(gpt_loaded_df.head())\n",
    "\n",
    "# - Based on your file extension, your file is formatted as a CSV file\n",
    "# - Your file contains 56 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
    "# - Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
    "# - All completions end with suffix ` `\n",
    "#   WARNING: Some of your completions contain the suffix ` ` more than once. We suggest that you review your completions and add a unique ending\n",
    "# - The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
    "\n",
    "# add a space to the end of each prompt\n",
    "# Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\n\\n###\\n\\n. The separator should not appear elsewhere in any prompt.\n",
    "\n",
    "gpt_loaded_df['prompt'] = gpt_loaded_df['prompt'].apply(lambda x: x + '\\n\\n###\\n\\n')\n",
    "\n",
    "# start each completion with a space \n",
    "# Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.\n",
    "# Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\n, ###, or any other token that does not appear in any completion.\n",
    "\n",
    "gpt_loaded_df['completion'] = gpt_loaded_df['completion'].apply(lambda x: ' ' + x + '\\n<+++>\\n')\n",
    "\n",
    "\n",
    "# save csv \n",
    "\n",
    "# gpt_loaded_df.to_csv('gpt-qa-formatted.csv', index=False)\n",
    "\n",
    "# split the df into two dfs, one with the first 80% of data and one with the last 20% of data\n",
    "gpt_train_df, gpt_valid_df = np.split(gpt_loaded_df.sample(frac=1), [int(.8*len(gpt_loaded_df))])\n",
    "\n",
    "# save the train and test dfs to csv files\n",
    "gpt_train_df.to_csv('gpt-qa-train.csv', index=False)\n",
    "gpt_valid_df.to_csv('gpt-qa-valid.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to format the API call to the fine tuned model \n",
    "\n",
    "### format the prompt from the user before sending it \n",
    "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string \n",
    "\n",
    "`\\n\\n###\\n\\n` \n",
    "\n",
    "for the model to start generating completions, rather than continuing with the prompt. \n",
    "\n",
    "### param in API call for stop sequence \n",
    "Make sure to include \n",
    "\n",
    "`stop=[\" \\n<+++>\\n\"]` \n",
    "\n",
    "so that the generated texts ends at the expected place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the output CSV file in terminal into a JSONL: \n",
    "\n",
    "openai tools fine_tunes.prepare_data -f <LOCAL_FILE>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"gpt-qa-valid.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !y| openai tools fine_tunes.prepare_data -f {filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai tools fine_tunes.prepare_data -f {filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the fine-tuned model \n",
    "- in CLI\n",
    "- using the files we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (819037427.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [116]\u001b[0;36m\u001b[0m\n\u001b[0;31m    openai api fine_tunes.create \\\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### in terminal: \n",
    "\n",
    "openai api fine_tunes.create \\\n",
    "-t gpt-qa-train_prepared.jsonl \\\n",
    "-v gpt-qa-valid_prepared.jsonl \\\n",
    "-m \"davinci\" \\\n",
    "--suffix \"gpt-ml-qa-pairs-A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2530784851.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [129]\u001b[0;36m\u001b[0m\n\u001b[0;31m    openai api fine_tunes.create \\\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### in terminal: \n",
    "\n",
    "# smaller learning rate because smaller batch \n",
    "\n",
    "openai api fine_tunes.create \\\n",
    "-t gpt-qa-train-formatted_prepared.jsonl \\\n",
    "-v gpt-qa-valid-formatted_prepared.jsonl \\\n",
    "-m \"davinci\" \\\n",
    "-learning_rate_multiplier 0.05 \\\n",
    "--suffix \"gpt-ml-qa-pairs-B\" \\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i could generate over several hyperparameters, \n",
    "# create a function that makes the call \n",
    "# saves the output \n",
    "# gets the finetuned model id\n",
    "# and then calls the next function with the finetuned model id as an argument\n",
    "\n",
    "\n",
    "# can i create a df of openai models and then iterate through it?\n",
    "# or do i need to create a function that takes the model id as an argument?\n",
    "# or do i need to create a function that takes the model id as an argument and then calls the next function with the finetuned model id as an argument?\n",
    "\n",
    "# i think i need to create a function that takes the model id as an argument and then calls the next function with the finetuned model id as an argument\n",
    "\n",
    "# function: takes in a dataset (just training set for now for simplicity) \n",
    "# outputs a finetuned model \"return\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === Abstracting the functions to create a fine-tuned model === "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create finetuned model \n",
    "\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def upload_file_openai(filename): \n",
    "    r = openai.File.create(\n",
    "        file=open(filename, \"rb\"),\n",
    "        purpose='fine-tune'\n",
    "    )\n",
    "    return r\n",
    "\n",
    "\n",
    "def create_finetuned_model(train_file_id, valid_file_id='', model=\"davinci\", learning_rate_multiplier='', n_epochs='', suffix=''):\n",
    "\n",
    "    # create a finetuned model\n",
    "    finetuned_model_response = openai.FineTune.create(\n",
    "        training_file=train_file_id,\n",
    "        # validation_file=valid_file_id,\n",
    "        model=model,\n",
    "        learning_rate_multiplier=learning_rate_multiplier,\n",
    "        n_epochs=n_epochs,\n",
    "        suffix=suffix,\n",
    "    )\n",
    "\n",
    "    return finetuned_model_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_file_id = upload_file_openai('gpt-qa-train-formatted_prepared.jsonl')['id']\n",
    "_valid_file_id = upload_file_openai('gpt-qa-valid-formatted_prepared.jsonl')['id']\n",
    "\n",
    "ftm = create_finetuned_model(\n",
    "    train_file_id=_train_file_id,\n",
    "    valid_file_id=_valid_file_id,\n",
    "    model=\"davinci\",\n",
    "    learning_rate_multiplier=0.05,\n",
    "    n_epochs=1,\n",
    "    suffix=\"gpt-ml-qa-pairs-C\",\n",
    ")\n",
    "\n",
    "ftm_id = ftm['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create the file_to_finetuned_model function \n",
    "\n",
    "# Simplify all the above \n",
    "\"\"\"\"\n",
    "file_to_finetuned_model\n",
    "\n",
    "Inputs: \n",
    "    filename: string, name of the file to be uploaded to openai\n",
    "    model: string, name of the model to be finetuned\n",
    "    learning_rate_multiplier: float, learning rate multiplier\n",
    "    n_epochs: int, number of epochs\n",
    "    suffix: string, suffix to be added to the finetuned model name\n",
    "\n",
    "Outputs: \n",
    "    finetuned_model_id: string, id of the finetuned model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# takes in .jsonl file, returns a finetuned model response \n",
    "\n",
    "def file_to_finetuned_model(train_file, valid_file='', model=\"davinci\", learning_rate_multiplier=0.1, n_epochs=4, suffix=''):\n",
    "\n",
    "    # upload file to openai\n",
    "    if train_file:\n",
    "        assert(train_file.endswith('.jsonl')), \"train_file must be a .jsonl file\"\n",
    "        _train_file_id = upload_file_openai(train_file)['id']\n",
    "    else:\n",
    "        FileNotFoundError('Must include a train_file')\n",
    "\n",
    "    if valid_file != '':\n",
    "        assert(valid_file.endswith('.jsonl')), \"valid_file must be a .jsonl file\"\n",
    "        _valid_file_id = upload_file_openai(valid_file)['id']\n",
    "    else: \n",
    "        _valid_file_id = ''\n",
    "\n",
    "    # create a finetuned model\n",
    "    finetuned_model_response = create_finetuned_model(\n",
    "        train_file_id=_train_file_id,\n",
    "        # valid_file_id=_valid_file_id,\n",
    "        model=model,\n",
    "        learning_rate_multiplier=learning_rate_multiplier,\n",
    "        n_epochs=n_epochs,\n",
    "        suffix=suffix,\n",
    "    )\n",
    "\n",
    "    return finetuned_model_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{\n",
      "  \"created_at\": 1668949486,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1668949486,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-qodKCfHvQpCRAdFPYzXowY22\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.1,\n",
      "    \"n_epochs\": 4,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-qodKCfHvQpCRAdFPYzXowY22\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 50561,\n",
      "      \"created_at\": 1668949486,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-J9K60guYJAQ3KkGglz8h4wdy\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1668949486,\n",
      "  \"validation_files\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "# test the file_to_finetuned_model function\n",
    "\n",
    "\n",
    "banana_file = \"gpt-qa-train-formatted_prepared.jsonl\"\n",
    "banana_suffix=\"qa-train-iterate\"\n",
    "\n",
    "ftm = file_to_finetuned_model(\n",
    "    train_file=banana_file,\n",
    "    suffix=banana_suffix + \"-z\"\n",
    ")\n",
    "\n",
    "ftm_id = ftm['id']\n",
    "ftm_name=ftm['fine_tuned_model']\n",
    "print(ftm_name)\n",
    "print (ftm)\n",
    "\n",
    "# ftm is only available after the model has been created!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ft-CgHrXIX0OF8EsJhWLiOGUQvp'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftm_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73754cf6dbb84ab391be17c0d28ba8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5089c3eb7f49c28b309d70825d99de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiated ftm: \n",
      " ftm_id: ft-2jSFUg0jBJUslmfkCI5iDrST. \n",
      " Now we wait for the model to train... \n",
      "\n",
      "waiting for ft-2jSFUg0jBJUslmfkCI5iDrST to train.\n",
      "ft-2jSFUg0jBJUslmfkCI5iDrST has trained for 100 seconds so far.\n",
      "ft-2jSFUg0jBJUslmfkCI5iDrST is ready!\n",
      "{\n",
      "  \"created_at\": 1668952253,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1668952253,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-2jSFUg0jBJUslmfkCI5iDrST\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952262,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune costs $0.35\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952263,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune enqueued. Queue number: 0\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952266,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune started\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952366,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 1/1\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952398,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded model: davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-13-53-18\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952399,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded result file: file-gmOSwKbxLCD8ObpwYy6P65iC\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952399,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune succeeded\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": \"davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-13-53-18\",\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"learning_rate_multiplier\": 0.1,\n",
      "    \"n_epochs\": 1,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-2jSFUg0jBJUslmfkCI5iDrST\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
      "  \"result_files\": [\n",
      "    {\n",
      "      \"bytes\": 2461,\n",
      "      \"created_at\": 1668952399,\n",
      "      \"filename\": \"compiled_results.csv\",\n",
      "      \"id\": \"file-gmOSwKbxLCD8ObpwYy6P65iC\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune-results\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 50561,\n",
      "      \"created_at\": 1668952253,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-rJXqjkVxH1ZJWOSUXci0n3l2\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1668952400,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-13-53-18 qa-train-iterate:lrm-0.1, _epoch-1, \n",
      "ft-2jSFUg0jBJUslmfkCI5iDrST finetuned model is now ready for testing. \n",
      "Its name is: davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-13-53-18\n",
      "\n",
      "starting test-prompts on finetuned model ft-2jSFUg0jBJUslmfkCI5iDrST\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2380f74826f14e7197026cafc930107d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-13-53-18 with prompt: What is the difference between a CNN and a DNN? \n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-13-53-18 with prompt: How many roads must a man walk down? \n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-13-53-18 with prompt: What does `DataLoaders` do in the fast.ai library? \n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-13-53-18 with prompt: Please write python code for a list comprehension\n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-13-53-18 with prompt: Please write a bash script to save the output of a terminal command to a json file\n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-13-53-18 with prompt: What are the steps for stochastic gradient descent? \n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Completed test prompts for ftmodel:  ft-2jSFUg0jBJUslmfkCI5iDrST\n",
      "Initiated ftm: \n",
      " ftm_id: ft-HG19PaSiLpcBN4gxw6sORuXA. \n",
      " Now we wait for the model to train... \n",
      "\n",
      "waiting for ft-HG19PaSiLpcBN4gxw6sORuXA to train.\n",
      "ft-HG19PaSiLpcBN4gxw6sORuXA has trained for 100 seconds so far.\n",
      "ft-HG19PaSiLpcBN4gxw6sORuXA is ready!\n",
      "{\n",
      "  \"created_at\": 1668952499,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1668952499,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-HG19PaSiLpcBN4gxw6sORuXA\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952502,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune costs $0.35\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952503,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune enqueued. Queue number: 0\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952504,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune started\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952605,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 1/1\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952647,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded model: davinci:ft-sandbox:qa-train-iterate-lrm-0-05-epoch-1-2022-11-20-13-57-27\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952648,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded result file: file-st9Snm2ZtlGfHn2YQg3BtzUb\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952648,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune succeeded\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": \"davinci:ft-sandbox:qa-train-iterate-lrm-0-05-epoch-1-2022-11-20-13-57-27\",\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"learning_rate_multiplier\": 0.05,\n",
      "    \"n_epochs\": 1,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-HG19PaSiLpcBN4gxw6sORuXA\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
      "  \"result_files\": [\n",
      "    {\n",
      "      \"bytes\": 2433,\n",
      "      \"created_at\": 1668952648,\n",
      "      \"filename\": \"compiled_results.csv\",\n",
      "      \"id\": \"file-st9Snm2ZtlGfHn2YQg3BtzUb\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune-results\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 50561,\n",
      "      \"created_at\": 1668952499,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-2hvRpTcRYFIS2u7Ddb9ubcj4\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1668952648,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "davinci:ft-sandbox:qa-train-iterate-lrm-0-05-epoch-1-2022-11-20-13-57-27 qa-train-iterate:lrm-0.05, _epoch-1, \n",
      "ft-HG19PaSiLpcBN4gxw6sORuXA finetuned model is now ready for testing. \n",
      "Its name is: davinci:ft-sandbox:qa-train-iterate-lrm-0-05-epoch-1-2022-11-20-13-57-27\n",
      "\n",
      "starting test-prompts on finetuned model ft-HG19PaSiLpcBN4gxw6sORuXA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091eee7cba1345829a885762f1b72b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-05-epoch-1-2022-11-20-13-57-27 with prompt: What is the difference between a CNN and a DNN? \n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-05-epoch-1-2022-11-20-13-57-27 with prompt: How many roads must a man walk down? \n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-05-epoch-1-2022-11-20-13-57-27 with prompt: What does `DataLoaders` do in the fast.ai library? \n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-05-epoch-1-2022-11-20-13-57-27 with prompt: Please write python code for a list comprehension\n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-05-epoch-1-2022-11-20-13-57-27 with prompt: Please write a bash script to save the output of a terminal command to a json file\n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-05-epoch-1-2022-11-20-13-57-27 with prompt: What are the steps for stochastic gradient descent? \n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Completed test prompts for ftmodel:  ft-HG19PaSiLpcBN4gxw6sORuXA\n",
      "Initiated ftm: \n",
      " ftm_id: ft-k8qbcdLPM8mr4V22xYqzNWGM. \n",
      " Now we wait for the model to train... \n",
      "\n",
      "waiting for ft-k8qbcdLPM8mr4V22xYqzNWGM to train.\n",
      "ft-k8qbcdLPM8mr4V22xYqzNWGM has trained for 100 seconds so far.\n",
      "ft-k8qbcdLPM8mr4V22xYqzNWGM is ready!\n",
      "{\n",
      "  \"created_at\": 1668952743,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1668952743,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-k8qbcdLPM8mr4V22xYqzNWGM\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952753,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune costs $0.35\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952753,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune enqueued. Queue number: 0\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952756,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune started\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952859,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 1/1\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952899,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded model: davinci:ft-sandbox:qa-train-iterate-lrm-0-01-epoch-1-2022-11-20-14-01-39\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952900,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded result file: file-Qe5s71yWpnR9qBhu1JPYFhU1\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952900,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune succeeded\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": \"davinci:ft-sandbox:qa-train-iterate-lrm-0-01-epoch-1-2022-11-20-14-01-39\",\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"learning_rate_multiplier\": 0.01,\n",
      "    \"n_epochs\": 1,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-k8qbcdLPM8mr4V22xYqzNWGM\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
      "  \"result_files\": [\n",
      "    {\n",
      "      \"bytes\": 2419,\n",
      "      \"created_at\": 1668952900,\n",
      "      \"filename\": \"compiled_results.csv\",\n",
      "      \"id\": \"file-Qe5s71yWpnR9qBhu1JPYFhU1\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune-results\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 50561,\n",
      "      \"created_at\": 1668952743,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-bFWyv4qDSZ1EeH8suCDdzO5E\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1668952901,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "davinci:ft-sandbox:qa-train-iterate-lrm-0-01-epoch-1-2022-11-20-14-01-39 qa-train-iterate:lrm-0.01, _epoch-1, \n",
      "ft-k8qbcdLPM8mr4V22xYqzNWGM finetuned model is now ready for testing. \n",
      "Its name is: davinci:ft-sandbox:qa-train-iterate-lrm-0-01-epoch-1-2022-11-20-14-01-39\n",
      "\n",
      "starting test-prompts on finetuned model ft-k8qbcdLPM8mr4V22xYqzNWGM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acf900b12414c4392684976813292ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-01-epoch-1-2022-11-20-14-01-39 with prompt: What is the difference between a CNN and a DNN? \n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-01-epoch-1-2022-11-20-14-01-39 with prompt: How many roads must a man walk down? \n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-01-epoch-1-2022-11-20-14-01-39 with prompt: What does `DataLoaders` do in the fast.ai library? \n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-01-epoch-1-2022-11-20-14-01-39 with prompt: Please write python code for a list comprehension\n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-01-epoch-1-2022-11-20-14-01-39 with prompt: Please write a bash script to save the output of a terminal command to a json file\n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-01-epoch-1-2022-11-20-14-01-39 with prompt: What are the steps for stochastic gradient descent? \n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Completed test prompts for ftmodel:  ft-k8qbcdLPM8mr4V22xYqzNWGM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2eb00232d94680a390ae49663456b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiated ftm: \n",
      " ftm_id: ft-RIqxCLVFMKnDOhW7rSS38zKX. \n",
      " Now we wait for the model to train... \n",
      "\n",
      "waiting for ft-RIqxCLVFMKnDOhW7rSS38zKX to train.\n",
      "ft-RIqxCLVFMKnDOhW7rSS38zKX has trained for 100 seconds so far.\n",
      "ft-RIqxCLVFMKnDOhW7rSS38zKX is ready!\n",
      "{\n",
      "  \"created_at\": 1668952954,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1668952954,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-RIqxCLVFMKnDOhW7rSS38zKX\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952962,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune costs $0.70\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952962,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune enqueued. Queue number: 0\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668952963,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune started\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668953065,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 1/2\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668953085,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 2/2\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668953117,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded model: davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-2-2022-11-20-14-05-16\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668953118,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded result file: file-kiBQuWw7Oz6FHjqYmpz23dLB\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668953118,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune succeeded\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": \"davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-2-2022-11-20-14-05-16\",\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"learning_rate_multiplier\": 0.1,\n",
      "    \"n_epochs\": 2,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-RIqxCLVFMKnDOhW7rSS38zKX\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
      "  \"result_files\": [\n",
      "    {\n",
      "      \"bytes\": 4751,\n",
      "      \"created_at\": 1668953117,\n",
      "      \"filename\": \"compiled_results.csv\",\n",
      "      \"id\": \"file-kiBQuWw7Oz6FHjqYmpz23dLB\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune-results\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 50561,\n",
      "      \"created_at\": 1668952954,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-OHZpJSIi3DRDYVxnA62V5b6Z\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1668953118,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-2-2022-11-20-14-05-16 qa-train-iterate:lrm-0.1, _epoch-2, \n",
      "ft-RIqxCLVFMKnDOhW7rSS38zKX finetuned model is now ready for testing. \n",
      "Its name is: davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-2-2022-11-20-14-05-16\n",
      "\n",
      "starting test-prompts on finetuned model ft-RIqxCLVFMKnDOhW7rSS38zKX\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddda0db734f43fe8e9c3e8db0f271a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-2-2022-11-20-14-05-16 with prompt: What is the difference between a CNN and a DNN? \n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-2-2022-11-20-14-05-16 with prompt: How many roads must a man walk down? \n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-2-2022-11-20-14-05-16 with prompt: What does `DataLoaders` do in the fast.ai library? \n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-2-2022-11-20-14-05-16 with prompt: Please write python code for a list comprehension\n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-2-2022-11-20-14-05-16 with prompt: Please write a bash script to save the output of a terminal command to a json file\n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "creating call from davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-2-2022-11-20-14-05-16 with prompt: What are the steps for stochastic gradient descent? \n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Completed test prompts for ftmodel:  ft-RIqxCLVFMKnDOhW7rSS38zKX\n",
      "Initiated ftm: \n",
      " ftm_id: ft-XTXZ1W8AIxRCOOYSnzH296CY. \n",
      " Now we wait for the model to train... \n",
      "\n",
      "waiting for ft-XTXZ1W8AIxRCOOYSnzH296CY to train.\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETER EXPLORATION \n",
    "\n",
    "# create a set of various hyperparameters to iterate through\n",
    "# create a ftmodel for each one \n",
    "# save the ftmodel id to a list\n",
    "# then iterate through the list and generate responses to the test-prompts for each one\n",
    "# save the responses to a list\n",
    "# then compare the responses to the test answers\n",
    "import time \n",
    "\n",
    "# wrap this in tqdm to show progress\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "currsuffix = banana_suffix+suffix\n",
    "currsuffix = \"curie:ft-personal-2022-11-20-08-38-46\"\n",
    "\n",
    "def create_ftm_suffix(base,**kwargs):\n",
    "    str = base + \":\" + \"_\".join([f\"{k}-{v}, \" for k,v in kwargs.items()])\n",
    "    return str\n",
    "\n",
    "def already_exists_ftm(suffix):\n",
    "    all_ft_models = openai.FineTune.list()['data']\n",
    "\n",
    "    # this checks if the current name is already in the list of ft models\n",
    "    # any( ) returns True if any element in the list is True\n",
    "    exists = any([True for i in all_ft_models if suffix in i['fine_tuned_model']])\n",
    "    print('ftmodel already exists: ', suffix)\n",
    "    return exists\n",
    " \n",
    "\n",
    "banana_file = \"gpt-qa-train-formatted_prepared.jsonl\"\n",
    "\n",
    "base_suffix=\"qa-train-iterate\"\n",
    "# create a set of various hyperparameters to iterate through\n",
    "# epochs=[1,2,3,4]\n",
    "# learning_rate_multiplier=[0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "epochs=[1,2]\n",
    "learning_rate_multiplier=[0.1, 0.05, 0.01]\n",
    "\n",
    "suffixes=['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "ftmodels=pd.DataFrame(columns=['id', 'epochs', 'learning_rate_multiplier', 'suffix'])\n",
    "\n",
    "ftmodelcompletions = pd.DataFrame(columns=['id', 'ftm_name', 'epochs', 'learning_rate_multiplier', 'suffix', 'prompt','completion'])\n",
    "\n",
    "# create a ftmodel for each one\n",
    "\n",
    "for epoch in tqdm(epochs):\n",
    "# for epoch in epochs:\n",
    "\n",
    "    for lrm in tqdm(learning_rate_multiplier):\n",
    "\n",
    "        #check if the current model already exists. if so, skip it\n",
    "        curr_suffix = create_ftm_suffix(base_suffix,lrm=lrm, epoch=epoch)\n",
    "\n",
    "        # if already_exists_ftm(curr_suffix): continue\n",
    "\n",
    "        # @TODO actually this should say: \n",
    "        # if already exists, then don't create the finetuned model, just get the id\n",
    "        # if doesn't exist then create it, and then get the id \n",
    "\n",
    "        # create a ft model with these parameters \n",
    "        ftm = file_to_finetuned_model(\n",
    "            train_file=banana_file,\n",
    "            suffix=curr_suffix,\n",
    "            n_epochs=epoch,\n",
    "            learning_rate_multiplier=lrm\n",
    "        )\n",
    "\n",
    "        ftm_id = ftm['id']\n",
    "\n",
    "        print(f\"\\n\\n======================= Initiated ftm:  ======================= \\n ftm_id: {ftm_id}. \\n Now we wait for the model to train... \\n\")\n",
    "\n",
    "        # wait for the ft model to finish training\n",
    "        current_ft_model_is_ready = await_ft_ready(ftm_id)\n",
    "        if not current_ft_model_is_ready: \n",
    "            print(f\"finetuned model timed out: {ftm_id}\")\n",
    "            continue\n",
    "\n",
    "        # the model has passed the await test, so it can be called now\n",
    "        # it's also been given a name, so it can be called by ftm_name now\n",
    "        # ftm_name = ftm['fine_tuned_model'] # the name is stored as ftm['fine_tuned_model']\n",
    "\n",
    "        ftm = openai.FineTune.retrieve(ftm_id) # need to retrieve the fresh version with its name! \n",
    "        \n",
    "        ftm_name = ftm['fine_tuned_model']\n",
    "\n",
    "        print (ftm_name, curr_suffix)\n",
    "        #also we could derive the name from the suffix! although they add a timestamp to the end of the name, which we'd want to emulate \n",
    "\n",
    "\n",
    "        print (f\"{ftm_id} finetuned model is now ready for testing. \\nIts name is: {ftm_name}\\n\")\n",
    "\n",
    "        # append this model to the df \n",
    "        ftmodels = ftmodels.append({'id': ftm_id, 'ftm_name': ftm_name, 'epochs': epoch, 'learning_rate_multiplier': lrm, 'suffix': curr_suffix}, ignore_index=True)\n",
    "\n",
    "        # now test every prompt with every ftmodel\n",
    "        with open ('test-prompts.txt') as f:\n",
    "            print(f\"starting test-prompts on finetuned model {ftm_id}\")\n",
    "            prompts = f.readlines()\n",
    "            for prompt in tqdm(prompts):\n",
    "                # get the completion for this prompt\n",
    "                completion = get_gpt_answer(prompt, ftm_name, printit=True)\n",
    "                # append this completion to the df\n",
    "                ftmodelcompletions = ftmodelcompletions.append(\n",
    "                    {'id': ftm_id, \n",
    "                    'ftm_name': ftm_name,\n",
    "                    'epochs': epoch, \n",
    "                    'learning_rate_multiplier': lrm, \n",
    "                    'suffix': curr_suffix, \n",
    "                    'prompt': prompt, \n",
    "                    'completion': completion}, \n",
    "                    ignore_index=True)\n",
    "            print('\\n\\n\\nCompleted test prompts for ftmodel: ', ftm_id)\n",
    "\n",
    "\n",
    "ftmodels.to_csv('ftmodels.csv')\n",
    "ftmodelcompletions.to_csv('ftmodelcompletions.csv')\n",
    "\n",
    "print(ftmodels.head())\n",
    "print(ftmodelcompletions.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-12-50-36\n",
      "ft-xXiSGiL0RrMUpKikJw09T1up\n"
     ]
    }
   ],
   "source": [
    "# HELPERS \n",
    "\n",
    "def get_ftmodel_name_from_id(id):\n",
    "    ftmodel = openai.FineTune.retrieve(id)\n",
    "    return ftmodel['fine_tuned_model']\n",
    "\n",
    "def get_ftmodel_id_from_name(name):\n",
    "    all_ft_models = openai.FineTune.list()['data']\n",
    "    for ftmodel in all_ft_models:\n",
    "        if name in ftmodel['fine_tuned_model']:\n",
    "            return ftmodel['id']\n",
    "    return None\n",
    "\n",
    "print(get_ftmodel_name_from_id(\"ft-xXiSGiL0RrMUpKikJw09T1up\"))\n",
    "print(get_ftmodel_id_from_name(\"davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-12-50-36\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.Model.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succeeded\n",
      "waiting for ft-6RlUCVJ6XrFOiKJ7yEzP8vbP to train.\n",
      "ft-6RlUCVJ6XrFOiKJ7yEzP8vbP is ready!\n",
      "{\n",
      "  \"created_at\": 1668951167,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1668951167,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-6RlUCVJ6XrFOiKJ7yEzP8vbP\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668951170,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune costs $0.35\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668951170,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune enqueued. Queue number: 0\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668951173,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune started\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668951274,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Completed epoch 1/1\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668951306,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded model: davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-13-35-06\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668951307,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Uploaded result file: file-YJSXZSSwAODZhngkFsxWjcU6\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1668951307,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tune succeeded\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": \"davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-13-35-06\",\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"learning_rate_multiplier\": 0.1,\n",
      "    \"n_epochs\": 1,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-6RlUCVJ6XrFOiKJ7yEzP8vbP\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
      "  \"result_files\": [\n",
      "    {\n",
      "      \"bytes\": 2464,\n",
      "      \"created_at\": 1668951306,\n",
      "      \"filename\": \"compiled_results.csv\",\n",
      "      \"id\": \"file-YJSXZSSwAODZhngkFsxWjcU6\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune-results\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 50561,\n",
      "      \"created_at\": 1668951167,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-BSZZjp0C6nRRpBv0S1keeT79\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1668951307,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# GET FTM AWAIT READY \n",
    "\n",
    "\n",
    "# the finetuned model needs to finish training before we can use it\n",
    "# so let's track its status and only make calls when its' ready \n",
    "# we can use the finetuned model id to check its status\n",
    "\n",
    "def get_finetuned_model_status(finetuned_model_id):\n",
    "    response = openai.FineTune.retrieve(id=finetuned_model_id)\n",
    "    return response['status']\n",
    "\n",
    "# check the status of the finetuned model\n",
    "print(get_finetuned_model_status(ftm_id))\n",
    "\n",
    "# now let's create a function that will wait until the finetuned model is ready\n",
    "# and then return the response\n",
    "\n",
    "def await_ft_ready(finetuned_model_id):\n",
    "\n",
    "    patience = 40 # number of times to check the status of the finetuned model\n",
    "    counter=0\n",
    "\n",
    "    print(f\"waiting for {finetuned_model_id} to train.\")\n",
    "\n",
    "    # print (openai.FineTune.retrieve(id=finetuned_model_id))\n",
    "\n",
    "    while True:\n",
    "        status = get_finetuned_model_status(finetuned_model_id)\n",
    "        if status == 'succeeded':\n",
    "            print(f\"{finetuned_model_id} is ready!\")\n",
    "            print (openai.FineTune.retrieve(id=finetuned_model_id))\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            # print(f\"{finetuned_model_id} finetuned model is still training. {patience*10} seconds so far.\")\n",
    "            counter +=1\n",
    "            if counter % 10 == 0: \n",
    "                print(f\"{finetuned_model_id} has trained for {counter*10} seconds so far.\")\n",
    "            if counter > patience: \n",
    "                # print that the current timed out\n",
    "                print(f\"{finetuned_model_id} finetuned model has taken longer than {patience*10} to train. Calling exceeds patience variable.\")\n",
    "                print (openai.FineTune.retrieve(id=finetuned_model_id))\n",
    "                return False\n",
    "            time.sleep(10) \n",
    "\n",
    "# now let's test it out\n",
    "print(await_ft_ready(ftm_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files.\n",
    "\n",
    "The _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. \n",
    "\n",
    "openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminal outputs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "(ml) SF-mbp:gpt-fine-tuning stephen$ openai api fine_tunes.create \\\n",
    "> -t gpt-qa-train_prepared.jsonl \\\n",
    "> -v gpt-qa-valid_prepared.jsonl\n",
    "-m \"davinci\" \\\n",
    "--suffix \"gpt-ml-qa-pairs-A\"\n",
    "Upload progress: 100%|███████████████████████| 50.8k/50.8k [00:00<00:00, 22.6Mit/s]\n",
    "Uploaded file from gpt-qa-train_prepared.jsonl: file-5fYU80lXy5FQgfmfVU56k8nb\n",
    "Upload progress: 100%|███████████████████████| 12.2k/12.2k [00:00<00:00, 8.80Mit/s]\n",
    "Uploaded file from gpt-qa-valid_prepared.jsonl: file-99tOIqCaPbNwL90bI1K1yq23\n",
    "Created fine-tune: ft-zHk8gGvZdaBNRVheXq5fZFqt\n",
    "Streaming events until fine-tuning is complete...\n",
    "\n",
    "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
    "[2022-11-20 00:36:48] Created fine-tune: ft-zHk8gGvZdaBNRVheXq5fZFqt\n",
    "[2022-11-20 00:36:52] Fine-tune costs $0.14\n",
    "[2022-11-20 00:36:52] Fine-tune enqueued. Queue number: 0\n",
    "[2022-11-20 00:36:54] Fine-tune started\n",
    "[2022-11-20 00:37:54] Completed epoch 1/4\n",
    "[2022-11-20 00:38:06] Completed epoch 2/4\n",
    "[2022-11-20 00:38:18] Completed epoch 3/4\n",
    "[2022-11-20 00:38:30] Completed epoch 4/4\n",
    "[2022-11-20 00:38:46] Uploaded model: curie:ft-personal-2022-11-20-08-38-46\n",
    "[2022-11-20 00:38:47] Uploaded result file: file-EZy5duulRUswzYdqj6LuW76a\n",
    "[2022-11-20 00:38:47] Fine-tune succeeded\n",
    "\n",
    "Job complete! Status: succeeded 🎉\n",
    "Try out your fine-tuned model:\n",
    "\n",
    "openai api completions.create -m curie:ft-personal-2022-11-20-08-38-46 -p <YOUR_PROMPT>\n",
    "(ml) SF-mbp:gpt-fine-tuning stephen$ -m \"davinci\" \\\n",
    "> --suffix \"gpt-ml-qa-pairs-A\"\n",
    "-bash: -m: command not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying again \n",
    "\n",
    "$ openai api fine_tunes.create \\\n",
    "> -t gpt-qa-train_prepared.jsonl \\\n",
    "> -v gpt-qa-valid_prepared.jsonl \\\n",
    "> -m \"davinci\" \\\n",
    "> --suffix \"gpt-ml-qa-pairs-A\"\n",
    "Found potentially duplicated files with name 'gpt-qa-train_prepared.jsonl', purpose 'fine-tune' and size 50782 bytes\n",
    "file-5fYU80lXy5FQgfmfVU56k8nb\n",
    "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway:\n",
    "Upload progress: 100%|███████████████████████████████| 50.8k/50.8k [00:00<00:00, 21.8Mit/s]\n",
    "Uploaded file from gpt-qa-train_prepared.jsonl: file-XCo6Sza9cVyfIWjPmoatpK3s\n",
    "Found potentially duplicated files with name 'gpt-qa-valid_prepared.jsonl', purpose 'fine-tune' and size 12227 bytes\n",
    "file-99tOIqCaPbNwL90bI1K1yq23\n",
    "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway:\n",
    "Upload progress: 100%|███████████████████████████████| 12.2k/12.2k [00:00<00:00, 6.49Mit/s]\n",
    "Uploaded file from gpt-qa-valid_prepared.jsonl: file-HbOaJkGRRu4hp4C8DQ4PtZ0t\n",
    "Created fine-tune: ft-xNzKl8ORDdASWIALy5WoSLcY\n",
    "Streaming events until fine-tuning is complete...\n",
    "\n",
    "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
    "[2022-11-20 00:45:39] Created fine-tune: ft-xNzKl8ORDdASWIALy5WoSLcY\n",
    "[2022-11-20 00:45:46] Fine-tune costs $1.42\n",
    "[2022-11-20 00:45:47] Fine-tune enqueued. Queue number: 0\n",
    "[2022-11-20 00:45:48] Fine-tune started\n",
    "[2022-11-20 00:47:30] Completed epoch 1/4\n",
    "[2022-11-20 00:47:50] Completed epoch 2/4\n",
    "[2022-11-20 00:48:10] Completed epoch 3/4\n",
    "[2022-11-20 00:48:30] Completed epoch 4/4\n",
    "[2022-11-20 00:49:22] Uploaded model: davinci:ft-sandbox:gpt-ml-qa-pairs-a-2022-11-20-08-49-22\n",
    "[2022-11-20 00:49:23] Uploaded result file: file-rGIPU3ndPAZXVRSBT4xhjDKy\n",
    "[2022-11-20 00:49:23] Fine-tune succeeded\n",
    "\n",
    "Job complete! Status: succeeded 🎉\n",
    "Try out your fine-tuned model:\n",
    "\n",
    "openai api completions.create -m davinci:ft-sandbox:gpt-ml-qa-pairs-a-2022-11-20-08-49-22 -p <YOUR_PROMPT>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, test the fine-tuned model with prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# set the api key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In general, \"deep learning\" refers to the use of many layers of computational units (i.e. neurons) in a neural network or other machine learning model. \n",
      "\n",
      " In other words, a deep learning model is a neural network or other machine learning model that uses many layers of computational units. \n",
      "\n",
      " In practice, the terms \"deep learning\" and \"neural network\" are often used interchangeably, since deep learning is typically used for building neural networks. However, the terms can be used more specifically to refer to different aspects of neural network or deep learning models. For example, a \"deep neural network\" may be used to refer to a neural network with many layers of hidden units or neurons, while the term \"deep learning\" may be used to refer to the use of GPUs or other hardware for performing large amounts of parallel processing on the data. ‍ ‍\n"
     ]
    }
   ],
   "source": [
    "# davinci:ft-sandbox:gpt-ml-qa-pairs-b-2022-11-20-09-29-25\n",
    "\n",
    "# call the openai api with a prompt and get a response\n",
    "\n",
    "ftm_name = \"davinci:ft-sandbox:gpt-ml-qa-pairs-b-2022-11-20-09-29-25\"\n",
    "\n",
    "prompt = \"What is the difference between a neural network and a deep learning model?\" \n",
    "\n",
    "def get_gpt_answer(prompt, ftm_name_, printit=False): \n",
    "\n",
    "    prompt += \"\\n\\n###\\n\\n\"\n",
    "\n",
    "    if printit:\n",
    "        print(f\"creating call from {ftm_name_} with prompt: {prompt}\")\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=ftm_name_,\n",
    "        # engine=\"text-davinci-002\", \n",
    "        prompt=prompt,\n",
    "        temperature=0.9,\n",
    "        max_tokens=500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0.6,\n",
    "        stop=[\" \\n<+++>\\n\"]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "\n",
    "print(get_gpt_answer(prompt, ftm_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0: What is the difference between a CNN and a DNN? \n",
      "\n",
      "Answer 0:  \n",
      " There are several differences between Convolutional Neural Networks (CNNs) and Deep Neural Networks (DNNs). \n",
      "\n",
      " Firstly, while both CNNs and DNNs use neural networks, the structure of the neural networks in a CNN is different from that of a DNN. In a CNN, the layers of the neural network are arranged in a \"convolutional\" manner, with the output of each layer being connected to the input of the next layer via a small number of hidden units. In contrast, the layers of a DNN are arranged in a \"feedforward\" manner, with the output of each layer being connected to the input of the next layer via a large number of hidden units. \n",
      "\n",
      " Secondly, while CNNs typically use a fixed number of filters per layer, DNNs use a variable number of filters per layer, determined by the optimization algorithm used to train the network. This is known as \"feature scaling\" in DNNs. Thirdly, while CNNs typically use a fixed number of hidden units per layer, DNNs use a variable number of hidden units per layer, determined by the optimization algorithm used to train the network. This is known as \"activation scaling\" in DNNs. Finally, while CNNs are typically applied to images (e.g. for image classification or object detection), DNNs can be applied to a wide range of different types of data, including images, audio, text and tabular data.\n",
      "<+++>\n",
      "\n",
      "+++\n",
      "\n",
      "What are the advantages and disadvantages of using a CNN vs. a DNN for image processing? \n",
      "\n",
      " There are both advantages and disadvantages to using a Convolutional Neural Network (CNN) or a Deep Neural Network (DNN) for image processing. \n",
      "\n",
      "\n",
      "\n",
      "Question 1: How many roads must a man walk down? \n",
      "\n",
      "Answer 1:  Where is the love? \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Question 2: What does `DataLoaders` do in the fast.ai library? \n",
      "\n",
      "Answer 2:  The `DataLoaders` directory in the fast.ai library contains code for loading data into theano or tensorflow computational graphs. \n",
      "\n",
      " The `DataLoaders` directory includes code for loading standard datasets (e.g. MNIST, CIFAR10), as well as supporting code to help you load your own data (e.g. text files, CSV files). ‍ ‍ \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb Cell 40\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(lines): \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     answer \u001b[39m=\u001b[39m get_gpt_answer(line, ftm_name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mQuestion \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mline\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAnswer \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00manswer\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb Cell 40\u001b[0m in \u001b[0;36mget_gpt_answer\u001b[0;34m(prompt, ftm_name_, printit)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mif\u001b[39;00m printit:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcreating call from \u001b[39m\u001b[39m{\u001b[39;00mftm_name_\u001b[39m}\u001b[39;00m\u001b[39m with prompt: \u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     model\u001b[39m=\u001b[39;49mftm_name_,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# engine=\"text-davinci-002\", \u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.6\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     stop\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m<+++>\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephen/Dev/gpt-fine-tuning/fastai-scrape-nb.ipynb#X62sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:115\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    107\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39mAPIRequestor(\n\u001b[1;32m    108\u001b[0m     api_key,\n\u001b[1;32m    109\u001b[0m     api_base\u001b[39m=\u001b[39mapi_base,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m     organization\u001b[39m=\u001b[39morganization,\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[0;32m--> 115\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    116\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    117\u001b[0m     url,\n\u001b[1;32m    118\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    119\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    120\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    121\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    122\u001b[0m     request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    126\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/openai/api_requestor.py:171\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    161\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    162\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    170\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 171\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    172\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    173\u001b[0m         url,\n\u001b[1;32m    174\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    175\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    176\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    177\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    178\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    179\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    180\u001b[0m     )\n\u001b[1;32m    181\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/openai/api_requestor.py:356\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    354\u001b[0m     _thread_context\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m _make_session()\n\u001b[1;32m    355\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 356\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    357\u001b[0m         method,\n\u001b[1;32m    358\u001b[0m         abs_url,\n\u001b[1;32m    359\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    360\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    361\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    362\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    363\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    364\u001b[0m     )\n\u001b[1;32m    365\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    366\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:542\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    537\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    538\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[1;32m    539\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[1;32m    540\u001b[0m }\n\u001b[1;32m    541\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 542\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    544\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:655\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    654\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    657\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    658\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:439\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 439\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    440\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    441\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    442\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    443\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    444\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    445\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    446\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    447\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    448\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    449\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    450\u001b[0m         )\n\u001b[1;32m    452\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1270\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1271\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1272\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ml/lib/python3.10/ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test prompts \n",
    "\n",
    "# for each line in the file, call the openai api and get a response\n",
    "with open('test-prompts.txt', 'r') as f: \n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines): \n",
    "        answer = get_gpt_answer(line, ftm_name)\n",
    "        print(f'Question {i}: {line}')\n",
    "        print(f'Answer {i}: {answer} \\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        print(\"{} = {}\".format(key, value))\n",
    "    myb = kwargs[\"b\"] if \"b\" in kwargs\n",
    "    print(myb)\n",
    "        \n",
    "\n",
    "bar(a=1, b=2, c=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d905c6a59c21f0f46be93fdc832728644d115a3fdfd57971d06d899b53e0576e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
