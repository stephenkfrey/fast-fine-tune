{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Fine-tuning API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# set the api key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Notes on formatting for GPT fine-tuning ####################\n",
    "\n",
    "# - Based on your file extension, your file is formatted as a CSV file\n",
    "# - Your file contains 56 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
    "# - Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
    "# - All completions end with suffix ` `\n",
    "#   WARNING: Some of your completions contain the suffix ` ` more than once. We suggest that you review your completions and add a unique ending\n",
    "# - The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
    "\n",
    "# add a space to the end of each prompt\n",
    "# Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\n\\n###\\n\\n. The separator should not appear elsewhere in any prompt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load local data setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FastAI Q&A pairs \n",
    "pc_csv_IN_path = \"data/gptchatqadata/gpt-qa-formatted.csv\"\n",
    "\n",
    "this_model_name = \"ML-GPTChat-QA\"\n",
    "\n",
    "test_prompts_path = \"tests/test-prompts.txt\"\n",
    "\n",
    "assert os.path.exists(pc_csv_IN_path), \"File does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_dir_path = \"tests/\"\n",
    "tests_output_dir_path = \"test-outputs/\"\n",
    "\n",
    "# test_prompts_path = os.path.join(tests_dir_path, 'test-prompts.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Load raw CSV files of prompt-completion pairs --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Format raw CSV --> formatted CSV --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####\n",
      "Raw CSV -> Formatted CSV conversion complete\n",
      "data/gptchatqadata/gpt-qa-formatted-MANUAL-formatted.csv\n",
      "#####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# INPUTS: \n",
    "# - csv: a csv with headers 'prompt' and 'completion'\n",
    "\n",
    "# PROCESSING: \n",
    "# - add a space to the end of each prompt\n",
    "# - add a \\n\\n###\\n\\n to the end of each prompt\n",
    "# - add a space to the beginning of each completion\n",
    "# - add a \\n\\n<+++>\\n\\n to the end of each completion\n",
    "\n",
    "# saves to df\n",
    "\n",
    "# OUTPUTS:\n",
    "# - csv: a cleaned, formatted csv ready for fine-tuning \n",
    "\n",
    "def format_csv_for_ft(qa_csv_path):\n",
    "    #### Check formatting on CSV \n",
    "    assert qa_csv_path[-4:] == '.csv', \"input path does not end with '.csv'\"\n",
    "    # - headers are 'prompt' and 'completion'\n",
    "    assert pd.read_csv(qa_csv_path, nrows=0).columns.tolist() == ['prompt', 'completion'], \"file headers are not 'prompt' and 'completion'\"\n",
    "\n",
    "    gpt_formatted_df = pd.read_csv(qa_csv_path)\n",
    "    # print(gpt_loaded_df.head()) # double check input \n",
    "\n",
    "    #### Format for fine-tuning \n",
    "    # 1) Completions: start with a space ' ', end with a seperator '\\n+END+\\n'\n",
    "    # Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.\n",
    "    # Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\n, ###, or any other token that does not appear in any completion.\n",
    "    gpt_formatted_df['completion'] = gpt_formatted_df['completion'].apply(lambda x: ' ' + x + ' END')\n",
    "    # 2) add a separater to the end of each prompt \n",
    "    gpt_formatted_df['prompt'] = gpt_formatted_df['prompt'].apply(lambda x: x + '\\n\\n###\\n\\n')\n",
    "\n",
    "    #### Save to csv \n",
    "    qa_csv_OUT_path=os.path.join(pc_csv_IN_path[:-4]+\"-MANUAL-formatted.csv\")\n",
    "    gpt_formatted_df.to_csv(qa_csv_OUT_path, index=False)\n",
    "    return qa_csv_OUT_path\n",
    "\n",
    "formatted_pc_csv_path = format_csv_for_ft(pc_csv_IN_path)\n",
    "\n",
    "print(f\"\\n#####\\nRaw CSV -> Formatted CSV conversion complete\\n{formatted_pc_csv_path}\\n#####\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date and Time = 2018-12-25 01:27:53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2023/01/13-04:24:29'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper \n",
    "import datetime as datetime\n",
    "\n",
    "# use the fromtimestamp method\n",
    "def get_datetime_from_timestamp(timestamp):\n",
    "    # timestamp = 1545730073\n",
    "    date_time = datetime.datetime.fromtimestamp(timestamp)\n",
    "    print(\"Date and Time =\", date_time)\n",
    "    return date_time\n",
    "\n",
    "# test this \n",
    "get_datetime_from_timestamp(1545730073)\n",
    "\n",
    "\n",
    "\n",
    "def get_datetime_now_string():\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    dt_string = now.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "    return dt_string\n",
    "\n",
    "get_datetime_now_string()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV -> JSONL, using the OpenAI CLI tool\n",
    "\n",
    "*Typically, we call the following in terminal to convert the file:*\n",
    "\n",
    "`openai tools fine_tunes.prepare_data -f <LOCAL_FILE>`\n",
    "\n",
    "The following section automates this with a bash script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_conversion_sh_script_path = './scripts/csv2jsonl_openai.sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assure permissions for the bash script and file \n",
    "! chmod a+x {jsonl_conversion_sh_script_path}\n",
    "! chmod a+x {formatted_pc_csv_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Based on your file extension, your file is formatted as a CSV file\n",
      "- Your file contains 56 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
      "- All prompts end with suffix `\\n\\n###\\n\\n\\n\\n###\\n\\n`. This suffix seems very long. Consider replacing with a shorter suffix, such as `\\n\\n===\\n\\n`\n",
      "- All completions end with suffix ` \\n<+++>\\n END`. This suffix seems very long. Consider replacing with a shorter suffix, such as `***`\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Necessary] Your format `CSV` will be converted to `JSONL`\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: \n",
      "Wrote modified file to `data/gptchatqadata/gpt-qa-formatted-MANUAL-formatted_prepared (2).jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"data/gptchatqadata/gpt-qa-formatted-MANUAL-formatted_prepared (2).jsonl\"\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\" \\n<+++>\\n END\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 3.21 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n",
      "Done!\n",
      "\n",
      "./scripts/csv2jsonl_openai.sh: line 18: syntax error near unexpected token `done'\n",
      "./scripts/csv2jsonl_openai.sh: line 18: `done'\n",
      "\n",
      "data/gptchatqadata/gpt-qa-formatted-MANUAL-formatted_prepared.jsonl\n",
      "\n",
      "#####\n",
      "CSV -> JSONL conversion complete\n",
      "#####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Run the OpenAI CLI tool to format the CSV to JSONL \n",
    "import subprocess\n",
    "# you need to add './' before the filename for bash to recognize it \n",
    "\n",
    "assert os.path.exists(formatted_pc_csv_path), \"file does not exist\"\n",
    "assert os.path.exists(jsonl_conversion_sh_script_path), \"sh file does not exist\"\n",
    "\n",
    "# Call the bash script to format the CSV to JSONL using the OpenAI CLI tool \n",
    "process = subprocess.Popen([jsonl_conversion_sh_script_path, formatted_pc_csv_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "if process is not None:\n",
    "    stdout, stderr = process.communicate()\n",
    "    print(stdout.decode('utf-8'))\n",
    "    print(stderr.decode('utf-8'))\n",
    "    \n",
    "    \n",
    "# get the new JSONL filename \n",
    "formatted_pc_jsonl_path = formatted_pc_csv_path[:-4] + '_prepared.jsonl'\n",
    "assert (os.path.exists(formatted_pc_jsonl_path)), \"JSONL file does not exist\"\n",
    "print(formatted_pc_jsonl_path)\n",
    "print(\"\\n#####\\nCSV -> JSONL conversion complete\\n#####\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Functions to create a fine-tuned model from the JSONL file --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# HELPER FUNCTIONS\n",
    "#####################\n",
    "\n",
    "# FUNCTION \n",
    "# upload_file_openai\n",
    "# - takes in a jsonl formatted dataset (just training set for now for simplicity)\n",
    "# - eventually could abstract to general file upload but not needed for now\n",
    "def upload_jsonl_ft_to_openai(filename): \n",
    "    # assert the filetype is jsonl \n",
    "    ext = os.path.splitext(filename)[-1].lower()\n",
    "    print ('ext:', ext)\n",
    "    assert ext== '.jsonl', \"filetype must be jsonl\"\n",
    "    r = openai.File.create(\n",
    "        file=open(filename, \"rb\"),\n",
    "        purpose='fine-tune'\n",
    "    )\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# FUNCTION \n",
    "# create_finetuned_model\n",
    "\n",
    "# INPUTS: \n",
    "# - train_file_id: a csv with headers 'prompt' and 'completion'\n",
    "# - model type\n",
    "# - hyperparameters\n",
    "\n",
    "# OUTPUTS:\n",
    "# finetuned_model_response: a response object including the finetuned model id\n",
    "\n",
    "# @TODO add the ability to turn on and off test-train splitting \n",
    "\n",
    "def create_finetuned_model(train_file_id, valid_file_id='', model=\"davinci-003\", learning_rate_multiplier='', n_epochs='', suffix=''):\n",
    "\n",
    "    #####################\n",
    "    # create the request to build the finetuned model \n",
    "    try: \n",
    "        finetuned_model_response = openai.FineTune.create(\n",
    "            training_file=train_file_id,\n",
    "            # validation_file=valid_file_id, # ignore valid file for now\n",
    "            model=model,\n",
    "            learning_rate_multiplier=learning_rate_multiplier,\n",
    "            n_epochs=n_epochs,\n",
    "            suffix=suffix,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print (\"Error creating finetuned model: \", e)\n",
    "        return None\n",
    "    return finetuned_model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the finetuned model id to the list.csv file, along with the datetime\n",
    "\n",
    "\n",
    "\n",
    "def log_finetuned_model_id(ftm_id, model_name, learning_rate_multiplier, n_epochs, suffix, log_file_path):\n",
    "    # get the current datetime \n",
    "    \n",
    "    # create the row to append \n",
    "    now = get_datetime_now_string()\n",
    "    row = f\"{now},{ftm_id},{model_name},{learning_rate_multiplier},{n_epochs},{suffix}\" \n",
    "    # append the row to the log file\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        f.write(row + '\\n')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# MAIN FUNCTION: file_to_finetuned_model\n",
    "\n",
    "\"\"\"\"\n",
    "Inputs: \n",
    "    filename: \n",
    "        - filename path of a csv file to be uploaded to openai\n",
    "        - must be formatted as a csv with headers 'prompt' and 'completion' - see the functions above to do that \n",
    "            eg \"/data/my-prompt-completion-pairs-list.csv\"\n",
    "\n",
    "    model: string, name of the model to be finetuned \n",
    "            eg \"davinci-003\"\n",
    "    \n",
    "    learning_rate_multiplier: float, learning rate multiplier for the finetuned model \n",
    "            eg \"0.05\", typically 0.05 - 0.2\n",
    "    \n",
    "    n_epochs: int, number of epochs for the finetuned model \n",
    "            eg \"1\", typically 1 - 3\n",
    "    \n",
    "    suffix: string, suffix to be added to the finetuned model \n",
    "            eg \"my-finetuned-model\"\n",
    "\n",
    "Outputs: \n",
    "    finetuned_model_response: the response of the OpenAI API call to create the finetuned model\n",
    "        - a dictionary with the finetuned model id, hyperparameters, model name, etc.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# import datetime\n",
    "\n",
    "def file_to_finetuned_model_wrapper(train_file, valid_file='', model=\"davinci\", learning_rate_multiplier=0.05, n_epochs=2, suffix=''):\n",
    "    # upload files to openai\n",
    "    if train_file:\n",
    "        assert(train_file.endswith('.jsonl')), \"train_file must be a .jsonl file\"\n",
    "        _train_file_id = upload_jsonl_ft_to_openai(train_file)['id']\n",
    "        print ('Successfully uploaded train JSONL file. train_file_id:', _train_file_id)\n",
    "    else:\n",
    "        FileNotFoundError('Must include a train_file')\n",
    "\n",
    "    if valid_file != '':\n",
    "        assert(valid_file.endswith('.jsonl')), \"valid_file must be a .jsonl file\"\n",
    "        _valid_file_id = upload_jsonl_ft_to_openai(valid_file)['id']\n",
    "    else: \n",
    "        _valid_file_id = ''\n",
    "\n",
    "    # ensure suffix is within the required length; if not then truncate\n",
    "    if len(suffix) > 40:\n",
    "        suffix = suffix[:40]\n",
    "        print('Suffix longer than required 40-character length; truncating to 40 characters: ', suffix)\n",
    "    \n",
    "    \n",
    "    # create a finetuned model\n",
    "    finetuned_model_response = create_finetuned_model(\n",
    "        train_file_id=_train_file_id,\n",
    "        # valid_file_id=_valid_file_id,\n",
    "        model=model,\n",
    "        learning_rate_multiplier=learning_rate_multiplier,\n",
    "        n_epochs=n_epochs,\n",
    "        suffix=suffix,\n",
    "    )\n",
    "\n",
    "    return finetuned_model_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Create a ftm from the JSONL file --- \n",
    "The magic is here! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ext: .jsonl\n",
      "Successfully uploaded train JSONL file. train_file_id: file-2v0x76EyT1qtw68XUUqHErzZ\n"
     ]
    }
   ],
   "source": [
    "ftm = file_to_finetuned_model_wrapper(\n",
    "    train_file=formatted_pc_jsonl_path, #from the previous step \n",
    "    learning_rate_multiplier=0.02,\n",
    "    suffix=this_model_name # declared at the beginning of the ntoebook \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log each element of the json response as a column in a csv file\n",
    "# from datetime import datetime\n",
    "\n",
    "def log_ftm(ftm):\n",
    "    \"\"\"\n",
    "    # Checks if the ftm id is already in the log file. If not, then adds to log file. \n",
    "    # INPUTS: ftm - the json response from the OpenAI API call to create a finetuned model\n",
    "    # OUTPUTS: True/False\n",
    "    \"\"\"\n",
    "    log_file_path = 'outputs/finetuned-models-log.csv'\n",
    "\n",
    "    if not os.path.exists(log_file_path):\n",
    "        with open(log_file_path, 'w') as f:\n",
    "            f.write('datetime,created_at,ftm_id, status\\n')\n",
    "\n",
    "    # check if the ftm id already exists in the csv \n",
    "    pattern = ftm['id']\n",
    "    with open(log_file_path, 'r') as csvfile:\n",
    "        if any(map(lambda x: pattern == x.rstrip(), csvfile)): # iterates through text looking for match\n",
    "            print (\"Finetuned model id already exists in log file\")\n",
    "            return False\n",
    "        else:\n",
    "            with open(log_file_path, 'a') as f:\n",
    "                f.write(f\"{datetime.datetime.now()}, {ftm['created_at']},{ftm['id']}, {ftm['status']}\\n\")\n",
    "                # maybe datetime.datetime.now() instead of datetime.now()\n",
    "            return True\n",
    "    \n",
    "\n",
    "log_ftm(ftm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print *initial* profile of ftm \n",
    "Note that the model takes 5-30 minutes to train. In the meantime, it will return these properties: \n",
    "\n",
    "Name: none\n",
    "\n",
    "Status: pending "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###\n",
      "Created fine-tuned model ft-37KR1qzwE8sS5pwTqFlYtiPS\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ftm_id = ftm['id']\n",
    "\n",
    "print(f\"\\n###\\nCreated fine-tuned model {ftm_id}\\n###\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Create Test with FastAI Question-Answer Pairs data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ftm_fastai = file_to_finetuned_model_wrapper(\n",
    "#     train_file='data/fastaiqadata/fastai-qa-cleaned-formatted-CSV-FIRST_prepared.jsonl', # NEEDS UPDATING \n",
    "#     suffix='fastai-qa-cleaned-formatted-CSV-FIRST_prepared',\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('id ', ftm_fastai['id'])\n",
    "# print('name ',ftm_fastai['fine_tuned_model'])\n",
    "# print('status ', ftm_fastai['status'])\n",
    "# print (display_unix_time(ftm['created_at']))\n",
    "# print ('ftm: ', ftm_fastai)\n",
    "\n",
    "# ####\n",
    "# whatsup = openai.FineTune.retrieve(ftm_fastai['id'])\n",
    "# whatsup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Wait for Upload --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERS \n",
    "\n",
    "def get_ftmodel_name_from_id(id):\n",
    "    ftmodel = openai.FineTune.retrieve(id)\n",
    "    return ftmodel['fine_tuned_model']\n",
    "\n",
    "def get_ftmodel_id_from_name(name):\n",
    "    all_ft_models = openai.FineTune.list()['data']\n",
    "    for ftmodel in all_ft_models:\n",
    "        if name in ftmodel['fine_tuned_model']:\n",
    "            return ftmodel['id']\n",
    "    return None\n",
    "\n",
    "def get_ftm_from_id(id):\n",
    "    return openai.FineTune.retrieve(id)\n",
    "\n",
    "# print(get_ftmodel_name_from_id(\"ft-xXiSGiL0RrMUpKikJw09T1up\"))\n",
    "# print(get_ftmodel_id_from_name(\"davinci:ft-sandbox:qa-train-iterate-lrm-0-1-epoch-1-2022-11-20-12-50-36\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET FTM AWAIT READY \n",
    "\n",
    "# the finetuned model needs to finish training before we can use it\n",
    "# so let's track its status and only make calls when its' ready \n",
    "# we can use the finetuned model id to check its status\n",
    "\n",
    "import time\n",
    "\n",
    "def get_finetuned_model_status(finetuned_model_id):\n",
    "    response = openai.FineTune.retrieve(id=finetuned_model_id)\n",
    "    return response['status']\n",
    "\n",
    "#####\n",
    "# FUNCTION: await_ft_ready\n",
    "# INPUT: finetuned_model_id, seconds_of_patience\n",
    "# prints every check_every_n_seconds seconds the status of the mdoel \n",
    "# times out after seconds_of_patience seconds\n",
    "# OUTPUT: True if finetuned model is ready, False if finetuned model is not ready\n",
    "\n",
    "def await_ft_ready(finetuned_model_id, seconds_of_patience=400):\n",
    "\n",
    "    check_every_n_seconds = 10\n",
    "    # round down \n",
    "    num_checkins = seconds_of_patience//check_every_n_seconds # number of times to check the status of the finetuned model\n",
    "    print(f\"###\\nWaiting for {finetuned_model_id} to train.\\n###\\n\")\n",
    "\n",
    "    finetuned_model_status = None \n",
    "    counter = 0\n",
    "\n",
    "    while True:\n",
    "        status = get_finetuned_model_status(finetuned_model_id)\n",
    "        if status == 'succeeded':\n",
    "            print(f\"###\\n{finetuned_model_id} is ready!\\n###\\n\")\n",
    "            print (openai.FineTune.retrieve(id=finetuned_model_id))\n",
    "            return True\n",
    "        else:\n",
    "            counter +=1\n",
    "            print(f\"{finetuned_model_id} finetuned model is still training. status: {status}. {counter*check_every_n_seconds} seconds so far. counter: {counter}\")\n",
    "            \n",
    "            if counter % 10 == 0: #arbitrary every N check ins, print update \n",
    "                print(f\"{finetuned_model_id}. seconds so far: at least {counter*check_every_n_seconds}. Status: {status}\")\n",
    "            if counter > num_checkins: \n",
    "                # print that the current timed out\n",
    "                print(f\"{finetuned_model_id} finetuned model has taken longer than {seconds_of_patience} to train, since we started checking in on its status. Calling exceeds patience variable. Exiting await-train loop.\")\n",
    "                print (openai.FineTune.retrieve(id=finetuned_model_id))\n",
    "                return False\n",
    "            time.sleep(check_every_n_seconds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-13 03:12:22 PST\n"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "\n",
    "#convert unix time to human readable time\n",
    "def display_unix_time(unix_time):\n",
    "    # return datetime.datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "    # convert the timezone to Pacific Time \n",
    "    # use 24 hour time instead of 12 hour time\n",
    "    return datetime.datetime.fromtimestamp(unix_time).astimezone(pytz.timezone('US/Pacific')).strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "    # return datetime.datefromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(display_unix_time(ftm['created_at']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_latest_ftm(_ftm_id):\n",
    "    ftm = openai.FineTune.retrieve(id=_ftm_id)\n",
    "    print (f\"###\\nid: {ftm['id']}\\nname:{ftm['fine_tuned_model']} \\ncreated at: {display_unix_time(ftm['created_at'])} \\nstatus: {ftm['status']}\\n###\\n\") \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment the cell below if you want the notebook to wait for the model we just trained \n",
    "\n",
    "Note: Fine-tuned models can take between 3-30 minutes to train, depending on the OpenAI server capacity. \n",
    "\n",
    "It's helpful to separate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "Waiting for ft-37KR1qzwE8sS5pwTqFlYtiPS to train.\n",
      "###\n",
      "\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 10 seconds so far. counter: 1\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 20 seconds so far. counter: 2\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 30 seconds so far. counter: 3\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 40 seconds so far. counter: 4\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 50 seconds so far. counter: 5\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 60 seconds so far. counter: 6\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 70 seconds so far. counter: 7\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 80 seconds so far. counter: 8\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 90 seconds so far. counter: 9\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 100 seconds so far. counter: 10\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS. seconds so far: at least 100. Status: pending\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 110 seconds so far. counter: 11\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 120 seconds so far. counter: 12\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 130 seconds so far. counter: 13\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 140 seconds so far. counter: 14\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 150 seconds so far. counter: 15\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 160 seconds so far. counter: 16\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 170 seconds so far. counter: 17\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 180 seconds so far. counter: 18\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 190 seconds so far. counter: 19\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 200 seconds so far. counter: 20\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS. seconds so far: at least 200. Status: pending\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 210 seconds so far. counter: 21\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 220 seconds so far. counter: 22\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 230 seconds so far. counter: 23\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 240 seconds so far. counter: 24\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 250 seconds so far. counter: 25\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 260 seconds so far. counter: 26\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 270 seconds so far. counter: 27\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 280 seconds so far. counter: 28\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 290 seconds so far. counter: 29\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 300 seconds so far. counter: 30\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS. seconds so far: at least 300. Status: pending\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 310 seconds so far. counter: 31\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 320 seconds so far. counter: 32\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 330 seconds so far. counter: 33\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 340 seconds so far. counter: 34\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 350 seconds so far. counter: 35\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 360 seconds so far. counter: 36\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 370 seconds so far. counter: 37\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 380 seconds so far. counter: 38\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 390 seconds so far. counter: 39\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 400 seconds so far. counter: 40\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS. seconds so far: at least 400. Status: pending\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model is still training. status: pending. 410 seconds so far. counter: 41\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS finetuned model has taken longer than 400 to train, since we started checking in on its status. Calling exceeds patience variable. Exiting await-train loop.\n",
      "{\n",
      "  \"created_at\": 1673608342,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1673608342,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-37KR1qzwE8sS5pwTqFlYtiPS\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 2,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-37KR1qzwE8sS5pwTqFlYtiPS\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-5RSpJP9M5vC6iGBmX0vRfiMp\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 65711,\n",
      "      \"created_at\": 1673608341,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-2v0x76EyT1qtw68XUUqHErzZ\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1673608342,\n",
      "  \"validation_files\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ftm_training_is_finished = await_ft_ready(ftm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO:\n",
    "# - add ftm to a local sql database upon creation \n",
    "# - cron job to check every 5 mins if status updated and is ready"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log training results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We attach a result file to each job once it has been completed. This results file ID will be listed when you retrieve a fine-tune, and also when you look at the events on a fine-tune. You can download these files.\n",
    "\n",
    "The _results.csv file contains a row for each training step, where a step refers to one forward and backward pass on a batch of data. \n",
    "\n",
    "`openai api fine_tunes.results -i <YOUR_FINE_TUNE_JOB_ID>`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- After training complete: Log the training-loss file as a csv  --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_training_results_obj(ftm_id):\n",
    "    \"\"\" \n",
    "    INPUT: finetuned model id\n",
    "    OUTPUT: bytes object of results \n",
    "    \"\"\"\n",
    "    ftm=get_ftm_from_id(ftm_id)\n",
    "    # get the results file id\n",
    "    results_files= ftm['result_files']\n",
    "    print('results_files', results_files)\n",
    "    # for now just get the first one\n",
    "    first_results_file_id = results_files[0]['id']\n",
    "    print('first_results_file_id', first_results_file_id)\n",
    "    # get the results file\n",
    "    return openai.File.download(first_results_file_id)\n",
    "\n",
    "def get_training_results_df(ftm_id):\n",
    "    \"\"\" \n",
    "    INPUT: finetuned model id\n",
    "    - calls get_training_results_obj to download results object\n",
    "    - converts results object to csv and to df \n",
    "    OUTPUT: df of results \n",
    "    \"\"\"\n",
    "    # call the previous function \n",
    "    results_file_content = get_training_results_obj(ftm_id)\n",
    "    # Will also want a CSV file of the results\n",
    "    filepath = os.path.join(tests_output_dir_path, f\"training-results-{ftm_id}_results.csv\")\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(results_file_content)\n",
    "    # create df\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_loss(df):\n",
    "    plt.plot(df['training_loss'])\n",
    "    plt.title('training loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "\n",
    "if ftm_training_is_finished:\n",
    "    df = get_training_results_df(ftm_id)\n",
    "    print(df.head())\n",
    "    plot_training_loss(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Test prompts --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_response_text(prompt, ftm_name_, printit=False): \n",
    "\n",
    "    # prompt += \"\\n\\n###\\n\\n\"\n",
    "\n",
    "    if printit:\n",
    "        print(f\"creating call from {ftm_name_} with prompt: {prompt}\")\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=ftm_name_,\n",
    "        # engine=\"text-davinci-002\", \n",
    "        prompt=prompt,\n",
    "        temperature=0.5,\n",
    "        max_tokens=500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\" END\"]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "# davinci:ft-sandbox:gpt-ml-qa-pairs-b-2022-11-20-09-29-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Run with the current model ---      \n",
    "\n",
    "--> Loads the model again from `ftm_id`\n",
    "\n",
    "Why? Only after it has trained will it have a `name`, which is needed to send as the `model` param to the OpenAI Completion endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft-37KR1qzwE8sS5pwTqFlYtiPS\n",
      "ft-37KR1qzwE8sS5pwTqFlYtiPS\n"
     ]
    }
   ],
   "source": [
    "print (ftm_id)\n",
    "print (\"ft-37KR1qzwE8sS5pwTqFlYtiPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "id: ft-37KR1qzwE8sS5pwTqFlYtiPS \n",
      "name: davinci:ft-sandbox:ml-gptchat-qa-2023-01-13-14-37-37 \n",
      "status: succeeded\n",
      "created_at: 2023-01-13 03:12:22 PST \n",
      "updated_at: 2023-01-13 06:37:39 PST\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_model_status (ftm_id):\n",
    "    ftm = get_ftm_from_id(ftm_id)\n",
    "    ftm_name = ftm['fine_tuned_model']\n",
    "    ftm_status = ftm['status']\n",
    "    print (f\"###\\nid: {ftm_id} \\nname: {ftm_name} \\nstatus: {ftm_status}\")\n",
    "    print(f\"created_at: {display_unix_time(ftm['created_at'])} \\nupdated_at: {display_unix_time(ftm['updated_at'])}\")\n",
    "    print (f\"###\\n\")\n",
    "\n",
    "print_model_status(ftm_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run a single test prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"What is the difference between a neural network and a deep learning model?\" \n",
    "# #ftm_name is defined after creating the ftm\n",
    "# print(get_gpt_response_text(prompt, ftm_name, printit=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run a battery of test prompts on the model from \"test-prompts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davinci:ft-sandbox:ml-gptchat-qa-2023-01-13-14-37-37\n",
      "Question 0: What is the difference between a CNN and a DNN? \n",
      "\n",
      "Answer 0: \n",
      "A CNN is a Convolutional Neural Network, it uses a single-layer neural network and is used for image classification and object recognition. A DNN is a Deep Neural Network, it uses multiple layers of neural networks and is used for image classification and object recognition. \n",
      "\n",
      "What is the difference between a CNN and a DNN? \n",
      "\n",
      "CNNs are used for image classification and object recognition, while DNNs are used for image classification and object recognition. \n",
      "\n",
      "What is the difference between a CNN and a DNN? \n",
      "\n",
      "CNNs are used for image classification and object recognition, while DNNs are used for image classification and object recognition. \n",
      "\n",
      "What is the difference between a CNN and a DNN? \n",
      "\n",
      "CNNs are used for image classification and object recognition, while DNNs are used for image classification and object recognition. \n",
      "\n",
      "What is the difference between a CNN and a DNN? \n",
      "\n",
      "CNNs are used for image classification and object recognition, while DNNs are used for image classification and object recognition. \n",
      "\n",
      "What is the difference between a CNN and a DNN? \n",
      "\n",
      "CNNs are used for image classification and object recognition, while DNNs are used for image classification and object recognition. \n",
      "\n",
      "What is the difference between a CNN and a DNN? \n",
      "\n",
      "CNNs are used for image classification and object recognition, while DNNs are used for image classification and object recognition. \n",
      "\n",
      "What is the difference between a CNN and a DNN? \n",
      "\n",
      "CNNs are used for image classification and object recognition, while DNNs are used for image classification and object recognition. \n",
      "\n",
      "What is the difference between a CNN and a DNN? \n",
      "\n",
      "CNNs are used for image classification and object recognition, while DNNs are used for image classification and object recognition. \n",
      "\n",
      "What is the difference between a CNN and a DNN? \n",
      "\n",
      "CNNs are used for image classification and object recognition, while DNNs are used for image classification and object recognition. \n",
      "\n",
      "What is the difference between a CNN and a DNN? \n",
      "\n",
      "CNNs are used for image classification and object recognition, while DNNs are used for image classification and object recognition. \n",
      "\n",
      "What is the difference between a CNN and a DNN? \n",
      "\n",
      "CNNs are used for image classification \n",
      "\n",
      "\n",
      "\n",
      "Question 1: How many roads must a man walk down? \n",
      "\n",
      "Answer 1: \n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk down? \n",
      "\n",
      "How many roads must a man walk \n",
      "\n",
      "\n",
      "\n",
      "Question 2: What does `DataLoaders` do in the fast.ai library? \n",
      "\n",
      "Answer 2: \n",
      "It is a Python library that allows you to load data into a machine learning model. It is a set of tools that you can use to load your data into a model. It is a set of tools that you can use to load your data into a model. \n",
      "\n",
      "The library is in the fast.ai library. It is a Python package. It is a set of tools that you can use to load your data into a model. It is a set of tools that you can use to load your data into a model. \n",
      "\n",
      "It contains a set of tools that you can use to load your data into a model. It is a set of tools that you can use to load your data into a model. It is a set of tools that you can use to load your data into a model. \n",
      "\n",
      "It is a set of tools that you can use to load your data into a model. It is a set of tools that you can use to load your data into a model. \n",
      "\n",
      "It is a set of tools that you can use to load your data into a model. It is a set of tools that you can use to load your data into a model. \n",
      "\n",
      "It is a set of tools that you can use to load your data into a model. It is a set of tools that you can use to load your data into a model. \n",
      "\n",
      "It is a set of tools that you can use to load your data into a model. It is a set of tools that you can use to load your data into a model. \n",
      "\n",
      "It is a set of tools that you can use to load your data into a model. It is a set of tools that you can use to load your data into a model. \n",
      "\n",
      "It is a set of tools that you can use to load your data into a model. It is a set of tools that you can use to load your data into a model. \n",
      "\n",
      "It is a set of tools that you can use to load your data into a model. It is a set of tools that you can use to load your data into a model. \n",
      "\n",
      "It is a set of tools that you can use to load your data into a model. It is a set of tools that you can use to load your data into a model. \n",
      "\n",
      "It is a set of tools that you can use to load your data into a model. It \n",
      "\n",
      "\n",
      "\n",
      "Question 3: Please write python code for a list comprehension\n",
      "\n",
      "Answer 3: \n",
      "that produces the following output:\n",
      "\n",
      "[’a’, ’b’, ’c’, ’d’, ’e’, ’f’, ’g’, ’h’, ’i’, ’j’, ’k’, ’l’, ’m’, ’n’,\n",
      "\n",
      "’o’, ’p’, ’q’, ’r’, ’s’, ’t’, ’u’, ’v’, ’w’, ’x’, ’y’, ’z’]\n",
      "\n",
      "You can use multiple variables in the list comprehension,\n",
      "\n",
      "but you cannot use a variable more than once.\n",
      "\n",
      "For example, the following is not valid:\n",
      "\n",
      ">>> [x + y for x in ’a’, ’b’, ’c’, ’d’, ’e’, ’f’, ’g’, ’h’, ’i’, ’j’, ’k’, ’l’, ’m’, ’n’, ’o’, ’p’, ’q’, ’r’, ’s’, ’t’, ’u’, ’v’, ’w’, ’x’, ’y’, ’z’]\n",
      "\n",
      ">>> [x + y for x in ’a’, ’b’, ’c’, ’d’, ’e’, ’f’, ’g’, ’h’, ’i’, ’j’, ’k’, ’l’, ’m’, ’n’, ’o’, ’p’, ’q’, ’r’, ’s’, ’t’, \n",
      "\n",
      "\n",
      "\n",
      "Question 4: Please write a bash script to save the output of a terminal command to a json file\n",
      "\n",
      "Answer 4: \n",
      "I am trying to write a bash script that can save the output of a terminal command to a json file. I have tried the following but it doesn't work.\n",
      "\n",
      "\n",
      "\n",
      "#!/bin/bash\n",
      "\n",
      "echo \"{\" > output.json\n",
      "\n",
      "echo \"\\\"status\\\": \\\"ok\\\"\" >> output.json\n",
      "\n",
      "echo \"}\" >> output.json\n",
      "\n",
      "echo \"done\" >> output.json\n",
      "\n",
      "echo \"{\" >> output.json\n",
      "\n",
      "echo \"\\\"status\\\": \\\"ok\\\"\" >> output.json\n",
      "\n",
      "echo \"}\" >> output.json\n",
      "\n",
      "echo \"done\" >> output.json\n",
      "\n",
      "\n",
      "\n",
      "I have tried using the following but it doesn't work either.\n",
      "\n",
      "\n",
      "\n",
      "#!/bin/bash\n",
      "\n",
      "echo \"{\" > output.json\n",
      "\n",
      "echo \"\\\"status\\\": \\\"ok\\\"\" >> output.json\n",
      "\n",
      "echo \"}\" >> output.json\n",
      "\n",
      "echo \"done\" >> output.json\n",
      "\n",
      "echo \"{\" >> output.json\n",
      "\n",
      "echo \"\\\"status\\\": \\\"ok\\\"\" >> output.json\n",
      "\n",
      "echo \"}\" >> output.json\n",
      "\n",
      "echo \"done\" >> output.json\n",
      "\n",
      "\n",
      "\n",
      "I want to save the output of the terminal command to a json file.\n",
      "\n",
      "Please help me. \n",
      "\n",
      "\n",
      "\n",
      "Question 5: What are the steps for stochastic gradient descent? \n",
      "Answer 5: \n",
      "\n",
      "The steps are as follows:\n",
      "\n",
      "1) Initialize the weights randomly.\n",
      "2) For each training example, compute the output of the network.\n",
      "3) Compute the error for the output of the network.\n",
      "4) Compute the gradient of the error with respect to the weights.\n",
      "5) Update the weights in order to reduce the error.\n",
      "6) Go back to step 2.\n",
      "\n",
      "What is a convolutional neural network? \n",
      "\n",
      "A convolutional neural network is a neural network that uses convolutional layers. A convolution is a mathematical operation that is used to calculate the output of a layer given its input.\n",
      "\n",
      "What is a convolutional layer? \n",
      "\n",
      "A convolutional layer is a layer that uses convolutional operations.\n",
      "\n",
      "What is a convolutional operation? \n",
      "\n",
      "A convolutional operation is an operation that uses convolutional layers.\n",
      "\n",
      "What is a convolutional operation used for? \n",
      "\n",
      "A convolutional operation is used to calculate the output of a layer given its input.\n",
      "\n",
      "What is a pooling layer? \n",
      "\n",
      "A pooling layer is a layer that uses pooling operations.\n",
      "\n",
      "What is a pooling operation? \n",
      "\n",
      "A pooling operation is an operation that uses pooling layers.\n",
      "\n",
      "What is a pooling operation used for? \n",
      "\n",
      "A pooling operation is used to reduce the number of parameters in a layer by reducing the size of the layer.\n",
      "\n",
      "What is a fully connected layer? \n",
      "\n",
      "A fully connected layer is a layer that uses fully connected operations.\n",
      "\n",
      "What is a fully connected operation? \n",
      "\n",
      "A fully connected operation is an operation that uses fully connected layers.\n",
      "\n",
      "What is a fully connected layer used for? \n",
      "\n",
      "A fully connected layer is used to connect the outputs of a pooling layer to the inputs of a convolutional layer.\n",
      "\n",
      "What is Batch Normalization? \n",
      "\n",
      "Batch Normalization is a technique used to improve the performance of a neural network by normalizing the inputs to the neural network.\n",
      "\n",
      "What is the difference between a convolutional layer and a fully connected layer? \n",
      "\n",
      "A convolutional layer is a layer that uses convolutional operations. A fully connected layer is a layer that uses fully connected operations.\n",
      "\n",
      "What is the difference between a convolutional \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# save all the question, answer pairs to a csv\n",
    "\n",
    "import csv\n",
    "\n",
    "ftm = get_ftm_from_id(ftm_id)\n",
    "ftm_name = ftm['fine_tuned_model']\n",
    "\n",
    "print (ftm_name)\n",
    "\n",
    "\n",
    "with open(test_prompts_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    now = get_datetime_now_string()\n",
    "    with open(f\"test-outputs/{this_model_name}-gpt-completions-to-test-prompts.csv\", 'w',newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['prompt','completion'])\n",
    "        for i, line in enumerate(lines):\n",
    "            print(f'Question {i}: {line}')\n",
    "            answer = get_gpt_response_text(line, ftm_name)\n",
    "            print(f'Answer {i}: {answer} \\n\\n\\n')\n",
    "            writer.writerow([line, answer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Gather Training results --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "def gather_training_results(ftm_trained):\n",
    "    \"\"\" \n",
    "    INPUT: finetuned model id\n",
    "    - calls get_training_results_obj to download results object\n",
    "    - converts results object to csv and to df \n",
    "    OUTPUT:\n",
    "    results \n",
    "    - results obj\n",
    "    - results df\n",
    "    - results csv path\n",
    "    losses\n",
    "    - train loss\n",
    "    - token loss\n",
    "    - valid loss\n",
    "    plots\n",
    "    - train, token, valid\n",
    "    \n",
    "    \"\"\"\n",
    "    # call the previous function \n",
    "    results_file_content = get_training_results_obj(ftm_trained)\n",
    "    # Will also want a CSV file of the results\n",
    "    # os path join a b \n",
    "    now = get_datetime_now_string()\n",
    "    filepath = os.path.join(tests_dir_path, f\"training-results-{ftm_trained}-{now}_results.csv\")\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(results_file_content)\n",
    "    # create df\n",
    "    df = pd.DataFrame(results_file_content)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Hyperparameter Exploration --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HYPERPARAMETER EXPLORATION \n",
    "\n",
    "EXPLORE_HYPERPARAMS=False\n",
    "\n",
    "# create a set of various hyperparameters to iterate through\n",
    "# create a ftmodel for each one \n",
    "# save the ftmodel id to a list\n",
    "# then iterate through the list and generate responses to the test-prompts for each one\n",
    "# save the responses to a list\n",
    "# then compare the responses to the test answers\n",
    "import time \n",
    "\n",
    "# wrap this in tqdm to show progress\n",
    "from tqdm import tqdm \n",
    "\n",
    "# load a previous ftm to test \n",
    "currsuffix = \"curie:ft-personal-2022-11-20-08-38-46\"\n",
    "\n",
    "def create_ftm_suffix(base,**kwargs):\n",
    "    str = base + \":\" + \"_\".join([f\"{k}-{v}, \" for k,v in kwargs.items()])\n",
    "    return str\n",
    "\n",
    "def already_exists_ftm(suffix):\n",
    "    all_ft_models = openai.FineTune.list()['data']\n",
    "\n",
    "    # this checks if the current name is already in the list of ft models\n",
    "    # any( ) returns True if any element in the list is True\n",
    "    exists = any([True for i in all_ft_models if suffix in i['fine_tuned_model']])\n",
    "    print('ftmodel already exists: ', suffix)\n",
    "    return exists\n",
    " \n",
    "\n",
    "test_file = \"gpt-qa-train-formatted_prepared.jsonl\"\n",
    "\n",
    "base_suffix=\"qa-train-iterate\"\n",
    "# create a set of various hyperparameters to iterate through\n",
    "# epochs=[1,2,3,4]\n",
    "# learning_rate_multiplier=[0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "epochs=[1,2]\n",
    "learning_rate_multiplier=[0.1, 0.05, 0.01]\n",
    "\n",
    "suffixes=['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "ftmodels=pd.DataFrame(columns=['id', 'epochs', 'learning_rate_multiplier', 'suffix'])\n",
    "\n",
    "ftmodelcompletions = pd.DataFrame(columns=['id', 'ftm_name', 'epochs', 'learning_rate_multiplier', 'suffix', 'prompt','completion'])\n",
    "\n",
    "# create a ftmodel for each one\n",
    "\n",
    "def explore_hyperparams(input_file, ftm, epochs, learning_rate_multiplier, suffixes):\n",
    "\n",
    "    ## Explore over: \n",
    "    # epochs\n",
    "    # learning rate multiplier\n",
    "\n",
    "\n",
    "    for epoch in tqdm(epochs):\n",
    "        for lrm in tqdm(learning_rate_multiplier):\n",
    "\n",
    "            #######\n",
    "            # Create a model with the current hyperparameters\n",
    "\n",
    "            curr_suffix = create_ftm_suffix(base_suffix,lrm=lrm, epoch=epoch)\n",
    "            # create a ft model with these parameters \n",
    "            ftm = file_to_finetuned_model(\n",
    "                train_file=input_file,\n",
    "                suffix=curr_suffix,\n",
    "                n_epochs=epoch,\n",
    "                learning_rate_multiplier=lrm\n",
    "            )\n",
    "\n",
    "            ftm_id = ftm['id']\n",
    "            print(f\"\\n\\n== Initiated ftm: == \\n ftm_id: {ftm_id}. \\n Now we wait for the model to train... \\n\")\n",
    "\n",
    "            # wait for the ft model to finish training\n",
    "            current_ft_model_is_ready = await_ft_ready(ftm_id)\n",
    "            if not current_ft_model_is_ready: \n",
    "                print(f\"finetuned model timed out: {ftm_id}\")\n",
    "                continue\n",
    "\n",
    "            ftm = openai.FineTune.retrieve(ftm_id) # need to retrieve the fresh version with its name! \n",
    "            ftm_name = ftm['fine_tuned_model']\n",
    "            print (f\"{ftm_id} finetuned model is now ready for testing. \\nIts name is: {ftm_name}\\n\")\n",
    "\n",
    "            # append this model to the df \n",
    "            ftmodels = ftmodels.append({'id': ftm_id, 'ftm_name': ftm_name, 'epochs': epoch, 'learning_rate_multiplier': lrm, 'suffix': curr_suffix}, ignore_index=True)\n",
    "\n",
    "            ##### \n",
    "            # now test every prompt with every ftmodel\n",
    "            with open ('test-prompts.txt') as f:\n",
    "                print(f\"starting test-prompts on finetuned model {ftm_id}\")\n",
    "                prompts = f.readlines()\n",
    "                for prompt in tqdm(prompts):\n",
    "                    # get the completion for this prompt\n",
    "                    completion = get_gpt_response_text(prompt, ftm_name, printit=True)\n",
    "                    # append this completion to the df\n",
    "                    ftmodelcompletions = ftmodelcompletions.append(\n",
    "                        {'id': ftm_id, \n",
    "                        'ftm_name': ftm_name,\n",
    "                        'epochs': epoch, \n",
    "                        'learning_rate_multiplier': lrm, \n",
    "                        'suffix': curr_suffix, \n",
    "                        'prompt': prompt, \n",
    "                        'completion': completion}, \n",
    "                        ignore_index=True)\n",
    "                print('\\n\\n\\nCompleted test prompts for ftmodel: ', ftm_id)\n",
    "\n",
    "    ftmodels.to_csv('ftmodels.csv')\n",
    "    ftmodelcompletions.to_csv('ftmodelcompletions.csv')\n",
    "\n",
    "    print(ftmodels.head())\n",
    "    print(ftmodelcompletions.head())\n",
    "\n",
    "\n",
    "if EXPLORE_HYPERPARAMS: \n",
    "    explore_hyperparams(formatted_pc_jsonl_path, ftm)\n",
    "\n",
    "    # SUCCESS! \n",
    "    print(ftmodels.head())\n",
    "    print(ftmodelcompletions.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d905c6a59c21f0f46be93fdc832728644d115a3fdfd57971d06d899b53e0576e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
